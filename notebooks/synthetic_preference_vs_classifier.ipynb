{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions import Normal\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(\"/home/ra43rid/torch_plnet\"))\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "class TorchMultivariateGaussianClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, class_params=None, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize the classifier with class parameters.\n",
    "\n",
    "        Parameters:\n",
    "        class_params: dict\n",
    "            A dictionary where keys are class labels and values are dictionaries with\n",
    "            'mean' (vector), 'cov' (matrix), and 'prior' for each class.\n",
    "        device: str\n",
    "            The device to use for computations ('cpu' or 'cuda').\n",
    "        \"\"\"\n",
    "        self.class_params = class_params if class_params is not None else {}\n",
    "        self.device = torch.device(device)\n",
    "        self.classes_ = torch.tensor(list(self.class_params.keys()), device=self.device)\n",
    "        self.n_features = len(self.class_params[0][\"cov\"])\n",
    "        self.n_classes = len(self.classes_)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method for compatibility. This classifier doesn't require fitting.\n",
    "        \"\"\"\n",
    "        self.classes_ = torch.tensor(list(self.class_params.keys()), device=self.device)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict the probability of each class for the given input data X.\n",
    "\n",
    "        Parameters:\n",
    "        X: torch.Tensor or array-like of shape (n_samples, n_features)\n",
    "            Input features.\n",
    "\n",
    "        Returns:\n",
    "        probs: torch.Tensor of shape (n_samples, n_classes)\n",
    "            Predicted probabilities for each class.\n",
    "        \"\"\"\n",
    "        if not isinstance(X, torch.Tensor):\n",
    "            X = torch.tensor(X, device=self.device, dtype=torch.float32)\n",
    "        \n",
    "        probs = torch.zeros((X.shape[0], len(self.classes_)), device=self.device)\n",
    "        for i, c in enumerate(self.classes_):\n",
    "            mean = torch.tensor(self.class_params[int(c)][\"mean\"], device=self.device, dtype=torch.float32)\n",
    "            cov = torch.tensor(self.class_params[int(c)][\"cov\"], device=self.device, dtype=torch.float32)\n",
    "            prior = self.class_params[int(c)][\"prior\"]\n",
    "            \n",
    "            # Multivariate normal distribution\n",
    "            mvn_dist = MultivariateNormal(mean, covariance_matrix=cov)\n",
    "            px_given_y = torch.exp(mvn_dist.log_prob(X))\n",
    "            \n",
    "            # Combine with prior\n",
    "            probs[:, i] = px_given_y * prior\n",
    "        \n",
    "        # Normalize to get P(y=c|x)\n",
    "        probs /= probs.sum(dim=1, keepdim=True)\n",
    "        return probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class label for each sample in X.\n",
    "\n",
    "        Parameters:\n",
    "        X: torch.Tensor or array-like of shape (n_samples, n_features)\n",
    "            Input features.\n",
    "\n",
    "        Returns:\n",
    "        predictions: torch.Tensor of shape (n_samples,)\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.classes_[torch.argmax(probs, dim=1)]\n",
    "    \n",
    "    def generate_data(self, n_samples=100):\n",
    "        \"\"\"\n",
    "        Generate synthetic data using the predefined class parameters.\n",
    "\n",
    "        Parameters:\n",
    "        n_samples: int\n",
    "            Number of samples to generate.\n",
    "\n",
    "        Returns:\n",
    "        X: torch.Tensor of shape (n_samples, n_features)\n",
    "            Generated features.\n",
    "        y: torch.Tensor of shape (n_samples,)\n",
    "            Generated labels.\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        y = []\n",
    "        for _ in range(n_samples):\n",
    "            # Sample class based on priors\n",
    "            sampled_class = torch.multinomial(\n",
    "                torch.tensor([self.class_params[int(c)][\"prior\"] for c in self.classes_], device=self.device),\n",
    "                num_samples=1\n",
    "            ).item()\n",
    "            mean = torch.tensor(self.class_params[int(self.classes_[sampled_class])][\"mean\"], device=self.device, dtype=torch.float32)\n",
    "            cov = torch.tensor(self.class_params[int(self.classes_[sampled_class])][\"cov\"], device=self.device, dtype=torch.float32)\n",
    "            \n",
    "            # Sample feature vector from the corresponding multivariate Gaussian\n",
    "            mvn_dist = MultivariateNormal(mean, covariance_matrix=cov)\n",
    "            sampled_x = mvn_dist.sample()\n",
    "            X.append(sampled_x)\n",
    "            y.append(self.classes_[sampled_class].item())\n",
    "        \n",
    "        return torch.stack(X), torch.tensor(y, device=self.device)\n",
    "\n",
    "class TorchGaussianSyntheticClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, class_params=None, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize the classifier with class parameters.\n",
    "\n",
    "        Parameters:\n",
    "        class_params: dict\n",
    "            A dictionary where keys are class labels and values are dictionaries with\n",
    "            'mean', 'std', and 'prior' for each class.\n",
    "        device: str\n",
    "            The device to use for computations ('cpu' or 'cuda').\n",
    "        \"\"\"\n",
    "        self.class_params = class_params if class_params is not None else {}\n",
    "        self.device = torch.device(device)\n",
    "        self.classes_ = torch.tensor(list(self.class_params.keys()), device=self.device)\n",
    "        self.n_features = 1\n",
    "        self.n_classes = len(self.classes_)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method for compatibility. This classifier doesn't require fitting.\n",
    "        \"\"\"\n",
    "        self.classes_ = torch.tensor(list(self.class_params.keys()), device=self.device)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict the probability of each class for the given input data X.\n",
    "\n",
    "        Parameters:\n",
    "        X: torch.Tensor or array-like of shape (n_samples,)\n",
    "            Input features.\n",
    "\n",
    "        Returns:\n",
    "        probs: torch.Tensor of shape (n_samples, n_classes)\n",
    "            Predicted probabilities for each class.\n",
    "        \"\"\"\n",
    "        if not isinstance(X, torch.Tensor):\n",
    "            X = torch.tensor(X, device=self.device, dtype=torch.float32)\n",
    "        \n",
    "        probs = torch.zeros((len(X), len(self.classes_)), device=self.device)\n",
    "        for i, c in enumerate(self.classes_):\n",
    "            mean = self.class_params[int(c)][\"mean\"]\n",
    "            std = self.class_params[int(c)][\"std\"]\n",
    "            prior = self.class_params[int(c)][\"prior\"]\n",
    "            \n",
    "            # Calculate Gaussian PDF: P(x|y=c)\n",
    "            normal_dist = Normal(loc=mean, scale=std)\n",
    "            px_given_y = torch.exp(normal_dist.log_prob(X))\n",
    "            \n",
    "            # Combine with prior: P(x|y=c) * P(y=c)\n",
    "            probs[:, i] = px_given_y * prior\n",
    "        \n",
    "        # Normalize to get P(y=c|x)\n",
    "        probs /= probs.sum(dim=1, keepdim=True)\n",
    "        return probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class label for each sample in X.\n",
    "\n",
    "        Parameters:\n",
    "        X: torch.Tensor or array-like of shape (n_samples,)\n",
    "            Input features.\n",
    "\n",
    "        Returns:\n",
    "        predictions: torch.Tensor of shape (n_samples,)\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.classes_[torch.argmax(probs, dim=1)]\n",
    "\n",
    "    def generate_data(self, n_samples=100):\n",
    "        \"\"\"\n",
    "        Generate synthetic data using the predefined class parameters.\n",
    "\n",
    "        Parameters:\n",
    "        n_samples: int\n",
    "            Number of samples to generate.\n",
    "\n",
    "        Returns:\n",
    "        X: torch.Tensor of shape (n_samples,)\n",
    "            Generated features.\n",
    "        y: torch.Tensor of shape (n_samples,)\n",
    "            Generated labels.\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        y = []\n",
    "        for _ in range(n_samples):\n",
    "            # Sample class based on priors\n",
    "            sampled_class = torch.multinomial(\n",
    "                torch.tensor([self.class_params[int(c)][\"prior\"] for c in self.classes_], device=self.device),\n",
    "                num_samples=1\n",
    "            ).item()\n",
    "            # Sample feature value from the corresponding Gaussian\n",
    "            mean = self.class_params[int(self.classes_[sampled_class])][\"mean\"]\n",
    "            std = self.class_params[int(self.classes_[sampled_class])][\"std\"]\n",
    "            normal_dist = Normal(loc=mean, scale=std)\n",
    "            sampled_x = normal_dist.sample().item()\n",
    "            X.append(sampled_x)\n",
    "            y.append(self.classes_[sampled_class].item())\n",
    "        \n",
    "        return torch.tensor(X, device=self.device), torch.tensor(y, device=self.device)\n",
    "\n",
    "def goodman_kruskal_gamma(x, y):\n",
    "    \"\"\"\n",
    "    Compute Goodman and Kruskal's Gamma for two ordinal variables.\n",
    "    \n",
    "    Parameters:\n",
    "    x, y: Lists or arrays of ordinal data (same length)\n",
    "    \n",
    "    Returns:\n",
    "    gamma: Goodman and Kruskal's Gamma\n",
    "    \"\"\"\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"Both variables must have the same length.\")\n",
    "    \n",
    "    concordant = 0\n",
    "    discordant = 0\n",
    "    \n",
    "    n = len(x)\n",
    "    for i in range(n - 1):\n",
    "        for j in range(i + 1, n):\n",
    "            # Determine concordance or discordance\n",
    "            if (x[i] > x[j] and y[i] > y[j]) or (x[i] < x[j] and y[i] < y[j]):\n",
    "                concordant += 1\n",
    "            elif (x[i] > x[j] and y[i] < y[j]) or (x[i] < x[j] and y[i] > y[j]):\n",
    "                discordant += 1\n",
    "    \n",
    "    # Compute Gamma\n",
    "    if concordant + discordant == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "    gamma = (concordant - discordant) / (concordant + discordant)\n",
    "    return gamma\n",
    "\n",
    "from torchcp.classification.score import APS, THR, SAPS\n",
    "aps = APS(score_type=\"identity\", randomized=False)\n",
    "rand_aps = APS(score_type=\"identity\", randomized=True)\n",
    "lac = THR(score_type=\"identity\",)\n",
    "saps = SAPS(score_type=\"identity\",randomized=False)\n",
    "\n",
    "class_params_1f_2c = {\n",
    "    0: {\"mean\": 1, \"std\": 1, \"prior\": 0.3},\n",
    "    1: {\"mean\": 3, \"std\": 1, \"prior\": 0.4},\n",
    "    # 2: {\"mean\": 4, \"std\": 2.2, \"prior\": 0.3},\n",
    "}   \n",
    "\n",
    "class_params_1f_3c = {\n",
    "    0: {\"mean\": 1, \"std\": 1, \"prior\": 0.3},\n",
    "    1: {\"mean\": 3, \"std\": 1, \"prior\": 0.4},\n",
    "    2: {\"mean\": 4, \"std\": 2.2, \"prior\": 0.3},\n",
    "}   \n",
    "\n",
    "# Initialize and fit the generator\n",
    "class_params_2d_3c = {\n",
    "    0: {\n",
    "        \"mean\": [3.0, 2.0],  # Mean vector for class 0\n",
    "        \"cov\": [\n",
    "            [1.0, 0.5],\n",
    "            [0.5, 1.2],\n",
    "            # [0.3, 0.4, 0.8]\n",
    "        ],  # Covariance matrix for class 0\n",
    "        \"prior\": 0.3  # Prior probability for class 0\n",
    "    },\n",
    "    1: {\n",
    "        \"mean\": [3.0, 4.0],  # Mean vector for class 1\n",
    "        \"cov\": [\n",
    "            [1.5, 0.3],\n",
    "            [0.3, 1.1],\n",
    "            # [0.2, 0.1, 0.9]\n",
    "        ],  # Covariance matrix for class 1\n",
    "        \"prior\": 0.4  # Prior probability for class 1\n",
    "    },\n",
    "    2: {\n",
    "        \"mean\": [1.0, 2.0],  # Mean vector for class 2\n",
    "        \"cov\": [\n",
    "            [1.2, 0.4],\n",
    "            [0.4, 1.3],\n",
    "            # [0.3, 0.5, 1.4]\n",
    "        ],  # Covariance matrix for class 2\n",
    "        \"prior\": 0.3  # Prior probability for class 2\n",
    "    },\n",
    "}\n",
    "\n",
    "# Initialize and fit the generator\n",
    "class_params_3d_3c = {\n",
    "    0: {\n",
    "        \"mean\": [3.0, 2.0, 4.0],  # Mean vector for class 0\n",
    "        \"cov\": [\n",
    "            [1.0, 0.5, 0.3],\n",
    "            [0.5, 1.2, 0.4],\n",
    "            [0.3, 0.4, 0.8]\n",
    "        ],  # Covariance matrix for class 0\n",
    "        \"prior\": 0.3  # Prior probability for class 0\n",
    "    },\n",
    "    1: {\n",
    "        \"mean\": [3.0, 4.0, 1.0],  # Mean vector for class 1\n",
    "        \"cov\": [\n",
    "            [1.5, 0.3, 0.2],\n",
    "            [0.3, 1.1, 0.1],\n",
    "            [0.2, 0.1, 0.9]\n",
    "        ],  # Covariance matrix for class 1\n",
    "        \"prior\": 0.4  # Prior probability for class 1\n",
    "    },\n",
    "    2: {\n",
    "        \"mean\": [1.0, 2.0, 2.0],  # Mean vector for class 2\n",
    "        \"cov\": [\n",
    "            [1.2, 0.4, 0.3],\n",
    "            [0.4, 1.3, 0.5],\n",
    "            [0.3, 0.5, 1.4]\n",
    "        ],  # Covariance matrix for class 2\n",
    "        \"prior\": 0.3  # Prior probability for class 2\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize and fit the generator\n",
    "class_params_2d_2c = {\n",
    "    0: {\n",
    "        \"mean\": [3.0, 2.0],  # Mean vector for class 0\n",
    "        \"cov\": [\n",
    "            [1.0, 0.5],\n",
    "            [0.5, 1.2],\n",
    "        ],  # Covariance matrix for class 0\n",
    "        \"prior\": 0.3  # Prior probability for class 0\n",
    "    },\n",
    "    1: {\n",
    "        \"mean\": [2.0, 3.0],  # Mean vector for class 1\n",
    "        \"cov\": [\n",
    "            [1.5, 0.3],\n",
    "            [0.3, 1.1],\n",
    "        ],  # Covariance matrix for class 1\n",
    "        \"prior\": 0.4  # Prior probability for class 1\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "clf_1d_2c = TorchGaussianSyntheticClassifier(class_params=class_params_1f_2c, device=\"cuda\")\n",
    "clf_1d_3c = TorchGaussianSyntheticClassifier(class_params=class_params_1f_3c, device=\"cuda\")\n",
    "clf_3d_3c = TorchMultivariateGaussianClassifier(class_params=class_params_3d_3c, device=\"cuda\")\n",
    "clf_2d_2c = TorchMultivariateGaussianClassifier(class_params=class_params_2d_2c, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OracleAnnotator:\n",
    "    def __init__(self,score, generator):\n",
    "        self.score = score\n",
    "        self.classes_ = generator.classes_\n",
    "        self.generator = generator\n",
    "\n",
    "    # we assume y is already label encoded\n",
    "    def get_conformity(self, X, y):\n",
    "        y_pred_proba = self.generator.predict_proba(X)\n",
    "        scores = self.score(y_pred_proba, y)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zmq import device\n",
    "from util.ranking_datasets import LabelPairDataset\n",
    "from models.ranking_models import LabelRankingModel\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from scipy.stats import kendalltau\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.stats import kendalltau\n",
    "from joblib import Parallel, delayed\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "def fit_model_with_all_pairs(X_train, y_train, oracle_annotator, generator, learning_rate = 0.01, num_epochs=200, X_val = None, y_val = None):\n",
    "    conformities = oracle_annotator.get_conformity(torch.tensor(X_train, device=\"cuda\"),torch.tensor(y_train,device=\"cuda\")).detach().cpu().numpy()\n",
    "\n",
    "    sort_idx = (-conformities).argsort(axis=0).flatten()\n",
    "    X_sorted = X_train[sort_idx]\n",
    "    X_sorted = X_sorted.detach().cpu().numpy()\n",
    "    y_sorted = y_train[sort_idx]\n",
    "    conformities_sorted = conformities[sort_idx]\n",
    "    \n",
    "    X_pairs = np.array([(X_sorted[i], X_sorted[j]) for i in range(len(X_sorted)) for j in range(i + 1, len(X_sorted))])\n",
    "    y_pairs = np.array([(y_sorted[i], y_sorted[j]) for i in range(len(y_sorted)) for j in range(i + 1, len(y_sorted))])\n",
    "    conformity_pairs = np.array([(conformities_sorted[i], conformities_sorted[j]) for i in range(len(conformities_sorted)) for j in range(i + 1, len(conformities_sorted))])\n",
    "    conformity_pairs = conformity_pairs.round(6)\n",
    "    mask = conformity_pairs[:,0] == conformity_pairs[:,1]\n",
    "    \n",
    "    X_pairs_distinct = X_pairs[~mask]\n",
    "    y_pairs_distinct = y_pairs[~mask]\n",
    "    X_pairs_nondistinct = X_pairs[mask]\n",
    "    y_pairs_nondistinct = y_pairs[mask]\n",
    "    X_pairs_nondistinct_swp = X_pairs_nondistinct[:,::-1]\n",
    "    y_pairs_nondistinct_swp = y_pairs_nondistinct[:,::-1]\n",
    "\n",
    "    X_pairs_augmented = np.vstack((X_pairs_distinct, X_pairs_nondistinct, X_pairs_nondistinct_swp))\n",
    "    y_pairs_augmented = np.vstack((y_pairs_distinct, y_pairs_nondistinct, y_pairs_nondistinct_swp))\n",
    "\n",
    "    y_pairs_augmented = np.expand_dims(y_pairs_augmented,axis=-1)\n",
    "\n",
    "    ds = LabelPairDataset()\n",
    "    ds.create_from_numpy_pairs(X_pairs_augmented, y_pairs_augmented)\n",
    "\n",
    "\n",
    "    # validation data\n",
    "    if X_val is not None:\n",
    "        conformities = oracle_annotator.get_conformity(torch.tensor(X_train, device=\"cuda\"),torch.tensor(y_train,device=\"cuda\")).detach().cpu().numpy()\n",
    "\n",
    "        sort_idx = (-conformities).argsort(axis=0).flatten()\n",
    "        X_sorted = X_train[sort_idx]\n",
    "        X_sorted = X_sorted.detach().cpu().numpy()\n",
    "        y_sorted = y_train[sort_idx]\n",
    "        conformities_sorted = conformities[sort_idx]\n",
    "        \n",
    "        X_pairs = np.array([(X_sorted[i], X_sorted[j]) for i in range(len(X_sorted)) for j in range(i + 1, len(X_sorted))])\n",
    "        y_pairs = np.array([(y_sorted[i], y_sorted[j]) for i in range(len(y_sorted)) for j in range(i + 1, len(y_sorted))])\n",
    "        conformity_pairs = np.array([(conformities_sorted[i], conformities_sorted[j]) for i in range(len(conformities_sorted)) for j in range(i + 1, len(conformities_sorted))])\n",
    "        conformity_pairs = conformity_pairs.round(6)\n",
    "        mask = conformity_pairs[:,0] == conformity_pairs[:,1]\n",
    "        \n",
    "        X_pairs_distinct = X_pairs[~mask]\n",
    "        y_pairs_distinct = y_pairs[~mask]\n",
    "        X_pairs_nondistinct = X_pairs[mask]\n",
    "        y_pairs_nondistinct = y_pairs[mask]\n",
    "        X_pairs_nondistinct_swp = X_pairs_nondistinct[:,::-1]\n",
    "        y_pairs_nondistinct_swp = y_pairs_nondistinct[:,::-1]\n",
    "\n",
    "        X_pairs_augmented = np.vstack((X_pairs_distinct, X_pairs_nondistinct, X_pairs_nondistinct_swp))\n",
    "        y_pairs_augmented = np.vstack((y_pairs_distinct, y_pairs_nondistinct, y_pairs_nondistinct_swp))\n",
    "\n",
    "        y_pairs_augmented = np.expand_dims(y_pairs_augmented,axis=-1)\n",
    "\n",
    "        ds_val = LabelPairDataset()\n",
    "        ds_val.create_from_numpy_pairs(X_pairs_augmented, y_pairs_augmented)\n",
    "        val_loader = DataLoader(ds_val, batch_size=64)\n",
    "    else:\n",
    "        val_loader = DataLoader(ds, batch_size=64)\n",
    "    pair_loader = DataLoader(ds, batch_size=64)\n",
    "\n",
    "    model = LabelRankingModel(input_dim=generator.n_features, hidden_dims=2*[5*generator.n_features], activations=[torch.nn.Sigmoid(), torch.nn.Sigmoid()], output_dim=len(generator.classes_))\n",
    "    model.num_classes = generator.n_classes\n",
    "    model.cuda()\n",
    "    model._fit(pair_loader, val_loader=val_loader, num_epochs=num_epochs, learning_rate=learning_rate, patience=20, verbose=True)\n",
    "    return model\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from scipy.stats import kendalltau\n",
    "def train_model(X_train, y_train, oracle_annotator, generator, learning_rate, num_epochs, X_val=None, y_val=None):\n",
    "    \"\"\"Trains a model with the given dataset and oracle.\"\"\"\n",
    "    return fit_model_with_all_pairs(X_train, y_train, oracle_annotator, generator, learning_rate=learning_rate, num_epochs=num_epochs, X_val=X_val, y_val=y_val)\n",
    "\n",
    "def evaluate_model(model, oracle, name, X_test, y_test, taus, gammas):\n",
    "    \"\"\"Evaluates the trained model and computes correlation scores.\"\"\"\n",
    "    skills = np.take_along_axis(\n",
    "        model.predict_class_skills(X_test),\n",
    "        y_test[:, np.newaxis].detach().cpu().numpy(),\n",
    "        axis=1\n",
    "    )\n",
    "    conformities = oracle.get_conformity(torch.tensor(X_test,device=\"cuda\"), torch.tensor(y_test,device=\"cuda\")).detach().cpu().numpy()\n",
    "    \n",
    "    tau_corr, _ = kendalltau(skills.detach().cpu().numpy(), conformities.detach().cpu().numpy())\n",
    "    gamma_corr = goodman_kruskal_gamma(skills.detach().cpu().numpy(), conformities.detach().cpu().numpy())\n",
    "\n",
    "    taus[name].append(tau_corr)\n",
    "    gammas[name].append(gamma_corr)\n",
    "\n",
    "\n",
    "def conduct_oracle_experiment(num_instances_to_check, generator, learning_rate=0.01, num_epochs=250):\n",
    "    \"\"\"Conducts an oracle experiment with parallelized training and evaluation.\"\"\"\n",
    "    \n",
    "    oracle_annotator_aps = OracleAnnotator(generator=generator, score=aps)\n",
    "    oracle_annotator_lac = OracleAnnotator(generator=generator, score=lac)\n",
    "    oracle_annotator_rand_aps = OracleAnnotator(generator=generator, score=rand_aps)\n",
    "\n",
    "    X_test, y_test = generator.generate_data(n_samples=100)\n",
    "    X_test, y_test = X_test.to(\"cuda\"), y_test.to(\"cuda\")\n",
    "    taus = {\"lac\": [], \"aps\": [], \"own_aps\": [], \"rand_aps\": [], \"own_rand_aps\": []}\n",
    "    gammas = {\"lac\": [], \"aps\": [], \"own_aps\": [], \"rand_aps\": [], \"own_rand_aps\": []}\n",
    "\n",
    "    for num_instances in num_instances_to_check:\n",
    "        X_gen, _ = generator.generate_data(n_samples=num_instances)\n",
    "        X_train = X_gen.repeat_interleave(generator.n_classes, dim=0)\n",
    "        y_train = np.tile(generator.classes_.detach().cpu().numpy(), num_instances)\n",
    "        X_gen, _ = generator.generate_data(n_samples=num_instances)\n",
    "        X_val = X_gen.repeat_interleave(generator.n_classes, dim=0)\n",
    "        y_val = np.tile(generator.classes_.detach().cpu().numpy(), num_instances)\n",
    "\n",
    "        # --- Parallel Model Training ---\n",
    "        models = Parallel(n_jobs=1, backend=\"loky\")(\n",
    "            delayed(train_model)(X_train, y_train, oracle, generator, learning_rate, num_epochs, X_val, y_val)\n",
    "            for oracle in [oracle_annotator_lac, oracle_annotator_aps, oracle_annotator_rand_aps]\n",
    "        )\n",
    "        model_lac, model_aps, model_rand_aps = models\n",
    "\n",
    "        models = [model_lac, model_aps, model_rand_aps]\n",
    "        oracles = [oracle_annotator_lac, oracle_annotator_aps, oracle_annotator_rand_aps]\n",
    "        names = [\"lac\", \"aps\", \"rand_aps\"]\n",
    "\n",
    "        # --- Parallel Model Evaluation ---\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [\n",
    "                executor.submit(evaluate_model, model, oracle, name, X_test, y_test, taus, gammas)\n",
    "                for model, oracle, name in zip(models, oracles, names)\n",
    "            ]\n",
    "            for future in futures:\n",
    "                future.result()  # Ensure completion\n",
    "\n",
    "        skills_from_model = model_lac(X_test)\n",
    "        own_lac = torch.take_along_dim(skills_from_model, y_test.unsqueeze(-1), dim=1).detach().cpu().numpy()\n",
    "        y_test = torch.tensor(y_test, device=\"cuda\")\n",
    "        skills_from_model = -skills_from_model\n",
    "        skills_from_model = (skills_from_model - skills_from_model.min()) / (skills_from_model.max() - skills_from_model.min()) \n",
    "        own_aps = aps._calculate_single_label(torch.tensor(skills_from_model), y_test).detach().cpu().numpy()\n",
    "        aps_scores = oracle_annotator_aps.get_conformity(X_test, y_test).detach().cpu().numpy()\n",
    "\n",
    "        tau_corr, p_value = kendalltau(own_aps, aps_scores)\n",
    "        gamma_corr = goodman_kruskal_gamma(own_aps,aps_scores)\n",
    "        taus[\"own_aps\"].append(tau_corr)\n",
    "        gammas[\"own_aps\"].append(gamma_corr)\n",
    "        # randomized APS reconstructed\n",
    "        own_rand_aps = rand_aps._calculate_single_label(-torch.tensor(skills_from_model), y_test).detach().cpu().numpy()\n",
    "        # own_aps = np.take_along_axis(own_aps, y_test.detach().numpy()[:,np.newaxis], axis=1)\n",
    "        rand_aps_scores = oracle_annotator_rand_aps.get_conformity(X_test, y_test).detach().cpu().numpy()\n",
    "        tau_corr, p_value = kendalltau(own_rand_aps, rand_aps_scores)\n",
    "        gamma_corr = goodman_kruskal_gamma(own_aps,aps_scores)\n",
    "        taus[\"own_rand_aps\"].append(tau_corr)\n",
    "        gammas[\"own_rand_aps\"].append(gamma_corr)\n",
    "\n",
    "\n",
    "    return taus, gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ra43rid/torch_plnet/venv/lib/python3.10/site-packages/torch/utils/_device.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  Train Loss: 0.1074\n",
      "  Val Loss: 0.0920\n",
      "Epoch 2/10\n",
      "  Train Loss: 0.0888\n",
      "  Val Loss: 0.0845\n",
      "Epoch 3/10\n",
      "  Train Loss: 0.0842\n",
      "  Val Loss: 0.0829\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.0830\n",
      "  Val Loss: 0.0815\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.0810\n",
      "  Val Loss: 0.0788\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.0778\n",
      "  Val Loss: 0.0753\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.0742\n",
      "  Val Loss: 0.0714\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.0698\n",
      "  Val Loss: 0.0665\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.0646\n",
      "  Val Loss: 0.0609\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.0589\n",
      "  Val Loss: 0.0553\n",
      "Epoch 1/10\n",
      "  Train Loss: 0.0895\n",
      "  Val Loss: 0.0848\n",
      "Epoch 2/10\n",
      "  Train Loss: 0.0851\n",
      "  Val Loss: 0.0841\n",
      "Epoch 3/10\n",
      "  Train Loss: 0.0841\n",
      "  Val Loss: 0.0835\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.0836\n",
      "  Val Loss: 0.0829\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.0831\n",
      "  Val Loss: 0.0820\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.0823\n",
      "  Val Loss: 0.0810\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.0812\n",
      "  Val Loss: 0.0796\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.0799\n",
      "  Val Loss: 0.0780\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.0783\n",
      "  Val Loss: 0.0762\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.0766\n",
      "  Val Loss: 0.0745\n",
      "Epoch 1/10\n",
      "  Train Loss: 0.1044\n",
      "  Val Loss: 0.0936\n",
      "Epoch 2/10\n",
      "  Train Loss: 0.0938\n",
      "  Val Loss: 0.0916\n",
      "Epoch 3/10\n",
      "  Train Loss: 0.0915\n",
      "  Val Loss: 0.0912\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.0905\n",
      "  Val Loss: 0.0905\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.0895\n",
      "  Val Loss: 0.0894\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.0885\n",
      "  Val Loss: 0.0881\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.0875\n",
      "  Val Loss: 0.0865\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.0861\n",
      "  Val Loss: 0.0846\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.0843\n",
      "  Val Loss: 0.0824\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.0821\n",
      "  Val Loss: 0.0797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ra43rid/torch_plnet/models/ranking_models.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  skills = self.forward(torch.tensor(X, dtype=torch.float32))\n",
      "/tmp/ipykernel_1471486/1672167741.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  conformities = oracle.get_conformity(torch.tensor(X_test,device=\"cuda\"), torch.tensor(y_test,device=\"cuda\")).detach().cpu().numpy()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m num_instances_to_check \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m50\u001b[39m,\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# num_instances_to_check = [40]\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m taus, gammas \u001b[38;5;241m=\u001b[39m \u001b[43mconduct_oracle_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_instances_to_check\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_instances_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 147\u001b[0m, in \u001b[0;36mconduct_oracle_experiment\u001b[0;34m(num_instances_to_check, generator, learning_rate, num_epochs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    143\u001b[0m         executor\u001b[38;5;241m.\u001b[39msubmit(evaluate_model, model, oracle, name, X_test, y_test, taus, gammas)\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m model, oracle, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(models, oracles, names)\n\u001b[1;32m    145\u001b[0m     ]\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[0;32m--> 147\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure completion\u001b[39;00m\n\u001b[1;32m    149\u001b[0m skills_from_model \u001b[38;5;241m=\u001b[39m model_lac(X_test)\n\u001b[1;32m    150\u001b[0m own_lac \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtake_along_dim(skills_from_model, y_test\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[0;32mIn[16], line 102\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, oracle, name, X_test, y_test, taus, gammas)\u001b[0m\n\u001b[1;32m     95\u001b[0m skills \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtake_along_axis(\n\u001b[1;32m     96\u001b[0m     model\u001b[38;5;241m.\u001b[39mpredict_class_skills(X_test),\n\u001b[1;32m     97\u001b[0m     y_test[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     98\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    100\u001b[0m conformities \u001b[38;5;241m=\u001b[39m oracle\u001b[38;5;241m.\u001b[39mget_conformity(torch\u001b[38;5;241m.\u001b[39mtensor(X_test,device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m), torch\u001b[38;5;241m.\u001b[39mtensor(y_test,device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m--> 102\u001b[0m tau_corr, _ \u001b[38;5;241m=\u001b[39m kendalltau(skills\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[43mconformities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    103\u001b[0m gamma_corr \u001b[38;5;241m=\u001b[39m goodman_kruskal_gamma(skills\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), conformities\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    105\u001b[0m taus[name]\u001b[38;5;241m.\u001b[39mappend(tau_corr)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "# X_train, y_train = make_classification(\n",
    "#     n_samples=100, n_features=3, n_classes=3, n_informative=3, n_redundant=0, n_repeated=0, n_clusters_per_class=1, random_state=42\n",
    "# )\n",
    "\n",
    "clf = clf_3d_3c\n",
    "\n",
    "clf.fit(None, None)\n",
    "num_instances_to_check = np.linspace(10,50,5).astype(int)\n",
    "# num_instances_to_check = [40]\n",
    "\n",
    "taus, gammas = conduct_oracle_experiment(num_instances_to_check=num_instances_to_check,generator=clf,learning_rate=0.01, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'taus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo. Instances\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# ax.set_ylim([0.0,1])\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot(x\u001b[38;5;241m=\u001b[39mnum_instances_to_check, y\u001b[38;5;241m=\u001b[39m\u001b[43mtaus\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlac\u001b[39m\u001b[38;5;124m\"\u001b[39m], ax \u001b[38;5;241m=\u001b[39m ax, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m\"\u001b[39m,label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAC\u001b[39m\u001b[38;5;124m\"\u001b[39m, legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot(x\u001b[38;5;241m=\u001b[39mnum_instances_to_check, y\u001b[38;5;241m=\u001b[39mtaus[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maps\u001b[39m\u001b[38;5;124m\"\u001b[39m], ax \u001b[38;5;241m=\u001b[39m ax, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m\"\u001b[39m,label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPS\u001b[39m\u001b[38;5;124m\"\u001b[39m, legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot(x\u001b[38;5;241m=\u001b[39mnum_instances_to_check, y\u001b[38;5;241m=\u001b[39mtaus[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mown_aps\u001b[39m\u001b[38;5;124m\"\u001b[39m], ax \u001b[38;5;241m=\u001b[39m ax, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m\"\u001b[39m,label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReconstructed APS\u001b[39m\u001b[38;5;124m\"\u001b[39m, legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'taus' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5,rc={'text.usetex' : True})\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rc('font', **{'family': 'serif'})\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 3)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title(\"\")\n",
    "ax.set_ylabel(r\"Kendall's $\\tau$-b\")\n",
    "ax.set_xlabel(r\"No. Instances\")\n",
    "# ax.set_ylim([0.0,1])\n",
    "sns.lineplot(x=num_instances_to_check, y=taus[\"lac\"], ax = ax, marker=\"o\",label=\"LAC\", legend=False)\n",
    "sns.lineplot(x=num_instances_to_check, y=taus[\"aps\"], ax = ax, marker=\"o\",label=\"APS\", legend=False)\n",
    "sns.lineplot(x=num_instances_to_check, y=taus[\"own_aps\"], ax = ax, marker=\"o\",label=\"Reconstructed APS\", legend=False, linestyle=\"--\")\n",
    "# sns.lineplot(x=num_instances_to_check, y=taus[\"rand_aps\"], ax = ax, marker=\"o\",label=\"APS\", legend=False)\n",
    "# sns.lineplot(x=num_instances_to_check, y=taus[\"own_rand_aps\"], ax = ax, marker=\"o\",label=\"APS\", legend=False)\n",
    "# sns.lineplot(x=num_instances_to_check, y=tau_corrs_own_aps, ax = ax, marker=\"o\",label=\"Reconstructed APS\", linestyle=\"--\", legend=False)\n",
    "# sns.lineplot(x=num_instances_to_check, y=tau_corrs_SAPS, ax = ax, marker=\"o\", label=\"SAPS\", legend=False)\n",
    "lgd = fig.legend(loc='upper center', ncol=3, bbox_to_anchor=(0.5, 0.08), frameon=False)\n",
    "\n",
    "fig.tight_layout() \n",
    "plt.savefig(\"replicating.pdf\",bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "# axes[1].set_title(\"APS\")\n",
    "# axes[1].set_ylabel(r\"Kendalls $\\tau$\")\n",
    "# axes[1].set_xlabel(r\"No. Pairs\")\n",
    "# sns.lineplot(x=num_pairs_to_check, y=tau_corrs_APS, ax = axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_lac' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m skills_from_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_lac\u001b[49m(X_test)\n\u001b[0;32m      2\u001b[0m y_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_test, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m skills_from_model \u001b[38;5;241m=\u001b[39m skills_from_model \u001b[38;5;241m-\u001b[39m skills_from_model\u001b[38;5;241m.\u001b[39mmin()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_lac' is not defined"
     ]
    }
   ],
   "source": [
    "skills_from_model = model_lac(X_test)\n",
    "y_test = torch.tensor(y_test, device=\"cuda\")\n",
    "skills_from_model = skills_from_model - skills_from_model.min()\n",
    "skills_from_model = skills_from_model / (skills_from_model.max() - skills_from_model.min())\n",
    "own_aps = aps._calculate_single_label(-torch.tensor(skills_from_model), y_test).detach().cpu().numpy()\n",
    "y_test = torch.tensor(y_test, device=\"cuda\")\n",
    "own_aps = aps._calculate_single_label(-torch.tensor(skills_from_model), y_test).detach().cpu().numpy()\n",
    "# own_aps = np.take_along_axis(own_aps, y_test.detach().numpy()[:,np.newaxis], axis=1)\n",
    "aps_scores = oracle_annotator_aps.get_conformity(X_test, y_test).detach().cpu().numpy()\n",
    "tau_corr, p_value = kendalltau(own_aps, aps_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
