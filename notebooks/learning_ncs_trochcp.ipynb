{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ra43rid/torch_plnet/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions import Normal\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(\"/home/ra43rid/torch_plnet\"))\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "class TorchMultivariateGaussianClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, class_params=None, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize the classifier with class parameters.\n",
    "\n",
    "        Parameters:\n",
    "        class_params: dict\n",
    "            A dictionary where keys are class labels and values are dictionaries with\n",
    "            'mean' (vector), 'cov' (matrix), and 'prior' for each class.\n",
    "        device: str\n",
    "            The device to use for computations ('cpu' or 'cuda').\n",
    "        \"\"\"\n",
    "        self.class_params = class_params if class_params is not None else {}\n",
    "        self.device = torch.device(device)\n",
    "        self.classes_ = torch.tensor(list(self.class_params.keys()), device=self.device)\n",
    "        self.n_features = len(self.class_params[0][\"cov\"])\n",
    "        self.n_classes = len(self.classes_)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method for compatibility. This classifier doesn't require fitting.\n",
    "        \"\"\"\n",
    "        self.classes_ = torch.tensor(list(self.class_params.keys()), device=self.device)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict the probability of each class for the given input data X.\n",
    "\n",
    "        Parameters:\n",
    "        X: torch.Tensor or array-like of shape (n_samples, n_features)\n",
    "            Input features.\n",
    "\n",
    "        Returns:\n",
    "        probs: torch.Tensor of shape (n_samples, n_classes)\n",
    "            Predicted probabilities for each class.\n",
    "        \"\"\"\n",
    "        if not isinstance(X, torch.Tensor):\n",
    "            X = torch.tensor(X, device=self.device, dtype=torch.float32)\n",
    "        \n",
    "        probs = torch.zeros((X.shape[0], len(self.classes_)), device=self.device)\n",
    "        for i, c in enumerate(self.classes_):\n",
    "            mean = torch.tensor(self.class_params[int(c)][\"mean\"], device=self.device, dtype=torch.float32)\n",
    "            cov = torch.tensor(self.class_params[int(c)][\"cov\"], device=self.device, dtype=torch.float32)\n",
    "            prior = self.class_params[int(c)][\"prior\"]\n",
    "            \n",
    "            # Multivariate normal distribution\n",
    "            mvn_dist = MultivariateNormal(mean, covariance_matrix=cov)\n",
    "            px_given_y = torch.exp(mvn_dist.log_prob(X))\n",
    "            \n",
    "            # Combine with prior\n",
    "            probs[:, i] = px_given_y * prior\n",
    "        \n",
    "        # Normalize to get P(y=c|x)\n",
    "        probs /= probs.sum(dim=1, keepdim=True)\n",
    "        return probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class label for each sample in X.\n",
    "\n",
    "        Parameters:\n",
    "        X: torch.Tensor or array-like of shape (n_samples, n_features)\n",
    "            Input features.\n",
    "\n",
    "        Returns:\n",
    "        predictions: torch.Tensor of shape (n_samples,)\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.classes_[torch.argmax(probs, dim=1)]\n",
    "    \n",
    "    def generate_data(self, n_samples=100):\n",
    "        \"\"\"\n",
    "        Generate synthetic data using the predefined class parameters.\n",
    "\n",
    "        Parameters:\n",
    "        n_samples: int\n",
    "            Number of samples to generate.\n",
    "\n",
    "        Returns:\n",
    "        X: torch.Tensor of shape (n_samples, n_features)\n",
    "            Generated features.\n",
    "        y: torch.Tensor of shape (n_samples,)\n",
    "            Generated labels.\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        y = []\n",
    "        for _ in range(n_samples):\n",
    "            # Sample class based on priors\n",
    "            sampled_class = torch.multinomial(\n",
    "                torch.tensor([self.class_params[int(c)][\"prior\"] for c in self.classes_], device=self.device),\n",
    "                num_samples=1\n",
    "            ).item()\n",
    "            mean = torch.tensor(self.class_params[int(self.classes_[sampled_class])][\"mean\"], device=self.device, dtype=torch.float32)\n",
    "            cov = torch.tensor(self.class_params[int(self.classes_[sampled_class])][\"cov\"], device=self.device, dtype=torch.float32)\n",
    "            \n",
    "            # Sample feature vector from the corresponding multivariate Gaussian\n",
    "            mvn_dist = MultivariateNormal(mean, covariance_matrix=cov)\n",
    "            sampled_x = mvn_dist.sample()\n",
    "            X.append(sampled_x)\n",
    "            y.append(self.classes_[sampled_class].item())\n",
    "        \n",
    "        return torch.stack(X), torch.tensor(y, device=self.device)\n",
    "\n",
    "class TorchGaussianSyntheticClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, class_params=None, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize the classifier with class parameters.\n",
    "\n",
    "        Parameters:\n",
    "        class_params: dict\n",
    "            A dictionary where keys are class labels and values are dictionaries with\n",
    "            'mean', 'std', and 'prior' for each class.\n",
    "        device: str\n",
    "            The device to use for computations ('cpu' or 'cuda').\n",
    "        \"\"\"\n",
    "        self.class_params = class_params if class_params is not None else {}\n",
    "        self.device = torch.device(device)\n",
    "        self.classes_ = torch.tensor(list(self.class_params.keys()), device=self.device)\n",
    "        self.n_features = 1\n",
    "        self.n_classes = len(self.classes_)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method for compatibility. This classifier doesn't require fitting.\n",
    "        \"\"\"\n",
    "        self.classes_ = torch.tensor(list(self.class_params.keys()), device=self.device)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict the probability of each class for the given input data X.\n",
    "\n",
    "        Parameters:\n",
    "        X: torch.Tensor or array-like of shape (n_samples,)\n",
    "            Input features.\n",
    "\n",
    "        Returns:\n",
    "        probs: torch.Tensor of shape (n_samples, n_classes)\n",
    "            Predicted probabilities for each class.\n",
    "        \"\"\"\n",
    "        if not isinstance(X, torch.Tensor):\n",
    "            X = torch.tensor(X, device=self.device, dtype=torch.float32)\n",
    "        \n",
    "        probs = torch.zeros((len(X), len(self.classes_)), device=self.device)\n",
    "        for i, c in enumerate(self.classes_):\n",
    "            mean = self.class_params[int(c)][\"mean\"]\n",
    "            std = self.class_params[int(c)][\"std\"]\n",
    "            prior = self.class_params[int(c)][\"prior\"]\n",
    "            \n",
    "            # Calculate Gaussian PDF: P(x|y=c)\n",
    "            normal_dist = Normal(loc=mean, scale=std)\n",
    "            px_given_y = torch.exp(normal_dist.log_prob(X))\n",
    "            \n",
    "            # Combine with prior: P(x|y=c) * P(y=c)\n",
    "            probs[:, i] = px_given_y * prior\n",
    "        \n",
    "        # Normalize to get P(y=c|x)\n",
    "        probs /= probs.sum(dim=1, keepdim=True)\n",
    "        return probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class label for each sample in X.\n",
    "\n",
    "        Parameters:\n",
    "        X: torch.Tensor or array-like of shape (n_samples,)\n",
    "            Input features.\n",
    "\n",
    "        Returns:\n",
    "        predictions: torch.Tensor of shape (n_samples,)\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.classes_[torch.argmax(probs, dim=1)]\n",
    "\n",
    "    def generate_data(self, n_samples=100):\n",
    "        \"\"\"\n",
    "        Generate synthetic data using the predefined class parameters.\n",
    "\n",
    "        Parameters:\n",
    "        n_samples: int\n",
    "            Number of samples to generate.\n",
    "\n",
    "        Returns:\n",
    "        X: torch.Tensor of shape (n_samples,)\n",
    "            Generated features.\n",
    "        y: torch.Tensor of shape (n_samples,)\n",
    "            Generated labels.\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        y = []\n",
    "        for _ in range(n_samples):\n",
    "            # Sample class based on priors\n",
    "            sampled_class = torch.multinomial(\n",
    "                torch.tensor([self.class_params[int(c)][\"prior\"] for c in self.classes_], device=self.device),\n",
    "                num_samples=1\n",
    "            ).item()\n",
    "            # Sample feature value from the corresponding Gaussian\n",
    "            mean = self.class_params[int(self.classes_[sampled_class])][\"mean\"]\n",
    "            std = self.class_params[int(self.classes_[sampled_class])][\"std\"]\n",
    "            normal_dist = Normal(loc=mean, scale=std)\n",
    "            sampled_x = normal_dist.sample().item()\n",
    "            X.append(sampled_x)\n",
    "            y.append(self.classes_[sampled_class].item())\n",
    "        \n",
    "        return torch.tensor(X, device=self.device), torch.tensor(y, device=self.device)\n",
    "\n",
    "def goodman_kruskal_gamma(x, y):\n",
    "    \"\"\"\n",
    "    Compute Goodman and Kruskal's Gamma for two ordinal variables.\n",
    "    \n",
    "    Parameters:\n",
    "    x, y: Lists or arrays of ordinal data (same length)\n",
    "    \n",
    "    Returns:\n",
    "    gamma: Goodman and Kruskal's Gamma\n",
    "    \"\"\"\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"Both variables must have the same length.\")\n",
    "    \n",
    "    concordant = 0\n",
    "    discordant = 0\n",
    "    \n",
    "    n = len(x)\n",
    "    for i in range(n - 1):\n",
    "        for j in range(i + 1, n):\n",
    "            # Determine concordance or discordance\n",
    "            if (x[i] > x[j] and y[i] > y[j]) or (x[i] < x[j] and y[i] < y[j]):\n",
    "                concordant += 1\n",
    "            elif (x[i] > x[j] and y[i] < y[j]) or (x[i] < x[j] and y[i] > y[j]):\n",
    "                discordant += 1\n",
    "    \n",
    "    # Compute Gamma\n",
    "    if concordant + discordant == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "    gamma = (concordant - discordant) / (concordant + discordant)\n",
    "    return gamma\n",
    "\n",
    "from torchcp.classification.score import APS, THR, SAPS\n",
    "aps = APS(score_type=\"identity\", randomized=False)\n",
    "rand_aps = APS(score_type=\"identity\", randomized=True)\n",
    "lac = THR(score_type=\"identity\",)\n",
    "saps = SAPS(score_type=\"identity\",randomized=False)\n",
    "\n",
    "class_params_1f_2c = {\n",
    "    0: {\"mean\": 1, \"std\": 1, \"prior\": 0.3},\n",
    "    1: {\"mean\": 3, \"std\": 1, \"prior\": 0.4},\n",
    "    # 2: {\"mean\": 4, \"std\": 2.2, \"prior\": 0.3},\n",
    "}   \n",
    "\n",
    "class_params_1f_3c = {\n",
    "    0: {\"mean\": 1, \"std\": 1, \"prior\": 0.3},\n",
    "    1: {\"mean\": 3, \"std\": 1, \"prior\": 0.4},\n",
    "    2: {\"mean\": 4, \"std\": 2.2, \"prior\": 0.3},\n",
    "}   \n",
    "\n",
    "# Initialize and fit the generator\n",
    "class_params_2d_3c = {\n",
    "    0: {\n",
    "        \"mean\": [3.0, 2.0],  # Mean vector for class 0\n",
    "        \"cov\": [\n",
    "            [1.0, 0.5],\n",
    "            [0.5, 1.2],\n",
    "            # [0.3, 0.4, 0.8]\n",
    "        ],  # Covariance matrix for class 0\n",
    "        \"prior\": 0.3  # Prior probability for class 0\n",
    "    },\n",
    "    1: {\n",
    "        \"mean\": [3.0, 4.0],  # Mean vector for class 1\n",
    "        \"cov\": [\n",
    "            [1.5, 0.3],\n",
    "            [0.3, 1.1],\n",
    "            # [0.2, 0.1, 0.9]\n",
    "        ],  # Covariance matrix for class 1\n",
    "        \"prior\": 0.4  # Prior probability for class 1\n",
    "    },\n",
    "    2: {\n",
    "        \"mean\": [1.0, 2.0],  # Mean vector for class 2\n",
    "        \"cov\": [\n",
    "            [1.2, 0.4],\n",
    "            [0.4, 1.3],\n",
    "            # [0.3, 0.5, 1.4]\n",
    "        ],  # Covariance matrix for class 2\n",
    "        \"prior\": 0.3  # Prior probability for class 2\n",
    "    },\n",
    "}\n",
    "\n",
    "# Initialize and fit the generator\n",
    "class_params_3d_3c = {\n",
    "    0: {\n",
    "        \"mean\": [3.0, 2.0, 4.0],  # Mean vector for class 0\n",
    "        \"cov\": [\n",
    "            [1.0, 0.5, 0.3],\n",
    "            [0.5, 1.2, 0.4],\n",
    "            [0.3, 0.4, 0.8]\n",
    "        ],  # Covariance matrix for class 0\n",
    "        \"prior\": 0.3  # Prior probability for class 0\n",
    "    },\n",
    "    1: {\n",
    "        \"mean\": [3.0, 4.0, 1.0],  # Mean vector for class 1\n",
    "        \"cov\": [\n",
    "            [1.5, 0.3, 0.2],\n",
    "            [0.3, 1.1, 0.1],\n",
    "            [0.2, 0.1, 0.9]\n",
    "        ],  # Covariance matrix for class 1\n",
    "        \"prior\": 0.4  # Prior probability for class 1\n",
    "    },\n",
    "    2: {\n",
    "        \"mean\": [1.0, 2.0, 2.0],  # Mean vector for class 2\n",
    "        \"cov\": [\n",
    "            [1.2, 0.4, 0.3],\n",
    "            [0.4, 1.3, 0.5],\n",
    "            [0.3, 0.5, 1.4]\n",
    "        ],  # Covariance matrix for class 2\n",
    "        \"prior\": 0.3  # Prior probability for class 2\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize and fit the generator\n",
    "class_params_2d_2c = {\n",
    "    0: {\n",
    "        \"mean\": [3.0, 2.0],  # Mean vector for class 0\n",
    "        \"cov\": [\n",
    "            [1.0, 0.5],\n",
    "            [0.5, 1.2],\n",
    "        ],  # Covariance matrix for class 0\n",
    "        \"prior\": 0.3  # Prior probability for class 0\n",
    "    },\n",
    "    1: {\n",
    "        \"mean\": [2.0, 3.0],  # Mean vector for class 1\n",
    "        \"cov\": [\n",
    "            [1.5, 0.3],\n",
    "            [0.3, 1.1],\n",
    "        ],  # Covariance matrix for class 1\n",
    "        \"prior\": 0.4  # Prior probability for class 1\n",
    "    },\n",
    "    # 2: {\n",
    "    #     \"mean\": [1.0, 2.0, 2.0],  # Mean vector for class 2\n",
    "    #     \"cov\": [\n",
    "    #         [1.2, 0.4],\n",
    "    #         [0.4, 1.3],\n",
    "    #         # [0.3, 0.5, 1.4]\n",
    "    #     ],  # Covariance matrix for class 2\n",
    "    #     \"prior\": 0.3  # Prior probability for class 2\n",
    "    # },\n",
    "}\n",
    "\n",
    "clf_1d_2c = TorchGaussianSyntheticClassifier(class_params=class_params_1f_2c, device=\"cuda\")\n",
    "clf_1d_3c = TorchGaussianSyntheticClassifier(class_params=class_params_1f_3c, device=\"cuda\")\n",
    "clf_3d_3c = TorchMultivariateGaussianClassifier(class_params=class_params_3d_3c, device=\"cuda\")\n",
    "clf_2d_2c = TorchMultivariateGaussianClassifier(class_params=class_params_2d_2c, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OracleAnnotator:\n",
    "    def __init__(self,score, generator):\n",
    "        self.score = score\n",
    "        self.classes_ = generator.classes_\n",
    "        self.generator = generator\n",
    "\n",
    "    # we assume y is already label encoded\n",
    "    def get_conformity(self, X, y):\n",
    "        y_pred_proba = self.generator.predict_proba(X)\n",
    "        scores = self.score(y_pred_proba, y)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zmq import device\n",
    "from util.ranking_datasets import LabelPairDataset\n",
    "from models.ranking_models import LabelRankingModel\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from scipy.stats import kendalltau\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.stats import kendalltau\n",
    "from joblib import Parallel, delayed\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "def fit_model_with_all_pairs(X_train, y_train, oracle_annotator, generator, learning_rate = 0.01, num_epochs=200):\n",
    "    conformities = oracle_annotator.get_conformity(torch.tensor(X_train, device=\"cuda\"),torch.tensor(y_train,device=\"cuda\")).detach().cpu().numpy()\n",
    "\n",
    "    sort_idx = (-conformities).argsort(axis=0).flatten()\n",
    "    X_sorted = X_train[sort_idx]\n",
    "    X_sorted = X_sorted.detach().cpu().numpy()\n",
    "    y_sorted = y_train[sort_idx]\n",
    "    conformities_sorted = conformities[sort_idx]\n",
    "    \n",
    "    X_pairs = np.array([(X_sorted[i], X_sorted[j]) for i in range(len(X_sorted)) for j in range(i + 1, len(X_sorted))])\n",
    "    y_pairs = np.array([(y_sorted[i], y_sorted[j]) for i in range(len(y_sorted)) for j in range(i + 1, len(y_sorted))])\n",
    "    conformity_pairs = np.array([(conformities_sorted[i], conformities_sorted[j]) for i in range(len(conformities_sorted)) for j in range(i + 1, len(conformities_sorted))])\n",
    "    conformity_pairs = conformity_pairs.round(6)\n",
    "    mask = conformity_pairs[:,0] == conformity_pairs[:,1]\n",
    "    \n",
    "    X_pairs_distinct = X_pairs[~mask]\n",
    "    y_pairs_distinct = y_pairs[~mask]\n",
    "    X_pairs_nondistinct = X_pairs[mask]\n",
    "    y_pairs_nondistinct = y_pairs[mask]\n",
    "    X_pairs_nondistinct_swp = X_pairs_nondistinct[:,::-1]\n",
    "    y_pairs_nondistinct_swp = y_pairs_nondistinct[:,::-1]\n",
    "\n",
    "    X_pairs_augmented = np.vstack((X_pairs_distinct, X_pairs_nondistinct, X_pairs_nondistinct_swp))\n",
    "    y_pairs_augmented = np.vstack((y_pairs_distinct, y_pairs_nondistinct, y_pairs_nondistinct_swp))\n",
    "\n",
    "    y_pairs_augmented = np.expand_dims(y_pairs_augmented,axis=-1)\n",
    "\n",
    "    ds = LabelPairDataset()\n",
    "    ds.create_from_numpy_pairs(X_pairs_augmented, y_pairs_augmented)\n",
    "    pair_loader = DataLoader(ds, batch_size=64)\n",
    "    # ds_val = LabelPairDataset()\n",
    "    # ds_val.create_from_numpy_pairs(X_pairs, y_pairs)\n",
    "    # val_loader = DataLoader(ds_val, batch_size=64)\n",
    "    model = LabelRankingModel(input_dim=generator.n_features, hidden_dims=2*[10*generator.n_features], activations=[torch.nn.Sigmoid(), torch.nn.Sigmoid()], output_dim=len(generator.classes_))\n",
    "    model.num_classes = generator.n_classes\n",
    "    model._fit(pair_loader, val_loader=None, num_epochs=num_epochs, learning_rate=learning_rate, patience=num_epochs, verbose=True)\n",
    "    return model\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from scipy.stats import kendalltau\n",
    "def train_model(X_train, y_train, oracle_annotator, generator, learning_rate, num_epochs):\n",
    "    \"\"\"Trains a model with the given dataset and oracle.\"\"\"\n",
    "    return fit_model_with_all_pairs(X_train, y_train, oracle_annotator, generator, learning_rate=learning_rate, num_epochs=num_epochs)\n",
    "\n",
    "def evaluate_model(model, oracle, name, X_test, y_test, taus, gammas):\n",
    "    \"\"\"Evaluates the trained model and computes correlation scores.\"\"\"\n",
    "    skills = np.take_along_axis(\n",
    "        model.predict_class_skills(X_test),\n",
    "        y_test[:, np.newaxis].detach().cpu().numpy(),\n",
    "        axis=1\n",
    "    )\n",
    "    conformities = oracle.get_conformity(torch.tensor(X_test,device=\"cuda\"), torch.tensor(y_test,device=\"cuda\")).detach().cpu().numpy()\n",
    "    \n",
    "    tau_corr, _ = kendalltau(skills, conformities)\n",
    "    gamma_corr = goodman_kruskal_gamma(skills, conformities)\n",
    "\n",
    "    taus[name].append(tau_corr)\n",
    "    gammas[name].append(gamma_corr)\n",
    "\n",
    "\n",
    "def conduct_oracle_experiment(num_instances_to_check, generator, learning_rate=0.01, num_epochs=250):\n",
    "    \"\"\"Conducts an oracle experiment with parallelized training and evaluation.\"\"\"\n",
    "    \n",
    "    oracle_annotator_aps = OracleAnnotator(generator=generator, score=aps)\n",
    "    oracle_annotator_lac = OracleAnnotator(generator=generator, score=lac)\n",
    "    oracle_annotator_rand_aps = OracleAnnotator(generator=generator, score=rand_aps)\n",
    "\n",
    "    X_test, y_test = generator.generate_data(n_samples=100)\n",
    "    X_test, y_test = X_test.to(\"cuda\"), y_test.to(\"cuda\")\n",
    "    taus = {\"lac\": [], \"aps\": [], \"own_aps\": [], \"rand_aps\": [], \"own_rand_aps\": []}\n",
    "    gammas = {\"lac\": [], \"aps\": [], \"own_aps\": [], \"rand_aps\": [], \"own_rand_aps\": []}\n",
    "\n",
    "    for num_instances in num_instances_to_check:\n",
    "        X_gen, _ = generator.generate_data(n_samples=num_instances)\n",
    "        X_train = X_gen.repeat_interleave(generator.n_classes, dim=0)\n",
    "        y_train = np.tile(generator.classes_.detach().cpu().numpy(), num_instances)\n",
    "\n",
    "        # --- Parallel Model Training ---\n",
    "        models = Parallel(n_jobs=1, backend=\"loky\")(\n",
    "            delayed(train_model)(X_train, y_train, oracle, generator, learning_rate, num_epochs)\n",
    "            for oracle in [oracle_annotator_lac, oracle_annotator_aps, oracle_annotator_rand_aps]\n",
    "        )\n",
    "        model_lac, model_aps, model_rand_aps = models\n",
    "\n",
    "        models = [model_lac, model_aps, model_rand_aps]\n",
    "        oracles = [oracle_annotator_lac, oracle_annotator_aps, oracle_annotator_rand_aps]\n",
    "        names = [\"lac\", \"aps\", \"rand_aps\"]\n",
    "\n",
    "        # --- Parallel Model Evaluation ---\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [\n",
    "                executor.submit(evaluate_model, model, oracle, name, X_test, y_test, taus, gammas)\n",
    "                for model, oracle, name in zip(models, oracles, names)\n",
    "            ]\n",
    "            for future in futures:\n",
    "                future.result()  # Ensure completion\n",
    "\n",
    "        skills_from_model = model_lac(X_test)\n",
    "        own_lac = torch.take_along_dim(skills_from_model, y_test.unsqueeze(-1), dim=1).detach().cpu().numpy()\n",
    "        y_test = torch.tensor(y_test, device=\"cuda\")\n",
    "        skills_from_model = -skills_from_model\n",
    "        skills_from_model = (skills_from_model - skills_from_model.min()) / (skills_from_model.max() - skills_from_model.min()) \n",
    "        own_aps = aps._calculate_single_label(torch.tensor(skills_from_model), y_test).detach().cpu().numpy()\n",
    "        aps_scores = oracle_annotator_aps.get_conformity(X_test, y_test).detach().cpu().numpy()\n",
    "\n",
    "        tau_corr, p_value = kendalltau(own_aps, aps_scores)\n",
    "        gamma_corr = goodman_kruskal_gamma(own_aps,aps_scores)\n",
    "        taus[\"own_aps\"].append(tau_corr)\n",
    "        gammas[\"own_aps\"].append(gamma_corr)\n",
    "        # randomized APS reconstructed\n",
    "        own_rand_aps = rand_aps._calculate_single_label(-torch.tensor(skills_from_model), y_test).detach().cpu().numpy()\n",
    "        # own_aps = np.take_along_axis(own_aps, y_test.detach().numpy()[:,np.newaxis], axis=1)\n",
    "        rand_aps_scores = oracle_annotator_rand_aps.get_conformity(X_test, y_test).detach().cpu().numpy()\n",
    "        tau_corr, p_value = kendalltau(own_rand_aps, rand_aps_scores)\n",
    "        gamma_corr = goodman_kruskal_gamma(own_aps,aps_scores)\n",
    "        taus[\"own_rand_aps\"].append(tau_corr)\n",
    "        gammas[\"own_rand_aps\"].append(gamma_corr)\n",
    "\n",
    "\n",
    "    return taus, gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ra43rid/torch_plnet/venv/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "  Train Loss: 0.0084\n",
      "Epoch 2/500\n",
      "  Train Loss: 0.0051\n",
      "Epoch 3/500\n",
      "  Train Loss: 0.0027\n",
      "Epoch 4/500\n",
      "  Train Loss: 0.0021\n",
      "Epoch 5/500\n",
      "  Train Loss: 0.0018\n",
      "Epoch 6/500\n",
      "  Train Loss: 0.0017\n",
      "Epoch 7/500\n",
      "  Train Loss: 0.0015\n",
      "Epoch 8/500\n",
      "  Train Loss: 0.0014\n",
      "Epoch 9/500\n",
      "  Train Loss: 0.0013\n",
      "Epoch 10/500\n",
      "  Train Loss: 0.0013\n",
      "Epoch 11/500\n",
      "  Train Loss: 0.0012\n",
      "Epoch 12/500\n",
      "  Train Loss: 0.0012\n",
      "Epoch 13/500\n",
      "  Train Loss: 0.0011\n",
      "Epoch 14/500\n",
      "  Train Loss: 0.0011\n",
      "Epoch 15/500\n",
      "  Train Loss: 0.0011\n",
      "Epoch 16/500\n",
      "  Train Loss: 0.0010\n",
      "Epoch 17/500\n",
      "  Train Loss: 0.0010\n",
      "Epoch 18/500\n",
      "  Train Loss: 0.0009\n",
      "Epoch 19/500\n",
      "  Train Loss: 0.0009\n",
      "Epoch 20/500\n",
      "  Train Loss: 0.0009\n",
      "Epoch 21/500\n",
      "  Train Loss: 0.0009\n",
      "Epoch 22/500\n",
      "  Train Loss: 0.0008\n",
      "Epoch 23/500\n",
      "  Train Loss: 0.0008\n",
      "Epoch 24/500\n",
      "  Train Loss: 0.0008\n",
      "Epoch 25/500\n",
      "  Train Loss: 0.0008\n",
      "Epoch 26/500\n",
      "  Train Loss: 0.0008\n",
      "Epoch 27/500\n",
      "  Train Loss: 0.0008\n",
      "Epoch 28/500\n",
      "  Train Loss: 0.0007\n",
      "Epoch 29/500\n",
      "  Train Loss: 0.0007\n",
      "Epoch 30/500\n",
      "  Train Loss: 0.0007\n",
      "Epoch 31/500\n",
      "  Train Loss: 0.0007\n",
      "Epoch 32/500\n",
      "  Train Loss: 0.0007\n",
      "Epoch 33/500\n",
      "  Train Loss: 0.0007\n",
      "Epoch 34/500\n",
      "  Train Loss: 0.0007\n",
      "Epoch 35/500\n",
      "  Train Loss: 0.0007\n",
      "Epoch 36/500\n",
      "  Train Loss: 0.0007\n",
      "Epoch 37/500\n",
      "  Train Loss: 0.0006\n",
      "Epoch 38/500\n",
      "  Train Loss: 0.0006\n",
      "Epoch 39/500\n",
      "  Train Loss: 0.0006\n",
      "Epoch 40/500\n",
      "  Train Loss: 0.0006\n",
      "Epoch 41/500\n",
      "  Train Loss: 0.0006\n",
      "Epoch 42/500\n",
      "  Train Loss: 0.0006\n",
      "Epoch 43/500\n",
      "  Train Loss: 0.0006\n",
      "Epoch 44/500\n",
      "  Train Loss: 0.0006\n",
      "Epoch 45/500\n",
      "  Train Loss: 0.0006\n",
      "Epoch 46/500\n",
      "  Train Loss: 0.0006\n",
      "Epoch 47/500\n",
      "  Train Loss: 0.0006\n",
      "Epoch 48/500\n",
      "  Train Loss: 0.0006\n",
      "Epoch 49/500\n",
      "  Train Loss: 0.0006\n",
      "Epoch 50/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 51/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 52/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 53/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 54/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 55/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 56/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 57/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 58/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 59/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 60/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 61/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 62/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 63/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 64/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 65/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 66/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 67/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 68/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 69/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 70/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 71/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 72/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 73/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 74/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 75/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 76/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 77/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 78/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 79/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 80/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 81/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 82/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 83/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 84/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 85/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 86/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 87/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 88/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 89/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 90/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 91/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 92/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 93/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 94/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 95/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 96/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 97/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 98/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 99/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 100/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 101/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 102/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 103/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 104/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 105/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 106/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 107/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 108/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 109/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 110/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 111/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 112/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 113/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 114/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 115/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 116/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 117/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 118/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 119/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 120/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 121/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 122/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 123/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 124/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 125/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 126/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 127/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 128/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 129/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 130/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 131/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 132/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 133/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 134/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 135/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 136/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 137/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 138/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 139/500\n",
      "  Train Loss: 0.0020\n",
      "Epoch 140/500\n",
      "  Train Loss: 0.0010\n",
      "Epoch 141/500\n",
      "  Train Loss: 0.0006\n",
      "Epoch 142/500\n",
      "  Train Loss: 0.0005\n",
      "Epoch 143/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 144/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 145/500\n",
      "  Train Loss: 0.0004\n",
      "Epoch 146/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 147/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 148/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 149/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 150/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 151/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 152/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 153/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 154/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 155/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 156/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 157/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 158/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 159/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 160/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 161/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 162/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 163/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 164/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 165/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 166/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 167/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 168/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 169/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 170/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 171/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 172/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 173/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 174/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 175/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 176/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 177/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 178/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 179/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 180/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 181/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 182/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 183/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 184/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 185/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 186/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 187/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 188/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 189/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 190/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 191/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 192/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 193/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 194/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 195/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 196/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 197/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 198/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 199/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 200/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 201/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 202/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 203/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 204/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 205/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 206/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 207/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 208/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 209/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 210/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 211/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 212/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 213/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 214/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 215/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 216/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 217/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 218/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 219/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 220/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 221/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 222/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 223/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 224/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 225/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 226/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 227/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 228/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 229/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 230/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 231/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 232/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 233/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 234/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 235/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 236/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 237/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 238/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 239/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 240/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 241/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 242/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 243/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 244/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 245/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 246/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 247/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 248/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 249/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 250/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 251/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 252/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 253/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 254/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 255/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 256/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 257/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 258/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 259/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 260/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 261/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 262/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 263/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 264/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 265/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 266/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 267/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 268/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 269/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 270/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 271/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 272/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 273/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 274/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 275/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 276/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 277/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 278/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 279/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 280/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 281/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 282/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 283/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 284/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 285/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 286/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 287/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 288/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 289/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 290/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 291/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 292/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 293/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 294/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 295/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 296/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 297/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 298/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 299/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 300/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 301/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 302/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 303/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 304/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 305/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 306/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 307/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 308/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 309/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 310/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 311/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 312/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 313/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 314/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 315/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 316/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 317/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 318/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 319/500\n",
      "  Train Loss: 0.0003\n",
      "Epoch 320/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 321/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 322/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 323/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 324/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 325/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 326/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 327/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 328/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 329/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 330/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 331/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 332/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 333/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 334/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 335/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 336/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 337/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 338/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 339/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 340/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 341/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 342/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 343/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 344/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 345/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 346/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 347/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 348/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 349/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 350/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 351/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 352/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 353/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 354/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 355/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 356/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 357/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 358/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 359/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 360/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 361/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 362/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 363/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 364/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 365/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 366/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 367/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 368/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 369/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 370/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 371/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 372/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 373/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 374/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 375/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 376/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 377/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 378/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 379/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 380/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 381/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 382/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 383/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 384/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 385/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 386/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 387/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 388/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 389/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 390/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 391/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 392/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 393/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 394/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 395/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 396/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 397/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 398/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 399/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 400/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 401/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 402/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 403/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 404/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 405/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 406/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 407/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 408/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 409/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 410/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 411/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 412/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 413/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 414/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 415/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 416/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 417/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 418/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 419/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 420/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 421/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 422/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 423/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 424/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 425/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 426/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 427/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 428/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 429/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 430/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 431/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 432/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 433/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 434/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 435/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 436/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 437/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 438/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 439/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 440/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 441/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 442/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 443/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 444/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 445/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 446/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 447/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 448/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 449/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 450/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 451/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 452/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 453/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 454/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 455/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 456/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 457/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 458/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 459/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 460/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 461/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 462/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 463/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 464/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 465/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 466/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 467/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 468/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 469/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 470/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 471/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 472/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 473/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 474/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 475/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 476/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 477/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 478/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 479/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 480/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 481/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 482/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 483/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 484/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 485/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 486/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 487/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 488/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 489/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 490/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 491/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 492/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 493/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 494/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 495/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 496/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 497/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 498/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 499/500\n",
      "  Train Loss: 0.0002\n",
      "Epoch 500/500\n",
      "  Train Loss: 0.0002\n"
     ]
    }
   ],
   "source": [
    "clf = clf_2d_2c\n",
    "num_instances=50\n",
    "X_gen, _ = clf.generate_data(n_samples=num_instances)\n",
    "X_train = X_gen.repeat_interleave(clf.n_classes, dim=0)\n",
    "y_train = np.tile(clf.classes_.detach().cpu().numpy(), num_instances)\n",
    "oa_aps = OracleAnnotator(aps,clf)\n",
    "oa_lac = OracleAnnotator(lac,clf)\n",
    "model_lac = fit_model_with_all_pairs(X_train, y_train, oa_lac, clf, learning_rate=0.01, num_epochs=500)\n",
    "\n",
    "X_test, y_test = clf.generate_data(100)\n",
    "\n",
    "skills_from_model = model_lac(X_test)\n",
    "y_test = torch.tensor(y_test, device=\"cuda\")\n",
    "skills_from_model = skills_from_model - skills_from_model.min() / (skills_from_model.max() - skills_from_model.min())\n",
    "own_aps = aps._calculate_single_label(-torch.tensor(skills_from_model), y_test).detach().cpu().numpy()\n",
    "y_test = torch.tensor(y_test, device=\"cuda\")\n",
    "own_aps = aps._calculate_single_label(-torch.tensor(skills_from_model), y_test).detach().cpu().numpy()\n",
    "# own_aps = np.take_along_axis(own_aps, y_test.detach().numpy()[:,np.newaxis], axis=1)\n",
    "aps_scores = oa_aps.get_conformity(X_test, y_test).detach().cpu().numpy()\n",
    "tau_corr, p_value = kendalltau(own_aps, aps_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lac SignificanceResult(statistic=0.9644444444444447, pvalue=7.132677618335952e-46)\n",
      "aps SignificanceResult(statistic=0.8354205625729639, pvalue=6.718342507880482e-34)\n"
     ]
    }
   ],
   "source": [
    "skills_from_model = model_lac(X_test)\n",
    "own_lac = torch.take_along_dim(skills_from_model, y_test.unsqueeze(-1), dim=1).detach().cpu().numpy()\n",
    "y_test = torch.tensor(y_test, device=\"cuda\")\n",
    "skills_from_model = -skills_from_model\n",
    "skills_from_model = (skills_from_model - skills_from_model.min()) / (skills_from_model.max() - skills_from_model.min()) \n",
    "own_aps = aps._calculate_single_label(torch.tensor(skills_from_model), y_test).detach().cpu().numpy()\n",
    "# own_aps = np.take_along_axis(own_aps, y_test.detach().numpy()[:,np.newaxis], axis=1)\n",
    "aps_scores = oa_aps.get_conformity(X_test, y_test).detach().cpu().numpy()\n",
    "lac_scores = oa_lac.get_conformity(X_test, y_test).detach().cpu().numpy()\n",
    "print(\"lac\",kendalltau(own_lac, lac_scores)) \n",
    "print(\"aps\",kendalltau(own_aps, aps_scores)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_from_model = skills_from_model - skills_from_model.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "  Train Loss: 0.1021\n",
      "Epoch 2/1000\n",
      "  Train Loss: 0.0947\n",
      "Epoch 3/1000\n",
      "  Train Loss: 0.0926\n",
      "Epoch 4/1000\n",
      "  Train Loss: 0.0881\n",
      "Epoch 5/1000\n",
      "  Train Loss: 0.0826\n",
      "Epoch 6/1000\n",
      "  Train Loss: 0.0761\n",
      "Epoch 7/1000\n",
      "  Train Loss: 0.0694\n",
      "Epoch 8/1000\n",
      "  Train Loss: 0.0631\n",
      "Epoch 9/1000\n",
      "  Train Loss: 0.0577\n",
      "Epoch 10/1000\n",
      "  Train Loss: 0.0532\n",
      "Epoch 11/1000\n",
      "  Train Loss: 0.0494\n",
      "Epoch 12/1000\n",
      "  Train Loss: 0.0460\n",
      "Epoch 13/1000\n",
      "  Train Loss: 0.0432\n",
      "Epoch 14/1000\n",
      "  Train Loss: 0.0408\n",
      "Epoch 15/1000\n",
      "  Train Loss: 0.0387\n",
      "Epoch 16/1000\n",
      "  Train Loss: 0.0369\n",
      "Epoch 17/1000\n",
      "  Train Loss: 0.0353\n",
      "Epoch 18/1000\n",
      "  Train Loss: 0.0339\n",
      "Epoch 19/1000\n",
      "  Train Loss: 0.0326\n",
      "Epoch 20/1000\n",
      "  Train Loss: 0.0315\n",
      "Epoch 21/1000\n",
      "  Train Loss: 0.0305\n",
      "Epoch 22/1000\n",
      "  Train Loss: 0.0296\n",
      "Epoch 23/1000\n",
      "  Train Loss: 0.0288\n",
      "Epoch 24/1000\n",
      "  Train Loss: 0.0281\n",
      "Epoch 25/1000\n",
      "  Train Loss: 0.0274\n",
      "Epoch 26/1000\n",
      "  Train Loss: 0.0268\n",
      "Epoch 27/1000\n",
      "  Train Loss: 0.0263\n",
      "Epoch 28/1000\n",
      "  Train Loss: 0.0258\n",
      "Epoch 29/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 30/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 31/1000\n",
      "  Train Loss: 0.0244\n",
      "Epoch 32/1000\n",
      "  Train Loss: 0.0240\n",
      "Epoch 33/1000\n",
      "  Train Loss: 0.0236\n",
      "Epoch 34/1000\n",
      "  Train Loss: 0.0233\n",
      "Epoch 35/1000\n",
      "  Train Loss: 0.0229\n",
      "Epoch 36/1000\n",
      "  Train Loss: 0.0226\n",
      "Epoch 37/1000\n",
      "  Train Loss: 0.0222\n",
      "Epoch 38/1000\n",
      "  Train Loss: 0.0219\n",
      "Epoch 39/1000\n",
      "  Train Loss: 0.0216\n",
      "Epoch 40/1000\n",
      "  Train Loss: 0.0213\n",
      "Epoch 41/1000\n",
      "  Train Loss: 0.0210\n",
      "Epoch 42/1000\n",
      "  Train Loss: 0.0207\n",
      "Epoch 43/1000\n",
      "  Train Loss: 0.0204\n",
      "Epoch 44/1000\n",
      "  Train Loss: 0.0202\n",
      "Epoch 45/1000\n",
      "  Train Loss: 0.0199\n",
      "Epoch 46/1000\n",
      "  Train Loss: 0.0196\n",
      "Epoch 47/1000\n",
      "  Train Loss: 0.0194\n",
      "Epoch 48/1000\n",
      "  Train Loss: 0.0191\n",
      "Epoch 49/1000\n",
      "  Train Loss: 0.0189\n",
      "Epoch 50/1000\n",
      "  Train Loss: 0.0186\n",
      "Epoch 51/1000\n",
      "  Train Loss: 0.0184\n",
      "Epoch 52/1000\n",
      "  Train Loss: 0.0182\n",
      "Epoch 53/1000\n",
      "  Train Loss: 0.0180\n",
      "Epoch 54/1000\n",
      "  Train Loss: 0.0178\n",
      "Epoch 55/1000\n",
      "  Train Loss: 0.0176\n",
      "Epoch 56/1000\n",
      "  Train Loss: 0.0174\n",
      "Epoch 57/1000\n",
      "  Train Loss: 0.0172\n",
      "Epoch 58/1000\n",
      "  Train Loss: 0.0170\n",
      "Epoch 59/1000\n",
      "  Train Loss: 0.0168\n",
      "Epoch 60/1000\n",
      "  Train Loss: 0.0166\n",
      "Epoch 61/1000\n",
      "  Train Loss: 0.0164\n",
      "Epoch 62/1000\n",
      "  Train Loss: 0.0163\n",
      "Epoch 63/1000\n",
      "  Train Loss: 0.0161\n",
      "Epoch 64/1000\n",
      "  Train Loss: 0.0160\n",
      "Epoch 65/1000\n",
      "  Train Loss: 0.0158\n",
      "Epoch 66/1000\n",
      "  Train Loss: 0.0156\n",
      "Epoch 67/1000\n",
      "  Train Loss: 0.0155\n",
      "Epoch 68/1000\n",
      "  Train Loss: 0.0154\n",
      "Epoch 69/1000\n",
      "  Train Loss: 0.0152\n",
      "Epoch 70/1000\n",
      "  Train Loss: 0.0151\n",
      "Epoch 71/1000\n",
      "  Train Loss: 0.0149\n",
      "Epoch 72/1000\n",
      "  Train Loss: 0.0148\n",
      "Epoch 73/1000\n",
      "  Train Loss: 0.0147\n",
      "Epoch 74/1000\n",
      "  Train Loss: 0.0145\n",
      "Epoch 75/1000\n",
      "  Train Loss: 0.0144\n",
      "Epoch 76/1000\n",
      "  Train Loss: 0.0143\n",
      "Epoch 77/1000\n",
      "  Train Loss: 0.0142\n",
      "Epoch 78/1000\n",
      "  Train Loss: 0.0141\n",
      "Epoch 79/1000\n",
      "  Train Loss: 0.0139\n",
      "Epoch 80/1000\n",
      "  Train Loss: 0.0138\n",
      "Epoch 81/1000\n",
      "  Train Loss: 0.0137\n",
      "Epoch 82/1000\n",
      "  Train Loss: 0.0136\n",
      "Epoch 83/1000\n",
      "  Train Loss: 0.0135\n",
      "Epoch 84/1000\n",
      "  Train Loss: 0.0134\n",
      "Epoch 85/1000\n",
      "  Train Loss: 0.0133\n",
      "Epoch 86/1000\n",
      "  Train Loss: 0.0132\n",
      "Epoch 87/1000\n",
      "  Train Loss: 0.0131\n",
      "Epoch 88/1000\n",
      "  Train Loss: 0.0130\n",
      "Epoch 89/1000\n",
      "  Train Loss: 0.0129\n",
      "Epoch 90/1000\n",
      "  Train Loss: 0.0128\n",
      "Epoch 91/1000\n",
      "  Train Loss: 0.0127\n",
      "Epoch 92/1000\n",
      "  Train Loss: 0.0126\n",
      "Epoch 93/1000\n",
      "  Train Loss: 0.0125\n",
      "Epoch 94/1000\n",
      "  Train Loss: 0.0124\n",
      "Epoch 95/1000\n",
      "  Train Loss: 0.0123\n",
      "Epoch 96/1000\n",
      "  Train Loss: 0.0122\n",
      "Epoch 97/1000\n",
      "  Train Loss: 0.0121\n",
      "Epoch 98/1000\n",
      "  Train Loss: 0.0120\n",
      "Epoch 99/1000\n",
      "  Train Loss: 0.0119\n",
      "Epoch 100/1000\n",
      "  Train Loss: 0.0118\n",
      "Epoch 101/1000\n",
      "  Train Loss: 0.0118\n",
      "Epoch 102/1000\n",
      "  Train Loss: 0.0117\n",
      "Epoch 103/1000\n",
      "  Train Loss: 0.0116\n",
      "Epoch 104/1000\n",
      "  Train Loss: 0.0115\n",
      "Epoch 105/1000\n",
      "  Train Loss: 0.0114\n",
      "Epoch 106/1000\n",
      "  Train Loss: 0.0113\n",
      "Epoch 107/1000\n",
      "  Train Loss: 0.0113\n",
      "Epoch 108/1000\n",
      "  Train Loss: 0.0112\n",
      "Epoch 109/1000\n",
      "  Train Loss: 0.0111\n",
      "Epoch 110/1000\n",
      "  Train Loss: 0.0110\n",
      "Epoch 111/1000\n",
      "  Train Loss: 0.0110\n",
      "Epoch 112/1000\n",
      "  Train Loss: 0.0109\n",
      "Epoch 113/1000\n",
      "  Train Loss: 0.0108\n",
      "Epoch 114/1000\n",
      "  Train Loss: 0.0107\n",
      "Epoch 115/1000\n",
      "  Train Loss: 0.0107\n",
      "Epoch 116/1000\n",
      "  Train Loss: 0.0106\n",
      "Epoch 117/1000\n",
      "  Train Loss: 0.0105\n",
      "Epoch 118/1000\n",
      "  Train Loss: 0.0105\n",
      "Epoch 119/1000\n",
      "  Train Loss: 0.0104\n",
      "Epoch 120/1000\n",
      "  Train Loss: 0.0103\n",
      "Epoch 121/1000\n",
      "  Train Loss: 0.0103\n",
      "Epoch 122/1000\n",
      "  Train Loss: 0.0102\n",
      "Epoch 123/1000\n",
      "  Train Loss: 0.0101\n",
      "Epoch 124/1000\n",
      "  Train Loss: 0.0101\n",
      "Epoch 125/1000\n",
      "  Train Loss: 0.0100\n",
      "Epoch 126/1000\n",
      "  Train Loss: 0.0099\n",
      "Epoch 127/1000\n",
      "  Train Loss: 0.0099\n",
      "Epoch 128/1000\n",
      "  Train Loss: 0.0098\n",
      "Epoch 129/1000\n",
      "  Train Loss: 0.0097\n",
      "Epoch 130/1000\n",
      "  Train Loss: 0.0097\n",
      "Epoch 131/1000\n",
      "  Train Loss: 0.0096\n",
      "Epoch 132/1000\n",
      "  Train Loss: 0.0096\n",
      "Epoch 133/1000\n",
      "  Train Loss: 0.0095\n",
      "Epoch 134/1000\n",
      "  Train Loss: 0.0094\n",
      "Epoch 135/1000\n",
      "  Train Loss: 0.0094\n",
      "Epoch 136/1000\n",
      "  Train Loss: 0.0093\n",
      "Epoch 137/1000\n",
      "  Train Loss: 0.0093\n",
      "Epoch 138/1000\n",
      "  Train Loss: 0.0092\n",
      "Epoch 139/1000\n",
      "  Train Loss: 0.0092\n",
      "Epoch 140/1000\n",
      "  Train Loss: 0.0091\n",
      "Epoch 141/1000\n",
      "  Train Loss: 0.0091\n",
      "Epoch 142/1000\n",
      "  Train Loss: 0.0090\n",
      "Epoch 143/1000\n",
      "  Train Loss: 0.0090\n",
      "Epoch 144/1000\n",
      "  Train Loss: 0.0089\n",
      "Epoch 145/1000\n",
      "  Train Loss: 0.0088\n",
      "Epoch 146/1000\n",
      "  Train Loss: 0.0088\n",
      "Epoch 147/1000\n",
      "  Train Loss: 0.0087\n",
      "Epoch 148/1000\n",
      "  Train Loss: 0.0087\n",
      "Epoch 149/1000\n",
      "  Train Loss: 0.0086\n",
      "Epoch 150/1000\n",
      "  Train Loss: 0.0086\n",
      "Epoch 151/1000\n",
      "  Train Loss: 0.0086\n",
      "Epoch 152/1000\n",
      "  Train Loss: 0.0085\n",
      "Epoch 153/1000\n",
      "  Train Loss: 0.0085\n",
      "Epoch 154/1000\n",
      "  Train Loss: 0.0084\n",
      "Epoch 155/1000\n",
      "  Train Loss: 0.0084\n",
      "Epoch 156/1000\n",
      "  Train Loss: 0.0083\n",
      "Epoch 157/1000\n",
      "  Train Loss: 0.0083\n",
      "Epoch 158/1000\n",
      "  Train Loss: 0.0082\n",
      "Epoch 159/1000\n",
      "  Train Loss: 0.0082\n",
      "Epoch 160/1000\n",
      "  Train Loss: 0.0081\n",
      "Epoch 161/1000\n",
      "  Train Loss: 0.0081\n",
      "Epoch 162/1000\n",
      "  Train Loss: 0.0081\n",
      "Epoch 163/1000\n",
      "  Train Loss: 0.0080\n",
      "Epoch 164/1000\n",
      "  Train Loss: 0.0080\n",
      "Epoch 165/1000\n",
      "  Train Loss: 0.0079\n",
      "Epoch 166/1000\n",
      "  Train Loss: 0.0079\n",
      "Epoch 167/1000\n",
      "  Train Loss: 0.0078\n",
      "Epoch 168/1000\n",
      "  Train Loss: 0.0078\n",
      "Epoch 169/1000\n",
      "  Train Loss: 0.0078\n",
      "Epoch 170/1000\n",
      "  Train Loss: 0.0077\n",
      "Epoch 171/1000\n",
      "  Train Loss: 0.0077\n",
      "Epoch 172/1000\n",
      "  Train Loss: 0.0076\n",
      "Epoch 173/1000\n",
      "  Train Loss: 0.0076\n",
      "Epoch 174/1000\n",
      "  Train Loss: 0.0076\n",
      "Epoch 175/1000\n",
      "  Train Loss: 0.0075\n",
      "Epoch 176/1000\n",
      "  Train Loss: 0.0075\n",
      "Epoch 177/1000\n",
      "  Train Loss: 0.0075\n",
      "Epoch 178/1000\n",
      "  Train Loss: 0.0074\n",
      "Epoch 179/1000\n",
      "  Train Loss: 0.0074\n",
      "Epoch 180/1000\n",
      "  Train Loss: 0.0073\n",
      "Epoch 181/1000\n",
      "  Train Loss: 0.0073\n",
      "Epoch 182/1000\n",
      "  Train Loss: 0.0073\n",
      "Epoch 183/1000\n",
      "  Train Loss: 0.0072\n",
      "Epoch 184/1000\n",
      "  Train Loss: 0.0072\n",
      "Epoch 185/1000\n",
      "  Train Loss: 0.0072\n",
      "Epoch 186/1000\n",
      "  Train Loss: 0.0071\n",
      "Epoch 187/1000\n",
      "  Train Loss: 0.0071\n",
      "Epoch 188/1000\n",
      "  Train Loss: 0.0071\n",
      "Epoch 189/1000\n",
      "  Train Loss: 0.0070\n",
      "Epoch 190/1000\n",
      "  Train Loss: 0.0070\n",
      "Epoch 191/1000\n",
      "  Train Loss: 0.0070\n",
      "Epoch 192/1000\n",
      "  Train Loss: 0.0069\n",
      "Epoch 193/1000\n",
      "  Train Loss: 0.0069\n",
      "Epoch 194/1000\n",
      "  Train Loss: 0.0069\n",
      "Epoch 195/1000\n",
      "  Train Loss: 0.0068\n",
      "Epoch 196/1000\n",
      "  Train Loss: 0.0068\n",
      "Epoch 197/1000\n",
      "  Train Loss: 0.0068\n",
      "Epoch 198/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 199/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 200/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 201/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 202/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 203/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 204/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 205/1000\n",
      "  Train Loss: 0.0065\n",
      "Epoch 206/1000\n",
      "  Train Loss: 0.0065\n",
      "Epoch 207/1000\n",
      "  Train Loss: 0.0065\n",
      "Epoch 208/1000\n",
      "  Train Loss: 0.0065\n",
      "Epoch 209/1000\n",
      "  Train Loss: 0.0065\n",
      "Epoch 210/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 211/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 212/1000\n",
      "  Train Loss: 0.0071\n",
      "Epoch 213/1000\n",
      "  Train Loss: 0.0072\n",
      "Epoch 214/1000\n",
      "  Train Loss: 0.0081\n",
      "Epoch 215/1000\n",
      "  Train Loss: 0.0079\n",
      "Epoch 216/1000\n",
      "  Train Loss: 0.0089\n",
      "Epoch 217/1000\n",
      "  Train Loss: 0.0081\n",
      "Epoch 218/1000\n",
      "  Train Loss: 0.0087\n",
      "Epoch 219/1000\n",
      "  Train Loss: 0.0073\n",
      "Epoch 220/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 221/1000\n",
      "  Train Loss: 0.0062\n",
      "Epoch 222/1000\n",
      "  Train Loss: 0.0062\n",
      "Epoch 223/1000\n",
      "  Train Loss: 0.0062\n",
      "Epoch 224/1000\n",
      "  Train Loss: 0.0061\n",
      "Epoch 225/1000\n",
      "  Train Loss: 0.0061\n",
      "Epoch 226/1000\n",
      "  Train Loss: 0.0061\n",
      "Epoch 227/1000\n",
      "  Train Loss: 0.0061\n",
      "Epoch 228/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 229/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 230/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 231/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 232/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 233/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 234/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 235/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 236/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 237/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 238/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 239/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 240/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 241/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 242/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 243/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 244/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 245/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 246/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 247/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 248/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 249/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 250/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 251/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 252/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 253/1000\n",
      "  Train Loss: 0.0060\n",
      "Epoch 254/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 255/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 256/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 257/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 258/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 259/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 260/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 261/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 262/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 263/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 264/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 265/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 266/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 267/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 268/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 269/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 270/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 271/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 272/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 273/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 274/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 275/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 276/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 277/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 278/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 279/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 280/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 281/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 282/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 283/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 284/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 285/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 286/1000\n",
      "  Train Loss: 0.0059\n",
      "Epoch 287/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 288/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 289/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 290/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 291/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 292/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 293/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 294/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 295/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 296/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 297/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 298/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 299/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 300/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 301/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 302/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 303/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 304/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 305/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 306/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 307/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 308/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 309/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 310/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 311/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 312/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 313/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 314/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 315/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 316/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 317/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 318/1000\n",
      "  Train Loss: 0.0058\n",
      "Epoch 319/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 320/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 321/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 322/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 323/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 324/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 325/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 326/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 327/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 328/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 329/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 330/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 331/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 332/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 333/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 334/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 335/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 336/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 337/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 338/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 339/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 340/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 341/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 342/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 343/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 344/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 345/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 346/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 347/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 348/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 349/1000\n",
      "  Train Loss: 0.0057\n",
      "Epoch 350/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 351/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 352/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 353/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 354/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 355/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 356/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 357/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 358/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 359/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 360/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 361/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 362/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 363/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 364/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 365/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 366/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 367/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 368/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 369/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 370/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 371/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 372/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 373/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 374/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 375/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 376/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 377/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 378/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 379/1000\n",
      "  Train Loss: 0.0056\n",
      "Epoch 380/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 381/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 382/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 383/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 384/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 385/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 386/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 387/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 388/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 389/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 390/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 391/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 392/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 393/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 394/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 395/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 396/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 397/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 398/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 399/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 400/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 401/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 402/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 403/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 404/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 405/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 406/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 407/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 408/1000\n",
      "  Train Loss: 0.0055\n",
      "Epoch 409/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 410/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 411/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 412/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 413/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 414/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 415/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 416/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 417/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 418/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 419/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 420/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 421/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 422/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 423/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 424/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 425/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 426/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 427/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 428/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 429/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 430/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 431/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 432/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 433/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 434/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 435/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 436/1000\n",
      "  Train Loss: 0.0054\n",
      "Epoch 437/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 438/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 439/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 440/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 441/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 442/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 443/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 444/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 445/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 446/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 447/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 448/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 449/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 450/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 451/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 452/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 453/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 454/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 455/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 456/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 457/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 458/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 459/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 460/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 461/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 462/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 463/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 464/1000\n",
      "  Train Loss: 0.0053\n",
      "Epoch 465/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 466/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 467/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 468/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 469/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 470/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 471/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 472/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 473/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 474/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 475/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 476/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 477/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 478/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 479/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 480/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 481/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 482/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 483/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 484/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 485/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 486/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 487/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 488/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 489/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 490/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 491/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 492/1000\n",
      "  Train Loss: 0.0052\n",
      "Epoch 493/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 494/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 495/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 496/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 497/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 498/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 499/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 500/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 501/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 502/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 503/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 504/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 505/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 506/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 507/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 508/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 509/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 510/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 511/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 512/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 513/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 514/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 515/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 516/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 517/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 518/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 519/1000\n",
      "  Train Loss: 0.0051\n",
      "Epoch 520/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 521/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 522/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 523/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 524/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 525/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 526/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 527/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 528/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 529/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 530/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 531/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 532/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 533/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 534/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 535/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 536/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 537/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 538/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 539/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 540/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 541/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 542/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 543/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 544/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 545/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 546/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 547/1000\n",
      "  Train Loss: 0.0050\n",
      "Epoch 548/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 549/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 550/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 551/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 552/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 553/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 554/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 555/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 556/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 557/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 558/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 559/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 560/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 561/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 562/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 563/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 564/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 565/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 566/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 567/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 568/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 569/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 570/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 571/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 572/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 573/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 574/1000\n",
      "  Train Loss: 0.0049\n",
      "Epoch 575/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 576/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 577/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 578/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 579/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 580/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 581/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 582/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 583/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 584/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 585/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 586/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 587/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 588/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 589/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 590/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 591/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 592/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 593/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 594/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 595/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 596/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 597/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 598/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 599/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 600/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 601/1000\n",
      "  Train Loss: 0.0048\n",
      "Epoch 602/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 603/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 604/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 605/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 606/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 607/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 608/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 609/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 610/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 611/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 612/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 613/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 614/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 615/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 616/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 617/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 618/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 619/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 620/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 621/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 622/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 623/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 624/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 625/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 626/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 627/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 628/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 629/1000\n",
      "  Train Loss: 0.0047\n",
      "Epoch 630/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 631/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 632/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 633/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 634/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 635/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 636/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 637/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 638/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 639/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 640/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 641/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 642/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 643/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 644/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 645/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 646/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 647/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 648/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 649/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 650/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 651/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 652/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 653/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 654/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 655/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 656/1000\n",
      "  Train Loss: 0.0046\n",
      "Epoch 657/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 658/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 659/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 660/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 661/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 662/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 663/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 664/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 665/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 666/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 667/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 668/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 669/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 670/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 671/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 672/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 673/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 674/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 675/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 676/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 677/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 678/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 679/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 680/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 681/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 682/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 683/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 684/1000\n",
      "  Train Loss: 0.0045\n",
      "Epoch 685/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 686/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 687/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 688/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 689/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 690/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 691/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 692/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 693/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 694/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 695/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 696/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 697/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 698/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 699/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 700/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 701/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 702/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 703/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 704/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 705/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 706/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 707/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 708/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 709/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 710/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 711/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 712/1000\n",
      "  Train Loss: 0.0044\n",
      "Epoch 713/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 714/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 715/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 716/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 717/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 718/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 719/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 720/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 721/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 722/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 723/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 724/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 725/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 726/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 727/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 728/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 729/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 730/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 731/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 732/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 733/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 734/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 735/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 736/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 737/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 738/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 739/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 740/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 741/1000\n",
      "  Train Loss: 0.0043\n",
      "Epoch 742/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 743/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 744/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 745/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 746/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 747/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 748/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 749/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 750/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 751/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 752/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 753/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 754/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 755/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 756/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 757/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 758/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 759/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 760/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 761/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 762/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 763/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 764/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 765/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 766/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 767/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 768/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 769/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 770/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 771/1000\n",
      "  Train Loss: 0.0042\n",
      "Epoch 772/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 773/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 774/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 775/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 776/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 777/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 778/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 779/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 780/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 781/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 782/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 783/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 784/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 785/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 786/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 787/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 788/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 789/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 790/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 791/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 792/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 793/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 794/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 795/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 796/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 797/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 798/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 799/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 800/1000\n",
      "  Train Loss: 0.0041\n",
      "Epoch 801/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 802/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 803/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 804/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 805/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 806/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 807/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 808/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 809/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 810/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 811/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 812/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 813/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 814/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 815/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 816/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 817/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 818/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 819/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 820/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 821/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 822/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 823/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 824/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 825/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 826/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 827/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 828/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 829/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 830/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 831/1000\n",
      "  Train Loss: 0.0040\n",
      "Epoch 832/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 833/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 834/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 835/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 836/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 837/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 838/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 839/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 840/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 841/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 842/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 843/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 844/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 845/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 846/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 847/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 848/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 849/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 850/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 851/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 852/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 853/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 854/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 855/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 856/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 857/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 858/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 859/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 860/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 861/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 862/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 863/1000\n",
      "  Train Loss: 0.0039\n",
      "Epoch 864/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 865/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 866/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 867/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 868/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 869/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 870/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 871/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 872/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 873/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 874/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 875/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 876/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 877/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 878/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 879/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 880/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 881/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 882/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 883/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 884/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 885/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 886/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 887/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 888/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 889/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 890/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 891/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 892/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 893/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 894/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 895/1000\n",
      "  Train Loss: 0.0038\n",
      "Epoch 896/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 897/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 898/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 899/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 900/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 901/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 902/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 903/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 904/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 905/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 906/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 907/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 908/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 909/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 910/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 911/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 912/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 913/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 914/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 915/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 916/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 917/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 918/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 919/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 920/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 921/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 922/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 923/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 924/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 925/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 926/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 927/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 928/1000\n",
      "  Train Loss: 0.0037\n",
      "Epoch 929/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 930/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 931/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 932/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 933/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 934/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 935/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 936/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 937/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 938/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 939/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 940/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 941/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 942/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 943/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 944/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 945/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 946/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 947/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 948/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 949/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 950/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 951/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 952/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 953/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 954/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 955/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 956/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 957/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 958/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 959/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 960/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 961/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 962/1000\n",
      "  Train Loss: 0.0036\n",
      "Epoch 963/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 964/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 965/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 966/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 967/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 968/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 969/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 970/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 971/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 972/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 973/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 974/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 975/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 976/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 977/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 978/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 979/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 980/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 981/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 982/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 983/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 984/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 985/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 986/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 987/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 988/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 989/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 990/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 991/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 992/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 993/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 994/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 995/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 996/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 997/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 998/1000\n",
      "  Train Loss: 0.0035\n",
      "Epoch 999/1000\n",
      "  Train Loss: 0.0034\n",
      "Epoch 1000/1000\n",
      "  Train Loss: 0.0034\n",
      "Epoch 1/1000\n",
      "  Train Loss: 0.0872\n",
      "Epoch 2/1000\n",
      "  Train Loss: 0.0815\n",
      "Epoch 3/1000\n",
      "  Train Loss: 0.0799\n",
      "Epoch 4/1000\n",
      "  Train Loss: 0.0794\n",
      "Epoch 5/1000\n",
      "  Train Loss: 0.0784\n",
      "Epoch 6/1000\n",
      "  Train Loss: 0.0771\n",
      "Epoch 7/1000\n",
      "  Train Loss: 0.0757\n",
      "Epoch 8/1000\n",
      "  Train Loss: 0.0745\n",
      "Epoch 9/1000\n",
      "  Train Loss: 0.0732\n",
      "Epoch 10/1000\n",
      "  Train Loss: 0.0719\n",
      "Epoch 11/1000\n",
      "  Train Loss: 0.0707\n",
      "Epoch 12/1000\n",
      "  Train Loss: 0.0696\n",
      "Epoch 13/1000\n",
      "  Train Loss: 0.0686\n",
      "Epoch 14/1000\n",
      "  Train Loss: 0.0677\n",
      "Epoch 15/1000\n",
      "  Train Loss: 0.0669\n",
      "Epoch 16/1000\n",
      "  Train Loss: 0.0663\n",
      "Epoch 17/1000\n",
      "  Train Loss: 0.0657\n",
      "Epoch 18/1000\n",
      "  Train Loss: 0.0653\n",
      "Epoch 19/1000\n",
      "  Train Loss: 0.0649\n",
      "Epoch 20/1000\n",
      "  Train Loss: 0.0646\n",
      "Epoch 21/1000\n",
      "  Train Loss: 0.0643\n",
      "Epoch 22/1000\n",
      "  Train Loss: 0.0641\n",
      "Epoch 23/1000\n",
      "  Train Loss: 0.0638\n",
      "Epoch 24/1000\n",
      "  Train Loss: 0.0636\n",
      "Epoch 25/1000\n",
      "  Train Loss: 0.0634\n",
      "Epoch 26/1000\n",
      "  Train Loss: 0.0632\n",
      "Epoch 27/1000\n",
      "  Train Loss: 0.0630\n",
      "Epoch 28/1000\n",
      "  Train Loss: 0.0629\n",
      "Epoch 29/1000\n",
      "  Train Loss: 0.0627\n",
      "Epoch 30/1000\n",
      "  Train Loss: 0.0625\n",
      "Epoch 31/1000\n",
      "  Train Loss: 0.0623\n",
      "Epoch 32/1000\n",
      "  Train Loss: 0.0621\n",
      "Epoch 33/1000\n",
      "  Train Loss: 0.0619\n",
      "Epoch 34/1000\n",
      "  Train Loss: 0.0617\n",
      "Epoch 35/1000\n",
      "  Train Loss: 0.0615\n",
      "Epoch 36/1000\n",
      "  Train Loss: 0.0613\n",
      "Epoch 37/1000\n",
      "  Train Loss: 0.0611\n",
      "Epoch 38/1000\n",
      "  Train Loss: 0.0609\n",
      "Epoch 39/1000\n",
      "  Train Loss: 0.0607\n",
      "Epoch 40/1000\n",
      "  Train Loss: 0.0605\n",
      "Epoch 41/1000\n",
      "  Train Loss: 0.0602\n",
      "Epoch 42/1000\n",
      "  Train Loss: 0.0600\n",
      "Epoch 43/1000\n",
      "  Train Loss: 0.0597\n",
      "Epoch 44/1000\n",
      "  Train Loss: 0.0595\n",
      "Epoch 45/1000\n",
      "  Train Loss: 0.0592\n",
      "Epoch 46/1000\n",
      "  Train Loss: 0.0589\n",
      "Epoch 47/1000\n",
      "  Train Loss: 0.0586\n",
      "Epoch 48/1000\n",
      "  Train Loss: 0.0583\n",
      "Epoch 49/1000\n",
      "  Train Loss: 0.0580\n",
      "Epoch 50/1000\n",
      "  Train Loss: 0.0577\n",
      "Epoch 51/1000\n",
      "  Train Loss: 0.0574\n",
      "Epoch 52/1000\n",
      "  Train Loss: 0.0571\n",
      "Epoch 53/1000\n",
      "  Train Loss: 0.0567\n",
      "Epoch 54/1000\n",
      "  Train Loss: 0.0564\n",
      "Epoch 55/1000\n",
      "  Train Loss: 0.0560\n",
      "Epoch 56/1000\n",
      "  Train Loss: 0.0556\n",
      "Epoch 57/1000\n",
      "  Train Loss: 0.0553\n",
      "Epoch 58/1000\n",
      "  Train Loss: 0.0549\n",
      "Epoch 59/1000\n",
      "  Train Loss: 0.0545\n",
      "Epoch 60/1000\n",
      "  Train Loss: 0.0541\n",
      "Epoch 61/1000\n",
      "  Train Loss: 0.0537\n",
      "Epoch 62/1000\n",
      "  Train Loss: 0.0533\n",
      "Epoch 63/1000\n",
      "  Train Loss: 0.0529\n",
      "Epoch 64/1000\n",
      "  Train Loss: 0.0524\n",
      "Epoch 65/1000\n",
      "  Train Loss: 0.0520\n",
      "Epoch 66/1000\n",
      "  Train Loss: 0.0516\n",
      "Epoch 67/1000\n",
      "  Train Loss: 0.0512\n",
      "Epoch 68/1000\n",
      "  Train Loss: 0.0507\n",
      "Epoch 69/1000\n",
      "  Train Loss: 0.0503\n",
      "Epoch 70/1000\n",
      "  Train Loss: 0.0499\n",
      "Epoch 71/1000\n",
      "  Train Loss: 0.0495\n",
      "Epoch 72/1000\n",
      "  Train Loss: 0.0491\n",
      "Epoch 73/1000\n",
      "  Train Loss: 0.0486\n",
      "Epoch 74/1000\n",
      "  Train Loss: 0.0482\n",
      "Epoch 75/1000\n",
      "  Train Loss: 0.0478\n",
      "Epoch 76/1000\n",
      "  Train Loss: 0.0474\n",
      "Epoch 77/1000\n",
      "  Train Loss: 0.0470\n",
      "Epoch 78/1000\n",
      "  Train Loss: 0.0466\n",
      "Epoch 79/1000\n",
      "  Train Loss: 0.0462\n",
      "Epoch 80/1000\n",
      "  Train Loss: 0.0459\n",
      "Epoch 81/1000\n",
      "  Train Loss: 0.0455\n",
      "Epoch 82/1000\n",
      "  Train Loss: 0.0451\n",
      "Epoch 83/1000\n",
      "  Train Loss: 0.0448\n",
      "Epoch 84/1000\n",
      "  Train Loss: 0.0444\n",
      "Epoch 85/1000\n",
      "  Train Loss: 0.0441\n",
      "Epoch 86/1000\n",
      "  Train Loss: 0.0437\n",
      "Epoch 87/1000\n",
      "  Train Loss: 0.0434\n",
      "Epoch 88/1000\n",
      "  Train Loss: 0.0431\n",
      "Epoch 89/1000\n",
      "  Train Loss: 0.0427\n",
      "Epoch 90/1000\n",
      "  Train Loss: 0.0424\n",
      "Epoch 91/1000\n",
      "  Train Loss: 0.0421\n",
      "Epoch 92/1000\n",
      "  Train Loss: 0.0418\n",
      "Epoch 93/1000\n",
      "  Train Loss: 0.0415\n",
      "Epoch 94/1000\n",
      "  Train Loss: 0.0412\n",
      "Epoch 95/1000\n",
      "  Train Loss: 0.0409\n",
      "Epoch 96/1000\n",
      "  Train Loss: 0.0406\n",
      "Epoch 97/1000\n",
      "  Train Loss: 0.0404\n",
      "Epoch 98/1000\n",
      "  Train Loss: 0.0401\n",
      "Epoch 99/1000\n",
      "  Train Loss: 0.0399\n",
      "Epoch 100/1000\n",
      "  Train Loss: 0.0396\n",
      "Epoch 101/1000\n",
      "  Train Loss: 0.0394\n",
      "Epoch 102/1000\n",
      "  Train Loss: 0.0391\n",
      "Epoch 103/1000\n",
      "  Train Loss: 0.0389\n",
      "Epoch 104/1000\n",
      "  Train Loss: 0.0387\n",
      "Epoch 105/1000\n",
      "  Train Loss: 0.0384\n",
      "Epoch 106/1000\n",
      "  Train Loss: 0.0382\n",
      "Epoch 107/1000\n",
      "  Train Loss: 0.0380\n",
      "Epoch 108/1000\n",
      "  Train Loss: 0.0378\n",
      "Epoch 109/1000\n",
      "  Train Loss: 0.0376\n",
      "Epoch 110/1000\n",
      "  Train Loss: 0.0374\n",
      "Epoch 111/1000\n",
      "  Train Loss: 0.0372\n",
      "Epoch 112/1000\n",
      "  Train Loss: 0.0370\n",
      "Epoch 113/1000\n",
      "  Train Loss: 0.0369\n",
      "Epoch 114/1000\n",
      "  Train Loss: 0.0367\n",
      "Epoch 115/1000\n",
      "  Train Loss: 0.0365\n",
      "Epoch 116/1000\n",
      "  Train Loss: 0.0364\n",
      "Epoch 117/1000\n",
      "  Train Loss: 0.0362\n",
      "Epoch 118/1000\n",
      "  Train Loss: 0.0361\n",
      "Epoch 119/1000\n",
      "  Train Loss: 0.0359\n",
      "Epoch 120/1000\n",
      "  Train Loss: 0.0358\n",
      "Epoch 121/1000\n",
      "  Train Loss: 0.0356\n",
      "Epoch 122/1000\n",
      "  Train Loss: 0.0355\n",
      "Epoch 123/1000\n",
      "  Train Loss: 0.0354\n",
      "Epoch 124/1000\n",
      "  Train Loss: 0.0352\n",
      "Epoch 125/1000\n",
      "  Train Loss: 0.0351\n",
      "Epoch 126/1000\n",
      "  Train Loss: 0.0350\n",
      "Epoch 127/1000\n",
      "  Train Loss: 0.0349\n",
      "Epoch 128/1000\n",
      "  Train Loss: 0.0347\n",
      "Epoch 129/1000\n",
      "  Train Loss: 0.0346\n",
      "Epoch 130/1000\n",
      "  Train Loss: 0.0345\n",
      "Epoch 131/1000\n",
      "  Train Loss: 0.0344\n",
      "Epoch 132/1000\n",
      "  Train Loss: 0.0343\n",
      "Epoch 133/1000\n",
      "  Train Loss: 0.0342\n",
      "Epoch 134/1000\n",
      "  Train Loss: 0.0341\n",
      "Epoch 135/1000\n",
      "  Train Loss: 0.0340\n",
      "Epoch 136/1000\n",
      "  Train Loss: 0.0339\n",
      "Epoch 137/1000\n",
      "  Train Loss: 0.0338\n",
      "Epoch 138/1000\n",
      "  Train Loss: 0.0337\n",
      "Epoch 139/1000\n",
      "  Train Loss: 0.0336\n",
      "Epoch 140/1000\n",
      "  Train Loss: 0.0335\n",
      "Epoch 141/1000\n",
      "  Train Loss: 0.0335\n",
      "Epoch 142/1000\n",
      "  Train Loss: 0.0334\n",
      "Epoch 143/1000\n",
      "  Train Loss: 0.0333\n",
      "Epoch 144/1000\n",
      "  Train Loss: 0.0332\n",
      "Epoch 145/1000\n",
      "  Train Loss: 0.0331\n",
      "Epoch 146/1000\n",
      "  Train Loss: 0.0331\n",
      "Epoch 147/1000\n",
      "  Train Loss: 0.0330\n",
      "Epoch 148/1000\n",
      "  Train Loss: 0.0329\n",
      "Epoch 149/1000\n",
      "  Train Loss: 0.0328\n",
      "Epoch 150/1000\n",
      "  Train Loss: 0.0328\n",
      "Epoch 151/1000\n",
      "  Train Loss: 0.0327\n",
      "Epoch 152/1000\n",
      "  Train Loss: 0.0326\n",
      "Epoch 153/1000\n",
      "  Train Loss: 0.0326\n",
      "Epoch 154/1000\n",
      "  Train Loss: 0.0325\n",
      "Epoch 155/1000\n",
      "  Train Loss: 0.0324\n",
      "Epoch 156/1000\n",
      "  Train Loss: 0.0324\n",
      "Epoch 157/1000\n",
      "  Train Loss: 0.0323\n",
      "Epoch 158/1000\n",
      "  Train Loss: 0.0323\n",
      "Epoch 159/1000\n",
      "  Train Loss: 0.0322\n",
      "Epoch 160/1000\n",
      "  Train Loss: 0.0321\n",
      "Epoch 161/1000\n",
      "  Train Loss: 0.0321\n",
      "Epoch 162/1000\n",
      "  Train Loss: 0.0320\n",
      "Epoch 163/1000\n",
      "  Train Loss: 0.0320\n",
      "Epoch 164/1000\n",
      "  Train Loss: 0.0319\n",
      "Epoch 165/1000\n",
      "  Train Loss: 0.0319\n",
      "Epoch 166/1000\n",
      "  Train Loss: 0.0318\n",
      "Epoch 167/1000\n",
      "  Train Loss: 0.0318\n",
      "Epoch 168/1000\n",
      "  Train Loss: 0.0317\n",
      "Epoch 169/1000\n",
      "  Train Loss: 0.0317\n",
      "Epoch 170/1000\n",
      "  Train Loss: 0.0316\n",
      "Epoch 171/1000\n",
      "  Train Loss: 0.0316\n",
      "Epoch 172/1000\n",
      "  Train Loss: 0.0315\n",
      "Epoch 173/1000\n",
      "  Train Loss: 0.0315\n",
      "Epoch 174/1000\n",
      "  Train Loss: 0.0314\n",
      "Epoch 175/1000\n",
      "  Train Loss: 0.0314\n",
      "Epoch 176/1000\n",
      "  Train Loss: 0.0313\n",
      "Epoch 177/1000\n",
      "  Train Loss: 0.0313\n",
      "Epoch 178/1000\n",
      "  Train Loss: 0.0312\n",
      "Epoch 179/1000\n",
      "  Train Loss: 0.0312\n",
      "Epoch 180/1000\n",
      "  Train Loss: 0.0311\n",
      "Epoch 181/1000\n",
      "  Train Loss: 0.0311\n",
      "Epoch 182/1000\n",
      "  Train Loss: 0.0310\n",
      "Epoch 183/1000\n",
      "  Train Loss: 0.0310\n",
      "Epoch 184/1000\n",
      "  Train Loss: 0.0310\n",
      "Epoch 185/1000\n",
      "  Train Loss: 0.0309\n",
      "Epoch 186/1000\n",
      "  Train Loss: 0.0309\n",
      "Epoch 187/1000\n",
      "  Train Loss: 0.0308\n",
      "Epoch 188/1000\n",
      "  Train Loss: 0.0308\n",
      "Epoch 189/1000\n",
      "  Train Loss: 0.0308\n",
      "Epoch 190/1000\n",
      "  Train Loss: 0.0307\n",
      "Epoch 191/1000\n",
      "  Train Loss: 0.0307\n",
      "Epoch 192/1000\n",
      "  Train Loss: 0.0306\n",
      "Epoch 193/1000\n",
      "  Train Loss: 0.0306\n",
      "Epoch 194/1000\n",
      "  Train Loss: 0.0306\n",
      "Epoch 195/1000\n",
      "  Train Loss: 0.0305\n",
      "Epoch 196/1000\n",
      "  Train Loss: 0.0305\n",
      "Epoch 197/1000\n",
      "  Train Loss: 0.0304\n",
      "Epoch 198/1000\n",
      "  Train Loss: 0.0304\n",
      "Epoch 199/1000\n",
      "  Train Loss: 0.0304\n",
      "Epoch 200/1000\n",
      "  Train Loss: 0.0303\n",
      "Epoch 201/1000\n",
      "  Train Loss: 0.0303\n",
      "Epoch 202/1000\n",
      "  Train Loss: 0.0303\n",
      "Epoch 203/1000\n",
      "  Train Loss: 0.0302\n",
      "Epoch 204/1000\n",
      "  Train Loss: 0.0302\n",
      "Epoch 205/1000\n",
      "  Train Loss: 0.0302\n",
      "Epoch 206/1000\n",
      "  Train Loss: 0.0301\n",
      "Epoch 207/1000\n",
      "  Train Loss: 0.0301\n",
      "Epoch 208/1000\n",
      "  Train Loss: 0.0301\n",
      "Epoch 209/1000\n",
      "  Train Loss: 0.0300\n",
      "Epoch 210/1000\n",
      "  Train Loss: 0.0300\n",
      "Epoch 211/1000\n",
      "  Train Loss: 0.0300\n",
      "Epoch 212/1000\n",
      "  Train Loss: 0.0299\n",
      "Epoch 213/1000\n",
      "  Train Loss: 0.0299\n",
      "Epoch 214/1000\n",
      "  Train Loss: 0.0299\n",
      "Epoch 215/1000\n",
      "  Train Loss: 0.0298\n",
      "Epoch 216/1000\n",
      "  Train Loss: 0.0298\n",
      "Epoch 217/1000\n",
      "  Train Loss: 0.0298\n",
      "Epoch 218/1000\n",
      "  Train Loss: 0.0297\n",
      "Epoch 219/1000\n",
      "  Train Loss: 0.0297\n",
      "Epoch 220/1000\n",
      "  Train Loss: 0.0297\n",
      "Epoch 221/1000\n",
      "  Train Loss: 0.0296\n",
      "Epoch 222/1000\n",
      "  Train Loss: 0.0296\n",
      "Epoch 223/1000\n",
      "  Train Loss: 0.0296\n",
      "Epoch 224/1000\n",
      "  Train Loss: 0.0296\n",
      "Epoch 225/1000\n",
      "  Train Loss: 0.0295\n",
      "Epoch 226/1000\n",
      "  Train Loss: 0.0295\n",
      "Epoch 227/1000\n",
      "  Train Loss: 0.0295\n",
      "Epoch 228/1000\n",
      "  Train Loss: 0.0294\n",
      "Epoch 229/1000\n",
      "  Train Loss: 0.0294\n",
      "Epoch 230/1000\n",
      "  Train Loss: 0.0294\n",
      "Epoch 231/1000\n",
      "  Train Loss: 0.0294\n",
      "Epoch 232/1000\n",
      "  Train Loss: 0.0293\n",
      "Epoch 233/1000\n",
      "  Train Loss: 0.0293\n",
      "Epoch 234/1000\n",
      "  Train Loss: 0.0293\n",
      "Epoch 235/1000\n",
      "  Train Loss: 0.0292\n",
      "Epoch 236/1000\n",
      "  Train Loss: 0.0292\n",
      "Epoch 237/1000\n",
      "  Train Loss: 0.0292\n",
      "Epoch 238/1000\n",
      "  Train Loss: 0.0292\n",
      "Epoch 239/1000\n",
      "  Train Loss: 0.0291\n",
      "Epoch 240/1000\n",
      "  Train Loss: 0.0291\n",
      "Epoch 241/1000\n",
      "  Train Loss: 0.0291\n",
      "Epoch 242/1000\n",
      "  Train Loss: 0.0291\n",
      "Epoch 243/1000\n",
      "  Train Loss: 0.0290\n",
      "Epoch 244/1000\n",
      "  Train Loss: 0.0290\n",
      "Epoch 245/1000\n",
      "  Train Loss: 0.0290\n",
      "Epoch 246/1000\n",
      "  Train Loss: 0.0289\n",
      "Epoch 247/1000\n",
      "  Train Loss: 0.0289\n",
      "Epoch 248/1000\n",
      "  Train Loss: 0.0289\n",
      "Epoch 249/1000\n",
      "  Train Loss: 0.0289\n",
      "Epoch 250/1000\n",
      "  Train Loss: 0.0288\n",
      "Epoch 251/1000\n",
      "  Train Loss: 0.0288\n",
      "Epoch 252/1000\n",
      "  Train Loss: 0.0288\n",
      "Epoch 253/1000\n",
      "  Train Loss: 0.0288\n",
      "Epoch 254/1000\n",
      "  Train Loss: 0.0287\n",
      "Epoch 255/1000\n",
      "  Train Loss: 0.0287\n",
      "Epoch 256/1000\n",
      "  Train Loss: 0.0287\n",
      "Epoch 257/1000\n",
      "  Train Loss: 0.0287\n",
      "Epoch 258/1000\n",
      "  Train Loss: 0.0286\n",
      "Epoch 259/1000\n",
      "  Train Loss: 0.0286\n",
      "Epoch 260/1000\n",
      "  Train Loss: 0.0286\n",
      "Epoch 261/1000\n",
      "  Train Loss: 0.0286\n",
      "Epoch 262/1000\n",
      "  Train Loss: 0.0286\n",
      "Epoch 263/1000\n",
      "  Train Loss: 0.0285\n",
      "Epoch 264/1000\n",
      "  Train Loss: 0.0285\n",
      "Epoch 265/1000\n",
      "  Train Loss: 0.0285\n",
      "Epoch 266/1000\n",
      "  Train Loss: 0.0285\n",
      "Epoch 267/1000\n",
      "  Train Loss: 0.0284\n",
      "Epoch 268/1000\n",
      "  Train Loss: 0.0284\n",
      "Epoch 269/1000\n",
      "  Train Loss: 0.0284\n",
      "Epoch 270/1000\n",
      "  Train Loss: 0.0284\n",
      "Epoch 271/1000\n",
      "  Train Loss: 0.0283\n",
      "Epoch 272/1000\n",
      "  Train Loss: 0.0283\n",
      "Epoch 273/1000\n",
      "  Train Loss: 0.0283\n",
      "Epoch 274/1000\n",
      "  Train Loss: 0.0283\n",
      "Epoch 275/1000\n",
      "  Train Loss: 0.0283\n",
      "Epoch 276/1000\n",
      "  Train Loss: 0.0282\n",
      "Epoch 277/1000\n",
      "  Train Loss: 0.0282\n",
      "Epoch 278/1000\n",
      "  Train Loss: 0.0282\n",
      "Epoch 279/1000\n",
      "  Train Loss: 0.0282\n",
      "Epoch 280/1000\n",
      "  Train Loss: 0.0281\n",
      "Epoch 281/1000\n",
      "  Train Loss: 0.0281\n",
      "Epoch 282/1000\n",
      "  Train Loss: 0.0281\n",
      "Epoch 283/1000\n",
      "  Train Loss: 0.0281\n",
      "Epoch 284/1000\n",
      "  Train Loss: 0.0281\n",
      "Epoch 285/1000\n",
      "  Train Loss: 0.0280\n",
      "Epoch 286/1000\n",
      "  Train Loss: 0.0280\n",
      "Epoch 287/1000\n",
      "  Train Loss: 0.0280\n",
      "Epoch 288/1000\n",
      "  Train Loss: 0.0280\n",
      "Epoch 289/1000\n",
      "  Train Loss: 0.0279\n",
      "Epoch 290/1000\n",
      "  Train Loss: 0.0279\n",
      "Epoch 291/1000\n",
      "  Train Loss: 0.0279\n",
      "Epoch 292/1000\n",
      "  Train Loss: 0.0279\n",
      "Epoch 293/1000\n",
      "  Train Loss: 0.0279\n",
      "Epoch 294/1000\n",
      "  Train Loss: 0.0278\n",
      "Epoch 295/1000\n",
      "  Train Loss: 0.0278\n",
      "Epoch 296/1000\n",
      "  Train Loss: 0.0278\n",
      "Epoch 297/1000\n",
      "  Train Loss: 0.0278\n",
      "Epoch 298/1000\n",
      "  Train Loss: 0.0278\n",
      "Epoch 299/1000\n",
      "  Train Loss: 0.0277\n",
      "Epoch 300/1000\n",
      "  Train Loss: 0.0277\n",
      "Epoch 301/1000\n",
      "  Train Loss: 0.0277\n",
      "Epoch 302/1000\n",
      "  Train Loss: 0.0277\n",
      "Epoch 303/1000\n",
      "  Train Loss: 0.0277\n",
      "Epoch 304/1000\n",
      "  Train Loss: 0.0276\n",
      "Epoch 305/1000\n",
      "  Train Loss: 0.0276\n",
      "Epoch 306/1000\n",
      "  Train Loss: 0.0276\n",
      "Epoch 307/1000\n",
      "  Train Loss: 0.0276\n",
      "Epoch 308/1000\n",
      "  Train Loss: 0.0276\n",
      "Epoch 309/1000\n",
      "  Train Loss: 0.0275\n",
      "Epoch 310/1000\n",
      "  Train Loss: 0.0275\n",
      "Epoch 311/1000\n",
      "  Train Loss: 0.0275\n",
      "Epoch 312/1000\n",
      "  Train Loss: 0.0275\n",
      "Epoch 313/1000\n",
      "  Train Loss: 0.0275\n",
      "Epoch 314/1000\n",
      "  Train Loss: 0.0274\n",
      "Epoch 315/1000\n",
      "  Train Loss: 0.0274\n",
      "Epoch 316/1000\n",
      "  Train Loss: 0.0274\n",
      "Epoch 317/1000\n",
      "  Train Loss: 0.0274\n",
      "Epoch 318/1000\n",
      "  Train Loss: 0.0274\n",
      "Epoch 319/1000\n",
      "  Train Loss: 0.0274\n",
      "Epoch 320/1000\n",
      "  Train Loss: 0.0273\n",
      "Epoch 321/1000\n",
      "  Train Loss: 0.0273\n",
      "Epoch 322/1000\n",
      "  Train Loss: 0.0273\n",
      "Epoch 323/1000\n",
      "  Train Loss: 0.0273\n",
      "Epoch 324/1000\n",
      "  Train Loss: 0.0273\n",
      "Epoch 325/1000\n",
      "  Train Loss: 0.0272\n",
      "Epoch 326/1000\n",
      "  Train Loss: 0.0272\n",
      "Epoch 327/1000\n",
      "  Train Loss: 0.0272\n",
      "Epoch 328/1000\n",
      "  Train Loss: 0.0272\n",
      "Epoch 329/1000\n",
      "  Train Loss: 0.0272\n",
      "Epoch 330/1000\n",
      "  Train Loss: 0.0272\n",
      "Epoch 331/1000\n",
      "  Train Loss: 0.0271\n",
      "Epoch 332/1000\n",
      "  Train Loss: 0.0271\n",
      "Epoch 333/1000\n",
      "  Train Loss: 0.0271\n",
      "Epoch 334/1000\n",
      "  Train Loss: 0.0271\n",
      "Epoch 335/1000\n",
      "  Train Loss: 0.0271\n",
      "Epoch 336/1000\n",
      "  Train Loss: 0.0270\n",
      "Epoch 337/1000\n",
      "  Train Loss: 0.0270\n",
      "Epoch 338/1000\n",
      "  Train Loss: 0.0270\n",
      "Epoch 339/1000\n",
      "  Train Loss: 0.0270\n",
      "Epoch 340/1000\n",
      "  Train Loss: 0.0270\n",
      "Epoch 341/1000\n",
      "  Train Loss: 0.0270\n",
      "Epoch 342/1000\n",
      "  Train Loss: 0.0269\n",
      "Epoch 343/1000\n",
      "  Train Loss: 0.0269\n",
      "Epoch 344/1000\n",
      "  Train Loss: 0.0269\n",
      "Epoch 345/1000\n",
      "  Train Loss: 0.0269\n",
      "Epoch 346/1000\n",
      "  Train Loss: 0.0269\n",
      "Epoch 347/1000\n",
      "  Train Loss: 0.0269\n",
      "Epoch 348/1000\n",
      "  Train Loss: 0.0268\n",
      "Epoch 349/1000\n",
      "  Train Loss: 0.0268\n",
      "Epoch 350/1000\n",
      "  Train Loss: 0.0268\n",
      "Epoch 351/1000\n",
      "  Train Loss: 0.0268\n",
      "Epoch 352/1000\n",
      "  Train Loss: 0.0268\n",
      "Epoch 353/1000\n",
      "  Train Loss: 0.0268\n",
      "Epoch 354/1000\n",
      "  Train Loss: 0.0268\n",
      "Epoch 355/1000\n",
      "  Train Loss: 0.0267\n",
      "Epoch 356/1000\n",
      "  Train Loss: 0.0267\n",
      "Epoch 357/1000\n",
      "  Train Loss: 0.0267\n",
      "Epoch 358/1000\n",
      "  Train Loss: 0.0267\n",
      "Epoch 359/1000\n",
      "  Train Loss: 0.0267\n",
      "Epoch 360/1000\n",
      "  Train Loss: 0.0267\n",
      "Epoch 361/1000\n",
      "  Train Loss: 0.0266\n",
      "Epoch 362/1000\n",
      "  Train Loss: 0.0266\n",
      "Epoch 363/1000\n",
      "  Train Loss: 0.0266\n",
      "Epoch 364/1000\n",
      "  Train Loss: 0.0266\n",
      "Epoch 365/1000\n",
      "  Train Loss: 0.0266\n",
      "Epoch 366/1000\n",
      "  Train Loss: 0.0266\n",
      "Epoch 367/1000\n",
      "  Train Loss: 0.0266\n",
      "Epoch 368/1000\n",
      "  Train Loss: 0.0265\n",
      "Epoch 369/1000\n",
      "  Train Loss: 0.0265\n",
      "Epoch 370/1000\n",
      "  Train Loss: 0.0265\n",
      "Epoch 371/1000\n",
      "  Train Loss: 0.0265\n",
      "Epoch 372/1000\n",
      "  Train Loss: 0.0265\n",
      "Epoch 373/1000\n",
      "  Train Loss: 0.0265\n",
      "Epoch 374/1000\n",
      "  Train Loss: 0.0265\n",
      "Epoch 375/1000\n",
      "  Train Loss: 0.0264\n",
      "Epoch 376/1000\n",
      "  Train Loss: 0.0264\n",
      "Epoch 377/1000\n",
      "  Train Loss: 0.0264\n",
      "Epoch 378/1000\n",
      "  Train Loss: 0.0264\n",
      "Epoch 379/1000\n",
      "  Train Loss: 0.0264\n",
      "Epoch 380/1000\n",
      "  Train Loss: 0.0264\n",
      "Epoch 381/1000\n",
      "  Train Loss: 0.0264\n",
      "Epoch 382/1000\n",
      "  Train Loss: 0.0264\n",
      "Epoch 383/1000\n",
      "  Train Loss: 0.0263\n",
      "Epoch 384/1000\n",
      "  Train Loss: 0.0263\n",
      "Epoch 385/1000\n",
      "  Train Loss: 0.0263\n",
      "Epoch 386/1000\n",
      "  Train Loss: 0.0263\n",
      "Epoch 387/1000\n",
      "  Train Loss: 0.0263\n",
      "Epoch 388/1000\n",
      "  Train Loss: 0.0263\n",
      "Epoch 389/1000\n",
      "  Train Loss: 0.0263\n",
      "Epoch 390/1000\n",
      "  Train Loss: 0.0263\n",
      "Epoch 391/1000\n",
      "  Train Loss: 0.0262\n",
      "Epoch 392/1000\n",
      "  Train Loss: 0.0262\n",
      "Epoch 393/1000\n",
      "  Train Loss: 0.0262\n",
      "Epoch 394/1000\n",
      "  Train Loss: 0.0262\n",
      "Epoch 395/1000\n",
      "  Train Loss: 0.0262\n",
      "Epoch 396/1000\n",
      "  Train Loss: 0.0262\n",
      "Epoch 397/1000\n",
      "  Train Loss: 0.0262\n",
      "Epoch 398/1000\n",
      "  Train Loss: 0.0262\n",
      "Epoch 399/1000\n",
      "  Train Loss: 0.0261\n",
      "Epoch 400/1000\n",
      "  Train Loss: 0.0261\n",
      "Epoch 401/1000\n",
      "  Train Loss: 0.0261\n",
      "Epoch 402/1000\n",
      "  Train Loss: 0.0261\n",
      "Epoch 403/1000\n",
      "  Train Loss: 0.0261\n",
      "Epoch 404/1000\n",
      "  Train Loss: 0.0261\n",
      "Epoch 405/1000\n",
      "  Train Loss: 0.0261\n",
      "Epoch 406/1000\n",
      "  Train Loss: 0.0261\n",
      "Epoch 407/1000\n",
      "  Train Loss: 0.0261\n",
      "Epoch 408/1000\n",
      "  Train Loss: 0.0260\n",
      "Epoch 409/1000\n",
      "  Train Loss: 0.0260\n",
      "Epoch 410/1000\n",
      "  Train Loss: 0.0260\n",
      "Epoch 411/1000\n",
      "  Train Loss: 0.0260\n",
      "Epoch 412/1000\n",
      "  Train Loss: 0.0260\n",
      "Epoch 413/1000\n",
      "  Train Loss: 0.0260\n",
      "Epoch 414/1000\n",
      "  Train Loss: 0.0260\n",
      "Epoch 415/1000\n",
      "  Train Loss: 0.0261\n",
      "Epoch 416/1000\n",
      "  Train Loss: 0.0260\n",
      "Epoch 417/1000\n",
      "  Train Loss: 0.0262\n",
      "Epoch 418/1000\n",
      "  Train Loss: 0.0262\n",
      "Epoch 419/1000\n",
      "  Train Loss: 0.0265\n",
      "Epoch 420/1000\n",
      "  Train Loss: 0.0264\n",
      "Epoch 421/1000\n",
      "  Train Loss: 0.0267\n",
      "Epoch 422/1000\n",
      "  Train Loss: 0.0264\n",
      "Epoch 423/1000\n",
      "  Train Loss: 0.0266\n",
      "Epoch 424/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 425/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 426/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 427/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 428/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 429/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 430/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 431/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 432/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 433/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 434/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 435/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 436/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 437/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 438/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 439/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 440/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 441/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 442/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 443/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 444/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 445/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 446/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 447/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 448/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 449/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 450/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 451/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 452/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 453/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 454/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 455/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 456/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 457/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 458/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 459/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 460/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 461/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 462/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 463/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 464/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 465/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 466/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 467/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 468/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 469/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 470/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 471/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 472/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 473/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 474/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 475/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 476/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 477/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 478/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 479/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 480/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 481/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 482/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 483/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 484/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 485/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 486/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 487/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 488/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 489/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 490/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 491/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 492/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 493/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 494/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 495/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 496/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 497/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 498/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 499/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 500/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 501/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 502/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 503/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 504/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 505/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 506/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 507/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 508/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 509/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 510/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 511/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 512/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 513/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 514/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 515/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 516/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 517/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 518/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 519/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 520/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 521/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 522/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 523/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 524/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 525/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 526/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 527/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 528/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 529/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 530/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 531/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 532/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 533/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 534/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 535/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 536/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 537/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 538/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 539/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 540/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 541/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 542/1000\n",
      "  Train Loss: 0.0254\n",
      "Epoch 543/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 544/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 545/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 546/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 547/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 548/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 549/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 550/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 551/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 552/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 553/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 554/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 555/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 556/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 557/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 558/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 559/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 560/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 561/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 562/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 563/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 564/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 565/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 566/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 567/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 568/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 569/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 570/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 571/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 572/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 573/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 574/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 575/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 576/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 577/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 578/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 579/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 580/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 581/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 582/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 583/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 584/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 585/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 586/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 587/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 588/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 589/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 590/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 591/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 592/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 593/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 594/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 595/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 596/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 597/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 598/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 599/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 600/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 601/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 602/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 603/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 604/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 605/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 606/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 607/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 608/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 609/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 610/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 611/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 612/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 613/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 614/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 615/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 616/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 617/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 618/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 619/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 620/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 621/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 622/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 623/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 624/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 625/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 626/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 627/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 628/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 629/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 630/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 631/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 632/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 633/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 634/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 635/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 636/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 637/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 638/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 639/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 640/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 641/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 642/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 643/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 644/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 645/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 646/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 647/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 648/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 649/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 650/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 651/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 652/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 653/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 654/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 655/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 656/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 657/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 658/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 659/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 660/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 661/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 662/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 663/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 664/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 665/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 666/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 667/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 668/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 669/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 670/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 671/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 672/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 673/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 674/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 675/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 676/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 677/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 678/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 679/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 680/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 681/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 682/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 683/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 684/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 685/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 686/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 687/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 688/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 689/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 690/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 691/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 692/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 693/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 694/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 695/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 696/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 697/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 698/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 699/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 700/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 701/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 702/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 703/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 704/1000\n",
      "  Train Loss: 0.0252\n",
      "Epoch 705/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 706/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 707/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 708/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 709/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 710/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 711/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 712/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 713/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 714/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 715/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 716/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 717/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 718/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 719/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 720/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 721/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 722/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 723/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 724/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 725/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 726/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 727/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 728/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 729/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 730/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 731/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 732/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 733/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 734/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 735/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 736/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 737/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 738/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 739/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 740/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 741/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 742/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 743/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 744/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 745/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 746/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 747/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 748/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 749/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 750/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 751/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 752/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 753/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 754/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 755/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 756/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 757/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 758/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 759/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 760/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 761/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 762/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 763/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 764/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 765/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 766/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 767/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 768/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 769/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 770/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 771/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 772/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 773/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 774/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 775/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 776/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 777/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 778/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 779/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 780/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 781/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 782/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 783/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 784/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 785/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 786/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 787/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 788/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 789/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 790/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 791/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 792/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 793/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 794/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 795/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 796/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 797/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 798/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 799/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 800/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 801/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 802/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 803/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 804/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 805/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 806/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 807/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 808/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 809/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 810/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 811/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 812/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 813/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 814/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 815/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 816/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 817/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 818/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 819/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 820/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 821/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 822/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 823/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 824/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 825/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 826/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 827/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 828/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 829/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 830/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 831/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 832/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 833/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 834/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 835/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 836/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 837/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 838/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 839/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 840/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 841/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 842/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 843/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 844/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 845/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 846/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 847/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 848/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 849/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 850/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 851/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 852/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 853/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 854/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 855/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 856/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 857/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 858/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 859/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 860/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 861/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 862/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 863/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 864/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 865/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 866/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 867/1000\n",
      "  Train Loss: 0.0250\n",
      "Epoch 868/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 869/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 870/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 871/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 872/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 873/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 874/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 875/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 876/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 877/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 878/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 879/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 880/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 881/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 882/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 883/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 884/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 885/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 886/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 887/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 888/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 889/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 890/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 891/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 892/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 893/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 894/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 895/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 896/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 897/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 898/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 899/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 900/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 901/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 902/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 903/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 904/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 905/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 906/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 907/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 908/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 909/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 910/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 911/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 912/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 913/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 914/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 915/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 916/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 917/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 918/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 919/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 920/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 921/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 922/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 923/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 924/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 925/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 926/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 927/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 928/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 929/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 930/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 931/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 932/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 933/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 934/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 935/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 936/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 937/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 938/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 939/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 940/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 941/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 942/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 943/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 944/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 945/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 946/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 947/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 948/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 949/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 950/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 951/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 952/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 953/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 954/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 955/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 956/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 957/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 958/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 959/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 960/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 961/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 962/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 963/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 964/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 965/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 966/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 967/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 968/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 969/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 970/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 971/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 972/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 973/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 974/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 975/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 976/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 977/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 978/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 979/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 980/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 981/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 982/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 983/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 984/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 985/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 986/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 987/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 988/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 989/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 990/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 991/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 992/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 993/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 994/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 995/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 996/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 997/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 998/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 999/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 1000/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 1/1000\n",
      "  Train Loss: 0.1030\n",
      "Epoch 2/1000\n",
      "  Train Loss: 0.0971\n",
      "Epoch 3/1000\n",
      "  Train Loss: 0.0951\n",
      "Epoch 4/1000\n",
      "  Train Loss: 0.0930\n",
      "Epoch 5/1000\n",
      "  Train Loss: 0.0908\n",
      "Epoch 6/1000\n",
      "  Train Loss: 0.0879\n",
      "Epoch 7/1000\n",
      "  Train Loss: 0.0843\n",
      "Epoch 8/1000\n",
      "  Train Loss: 0.0799\n",
      "Epoch 9/1000\n",
      "  Train Loss: 0.0751\n",
      "Epoch 10/1000\n",
      "  Train Loss: 0.0705\n",
      "Epoch 11/1000\n",
      "  Train Loss: 0.0661\n",
      "Epoch 12/1000\n",
      "  Train Loss: 0.0623\n",
      "Epoch 13/1000\n",
      "  Train Loss: 0.0588\n",
      "Epoch 14/1000\n",
      "  Train Loss: 0.0558\n",
      "Epoch 15/1000\n",
      "  Train Loss: 0.0531\n",
      "Epoch 16/1000\n",
      "  Train Loss: 0.0506\n",
      "Epoch 17/1000\n",
      "  Train Loss: 0.0484\n",
      "Epoch 18/1000\n",
      "  Train Loss: 0.0464\n",
      "Epoch 19/1000\n",
      "  Train Loss: 0.0446\n",
      "Epoch 20/1000\n",
      "  Train Loss: 0.0430\n",
      "Epoch 21/1000\n",
      "  Train Loss: 0.0415\n",
      "Epoch 22/1000\n",
      "  Train Loss: 0.0402\n",
      "Epoch 23/1000\n",
      "  Train Loss: 0.0391\n",
      "Epoch 24/1000\n",
      "  Train Loss: 0.0380\n",
      "Epoch 25/1000\n",
      "  Train Loss: 0.0371\n",
      "Epoch 26/1000\n",
      "  Train Loss: 0.0363\n",
      "Epoch 27/1000\n",
      "  Train Loss: 0.0355\n",
      "Epoch 28/1000\n",
      "  Train Loss: 0.0349\n",
      "Epoch 29/1000\n",
      "  Train Loss: 0.0342\n",
      "Epoch 30/1000\n",
      "  Train Loss: 0.0337\n",
      "Epoch 31/1000\n",
      "  Train Loss: 0.0332\n",
      "Epoch 32/1000\n",
      "  Train Loss: 0.0327\n",
      "Epoch 33/1000\n",
      "  Train Loss: 0.0323\n",
      "Epoch 34/1000\n",
      "  Train Loss: 0.0319\n",
      "Epoch 35/1000\n",
      "  Train Loss: 0.0315\n",
      "Epoch 36/1000\n",
      "  Train Loss: 0.0312\n",
      "Epoch 37/1000\n",
      "  Train Loss: 0.0308\n",
      "Epoch 38/1000\n",
      "  Train Loss: 0.0305\n",
      "Epoch 39/1000\n",
      "  Train Loss: 0.0303\n",
      "Epoch 40/1000\n",
      "  Train Loss: 0.0300\n",
      "Epoch 41/1000\n",
      "  Train Loss: 0.0297\n",
      "Epoch 42/1000\n",
      "  Train Loss: 0.0295\n",
      "Epoch 43/1000\n",
      "  Train Loss: 0.0292\n",
      "Epoch 44/1000\n",
      "  Train Loss: 0.0290\n",
      "Epoch 45/1000\n",
      "  Train Loss: 0.0288\n",
      "Epoch 46/1000\n",
      "  Train Loss: 0.0286\n",
      "Epoch 47/1000\n",
      "  Train Loss: 0.0284\n",
      "Epoch 48/1000\n",
      "  Train Loss: 0.0282\n",
      "Epoch 49/1000\n",
      "  Train Loss: 0.0279\n",
      "Epoch 50/1000\n",
      "  Train Loss: 0.0278\n",
      "Epoch 51/1000\n",
      "  Train Loss: 0.0276\n",
      "Epoch 52/1000\n",
      "  Train Loss: 0.0274\n",
      "Epoch 53/1000\n",
      "  Train Loss: 0.0272\n",
      "Epoch 54/1000\n",
      "  Train Loss: 0.0270\n",
      "Epoch 55/1000\n",
      "  Train Loss: 0.0268\n",
      "Epoch 56/1000\n",
      "  Train Loss: 0.0266\n",
      "Epoch 57/1000\n",
      "  Train Loss: 0.0264\n",
      "Epoch 58/1000\n",
      "  Train Loss: 0.0262\n",
      "Epoch 59/1000\n",
      "  Train Loss: 0.0261\n",
      "Epoch 60/1000\n",
      "  Train Loss: 0.0259\n",
      "Epoch 61/1000\n",
      "  Train Loss: 0.0257\n",
      "Epoch 62/1000\n",
      "  Train Loss: 0.0255\n",
      "Epoch 63/1000\n",
      "  Train Loss: 0.0253\n",
      "Epoch 64/1000\n",
      "  Train Loss: 0.0251\n",
      "Epoch 65/1000\n",
      "  Train Loss: 0.0249\n",
      "Epoch 66/1000\n",
      "  Train Loss: 0.0248\n",
      "Epoch 67/1000\n",
      "  Train Loss: 0.0246\n",
      "Epoch 68/1000\n",
      "  Train Loss: 0.0244\n",
      "Epoch 69/1000\n",
      "  Train Loss: 0.0242\n",
      "Epoch 70/1000\n",
      "  Train Loss: 0.0240\n",
      "Epoch 71/1000\n",
      "  Train Loss: 0.0238\n",
      "Epoch 72/1000\n",
      "  Train Loss: 0.0237\n",
      "Epoch 73/1000\n",
      "  Train Loss: 0.0235\n",
      "Epoch 74/1000\n",
      "  Train Loss: 0.0233\n",
      "Epoch 75/1000\n",
      "  Train Loss: 0.0231\n",
      "Epoch 76/1000\n",
      "  Train Loss: 0.0230\n",
      "Epoch 77/1000\n",
      "  Train Loss: 0.0228\n",
      "Epoch 78/1000\n",
      "  Train Loss: 0.0227\n",
      "Epoch 79/1000\n",
      "  Train Loss: 0.0225\n",
      "Epoch 80/1000\n",
      "  Train Loss: 0.0223\n",
      "Epoch 81/1000\n",
      "  Train Loss: 0.0222\n",
      "Epoch 82/1000\n",
      "  Train Loss: 0.0220\n",
      "Epoch 83/1000\n",
      "  Train Loss: 0.0219\n",
      "Epoch 84/1000\n",
      "  Train Loss: 0.0217\n",
      "Epoch 85/1000\n",
      "  Train Loss: 0.0216\n",
      "Epoch 86/1000\n",
      "  Train Loss: 0.0214\n",
      "Epoch 87/1000\n",
      "  Train Loss: 0.0213\n",
      "Epoch 88/1000\n",
      "  Train Loss: 0.0211\n",
      "Epoch 89/1000\n",
      "  Train Loss: 0.0210\n",
      "Epoch 90/1000\n",
      "  Train Loss: 0.0209\n",
      "Epoch 91/1000\n",
      "  Train Loss: 0.0207\n",
      "Epoch 92/1000\n",
      "  Train Loss: 0.0206\n",
      "Epoch 93/1000\n",
      "  Train Loss: 0.0205\n",
      "Epoch 94/1000\n",
      "  Train Loss: 0.0204\n",
      "Epoch 95/1000\n",
      "  Train Loss: 0.0202\n",
      "Epoch 96/1000\n",
      "  Train Loss: 0.0201\n",
      "Epoch 97/1000\n",
      "  Train Loss: 0.0200\n",
      "Epoch 98/1000\n",
      "  Train Loss: 0.0199\n",
      "Epoch 99/1000\n",
      "  Train Loss: 0.0198\n",
      "Epoch 100/1000\n",
      "  Train Loss: 0.0197\n",
      "Epoch 101/1000\n",
      "  Train Loss: 0.0195\n",
      "Epoch 102/1000\n",
      "  Train Loss: 0.0194\n",
      "Epoch 103/1000\n",
      "  Train Loss: 0.0193\n",
      "Epoch 104/1000\n",
      "  Train Loss: 0.0192\n",
      "Epoch 105/1000\n",
      "  Train Loss: 0.0191\n",
      "Epoch 106/1000\n",
      "  Train Loss: 0.0190\n",
      "Epoch 107/1000\n",
      "  Train Loss: 0.0189\n",
      "Epoch 108/1000\n",
      "  Train Loss: 0.0188\n",
      "Epoch 109/1000\n",
      "  Train Loss: 0.0187\n",
      "Epoch 110/1000\n",
      "  Train Loss: 0.0186\n",
      "Epoch 111/1000\n",
      "  Train Loss: 0.0186\n",
      "Epoch 112/1000\n",
      "  Train Loss: 0.0185\n",
      "Epoch 113/1000\n",
      "  Train Loss: 0.0184\n",
      "Epoch 114/1000\n",
      "  Train Loss: 0.0183\n",
      "Epoch 115/1000\n",
      "  Train Loss: 0.0182\n",
      "Epoch 116/1000\n",
      "  Train Loss: 0.0181\n",
      "Epoch 117/1000\n",
      "  Train Loss: 0.0180\n",
      "Epoch 118/1000\n",
      "  Train Loss: 0.0179\n",
      "Epoch 119/1000\n",
      "  Train Loss: 0.0179\n",
      "Epoch 120/1000\n",
      "  Train Loss: 0.0178\n",
      "Epoch 121/1000\n",
      "  Train Loss: 0.0177\n",
      "Epoch 122/1000\n",
      "  Train Loss: 0.0176\n",
      "Epoch 123/1000\n",
      "  Train Loss: 0.0175\n",
      "Epoch 124/1000\n",
      "  Train Loss: 0.0175\n",
      "Epoch 125/1000\n",
      "  Train Loss: 0.0174\n",
      "Epoch 126/1000\n",
      "  Train Loss: 0.0173\n",
      "Epoch 127/1000\n",
      "  Train Loss: 0.0172\n",
      "Epoch 128/1000\n",
      "  Train Loss: 0.0172\n",
      "Epoch 129/1000\n",
      "  Train Loss: 0.0171\n",
      "Epoch 130/1000\n",
      "  Train Loss: 0.0170\n",
      "Epoch 131/1000\n",
      "  Train Loss: 0.0169\n",
      "Epoch 132/1000\n",
      "  Train Loss: 0.0169\n",
      "Epoch 133/1000\n",
      "  Train Loss: 0.0168\n",
      "Epoch 134/1000\n",
      "  Train Loss: 0.0167\n",
      "Epoch 135/1000\n",
      "  Train Loss: 0.0166\n",
      "Epoch 136/1000\n",
      "  Train Loss: 0.0166\n",
      "Epoch 137/1000\n",
      "  Train Loss: 0.0165\n",
      "Epoch 138/1000\n",
      "  Train Loss: 0.0164\n",
      "Epoch 139/1000\n",
      "  Train Loss: 0.0164\n",
      "Epoch 140/1000\n",
      "  Train Loss: 0.0163\n",
      "Epoch 141/1000\n",
      "  Train Loss: 0.0162\n",
      "Epoch 142/1000\n",
      "  Train Loss: 0.0162\n",
      "Epoch 143/1000\n",
      "  Train Loss: 0.0161\n",
      "Epoch 144/1000\n",
      "  Train Loss: 0.0160\n",
      "Epoch 145/1000\n",
      "  Train Loss: 0.0160\n",
      "Epoch 146/1000\n",
      "  Train Loss: 0.0159\n",
      "Epoch 147/1000\n",
      "  Train Loss: 0.0158\n",
      "Epoch 148/1000\n",
      "  Train Loss: 0.0158\n",
      "Epoch 149/1000\n",
      "  Train Loss: 0.0157\n",
      "Epoch 150/1000\n",
      "  Train Loss: 0.0156\n",
      "Epoch 151/1000\n",
      "  Train Loss: 0.0156\n",
      "Epoch 152/1000\n",
      "  Train Loss: 0.0155\n",
      "Epoch 153/1000\n",
      "  Train Loss: 0.0154\n",
      "Epoch 154/1000\n",
      "  Train Loss: 0.0154\n",
      "Epoch 155/1000\n",
      "  Train Loss: 0.0153\n",
      "Epoch 156/1000\n",
      "  Train Loss: 0.0153\n",
      "Epoch 157/1000\n",
      "  Train Loss: 0.0152\n",
      "Epoch 158/1000\n",
      "  Train Loss: 0.0151\n",
      "Epoch 159/1000\n",
      "  Train Loss: 0.0151\n",
      "Epoch 160/1000\n",
      "  Train Loss: 0.0150\n",
      "Epoch 161/1000\n",
      "  Train Loss: 0.0150\n",
      "Epoch 162/1000\n",
      "  Train Loss: 0.0149\n",
      "Epoch 163/1000\n",
      "  Train Loss: 0.0148\n",
      "Epoch 164/1000\n",
      "  Train Loss: 0.0148\n",
      "Epoch 165/1000\n",
      "  Train Loss: 0.0147\n",
      "Epoch 166/1000\n",
      "  Train Loss: 0.0146\n",
      "Epoch 167/1000\n",
      "  Train Loss: 0.0146\n",
      "Epoch 168/1000\n",
      "  Train Loss: 0.0145\n",
      "Epoch 169/1000\n",
      "  Train Loss: 0.0145\n",
      "Epoch 170/1000\n",
      "  Train Loss: 0.0144\n",
      "Epoch 171/1000\n",
      "  Train Loss: 0.0143\n",
      "Epoch 172/1000\n",
      "  Train Loss: 0.0143\n",
      "Epoch 173/1000\n",
      "  Train Loss: 0.0142\n",
      "Epoch 174/1000\n",
      "  Train Loss: 0.0142\n",
      "Epoch 175/1000\n",
      "  Train Loss: 0.0141\n",
      "Epoch 176/1000\n",
      "  Train Loss: 0.0140\n",
      "Epoch 177/1000\n",
      "  Train Loss: 0.0140\n",
      "Epoch 178/1000\n",
      "  Train Loss: 0.0139\n",
      "Epoch 179/1000\n",
      "  Train Loss: 0.0139\n",
      "Epoch 180/1000\n",
      "  Train Loss: 0.0138\n",
      "Epoch 181/1000\n",
      "  Train Loss: 0.0138\n",
      "Epoch 182/1000\n",
      "  Train Loss: 0.0137\n",
      "Epoch 183/1000\n",
      "  Train Loss: 0.0136\n",
      "Epoch 184/1000\n",
      "  Train Loss: 0.0136\n",
      "Epoch 185/1000\n",
      "  Train Loss: 0.0135\n",
      "Epoch 186/1000\n",
      "  Train Loss: 0.0135\n",
      "Epoch 187/1000\n",
      "  Train Loss: 0.0134\n",
      "Epoch 188/1000\n",
      "  Train Loss: 0.0133\n",
      "Epoch 189/1000\n",
      "  Train Loss: 0.0133\n",
      "Epoch 190/1000\n",
      "  Train Loss: 0.0132\n",
      "Epoch 191/1000\n",
      "  Train Loss: 0.0132\n",
      "Epoch 192/1000\n",
      "  Train Loss: 0.0131\n",
      "Epoch 193/1000\n",
      "  Train Loss: 0.0130\n",
      "Epoch 194/1000\n",
      "  Train Loss: 0.0130\n",
      "Epoch 195/1000\n",
      "  Train Loss: 0.0129\n",
      "Epoch 196/1000\n",
      "  Train Loss: 0.0129\n",
      "Epoch 197/1000\n",
      "  Train Loss: 0.0128\n",
      "Epoch 198/1000\n",
      "  Train Loss: 0.0128\n",
      "Epoch 199/1000\n",
      "  Train Loss: 0.0127\n",
      "Epoch 200/1000\n",
      "  Train Loss: 0.0126\n",
      "Epoch 201/1000\n",
      "  Train Loss: 0.0126\n",
      "Epoch 202/1000\n",
      "  Train Loss: 0.0125\n",
      "Epoch 203/1000\n",
      "  Train Loss: 0.0125\n",
      "Epoch 204/1000\n",
      "  Train Loss: 0.0125\n",
      "Epoch 205/1000\n",
      "  Train Loss: 0.0125\n",
      "Epoch 206/1000\n",
      "  Train Loss: 0.0128\n",
      "Epoch 207/1000\n",
      "  Train Loss: 0.0141\n",
      "Epoch 208/1000\n",
      "  Train Loss: 0.0170\n",
      "Epoch 209/1000\n",
      "  Train Loss: 0.0264\n",
      "Epoch 210/1000\n",
      "  Train Loss: 0.0335\n",
      "Epoch 211/1000\n",
      "  Train Loss: 0.0162\n",
      "Epoch 212/1000\n",
      "  Train Loss: 0.0146\n",
      "Epoch 213/1000\n",
      "  Train Loss: 0.0137\n",
      "Epoch 214/1000\n",
      "  Train Loss: 0.0130\n",
      "Epoch 215/1000\n",
      "  Train Loss: 0.0126\n",
      "Epoch 216/1000\n",
      "  Train Loss: 0.0124\n",
      "Epoch 217/1000\n",
      "  Train Loss: 0.0123\n",
      "Epoch 218/1000\n",
      "  Train Loss: 0.0122\n",
      "Epoch 219/1000\n",
      "  Train Loss: 0.0122\n",
      "Epoch 220/1000\n",
      "  Train Loss: 0.0122\n",
      "Epoch 221/1000\n",
      "  Train Loss: 0.0122\n",
      "Epoch 222/1000\n",
      "  Train Loss: 0.0122\n",
      "Epoch 223/1000\n",
      "  Train Loss: 0.0122\n",
      "Epoch 224/1000\n",
      "  Train Loss: 0.0121\n",
      "Epoch 225/1000\n",
      "  Train Loss: 0.0121\n",
      "Epoch 226/1000\n",
      "  Train Loss: 0.0121\n",
      "Epoch 227/1000\n",
      "  Train Loss: 0.0121\n",
      "Epoch 228/1000\n",
      "  Train Loss: 0.0121\n",
      "Epoch 229/1000\n",
      "  Train Loss: 0.0121\n",
      "Epoch 230/1000\n",
      "  Train Loss: 0.0121\n",
      "Epoch 231/1000\n",
      "  Train Loss: 0.0121\n",
      "Epoch 232/1000\n",
      "  Train Loss: 0.0121\n",
      "Epoch 233/1000\n",
      "  Train Loss: 0.0121\n",
      "Epoch 234/1000\n",
      "  Train Loss: 0.0120\n",
      "Epoch 235/1000\n",
      "  Train Loss: 0.0120\n",
      "Epoch 236/1000\n",
      "  Train Loss: 0.0120\n",
      "Epoch 237/1000\n",
      "  Train Loss: 0.0120\n",
      "Epoch 238/1000\n",
      "  Train Loss: 0.0120\n",
      "Epoch 239/1000\n",
      "  Train Loss: 0.0120\n",
      "Epoch 240/1000\n",
      "  Train Loss: 0.0120\n",
      "Epoch 241/1000\n",
      "  Train Loss: 0.0120\n",
      "Epoch 242/1000\n",
      "  Train Loss: 0.0120\n",
      "Epoch 243/1000\n",
      "  Train Loss: 0.0120\n",
      "Epoch 244/1000\n",
      "  Train Loss: 0.0120\n",
      "Epoch 245/1000\n",
      "  Train Loss: 0.0120\n",
      "Epoch 246/1000\n",
      "  Train Loss: 0.0119\n",
      "Epoch 247/1000\n",
      "  Train Loss: 0.0119\n",
      "Epoch 248/1000\n",
      "  Train Loss: 0.0119\n",
      "Epoch 249/1000\n",
      "  Train Loss: 0.0119\n",
      "Epoch 250/1000\n",
      "  Train Loss: 0.0119\n",
      "Epoch 251/1000\n",
      "  Train Loss: 0.0119\n",
      "Epoch 252/1000\n",
      "  Train Loss: 0.0119\n",
      "Epoch 253/1000\n",
      "  Train Loss: 0.0119\n",
      "Epoch 254/1000\n",
      "  Train Loss: 0.0119\n",
      "Epoch 255/1000\n",
      "  Train Loss: 0.0119\n",
      "Epoch 256/1000\n",
      "  Train Loss: 0.0119\n",
      "Epoch 257/1000\n",
      "  Train Loss: 0.0119\n",
      "Epoch 258/1000\n",
      "  Train Loss: 0.0119\n",
      "Epoch 259/1000\n",
      "  Train Loss: 0.0119\n",
      "Epoch 260/1000\n",
      "  Train Loss: 0.0118\n",
      "Epoch 261/1000\n",
      "  Train Loss: 0.0118\n",
      "Epoch 262/1000\n",
      "  Train Loss: 0.0118\n",
      "Epoch 263/1000\n",
      "  Train Loss: 0.0118\n",
      "Epoch 264/1000\n",
      "  Train Loss: 0.0118\n",
      "Epoch 265/1000\n",
      "  Train Loss: 0.0118\n",
      "Epoch 266/1000\n",
      "  Train Loss: 0.0118\n",
      "Epoch 267/1000\n",
      "  Train Loss: 0.0118\n",
      "Epoch 268/1000\n",
      "  Train Loss: 0.0118\n",
      "Epoch 269/1000\n",
      "  Train Loss: 0.0118\n",
      "Epoch 270/1000\n",
      "  Train Loss: 0.0118\n",
      "Epoch 271/1000\n",
      "  Train Loss: 0.0118\n",
      "Epoch 272/1000\n",
      "  Train Loss: 0.0118\n",
      "Epoch 273/1000\n",
      "  Train Loss: 0.0118\n",
      "Epoch 274/1000\n",
      "  Train Loss: 0.0118\n",
      "Epoch 275/1000\n",
      "  Train Loss: 0.0117\n",
      "Epoch 276/1000\n",
      "  Train Loss: 0.0117\n",
      "Epoch 277/1000\n",
      "  Train Loss: 0.0117\n",
      "Epoch 278/1000\n",
      "  Train Loss: 0.0117\n",
      "Epoch 279/1000\n",
      "  Train Loss: 0.0117\n",
      "Epoch 280/1000\n",
      "  Train Loss: 0.0117\n",
      "Epoch 281/1000\n",
      "  Train Loss: 0.0117\n",
      "Epoch 282/1000\n",
      "  Train Loss: 0.0117\n",
      "Epoch 283/1000\n",
      "  Train Loss: 0.0117\n",
      "Epoch 284/1000\n",
      "  Train Loss: 0.0117\n",
      "Epoch 285/1000\n",
      "  Train Loss: 0.0117\n",
      "Epoch 286/1000\n",
      "  Train Loss: 0.0117\n",
      "Epoch 287/1000\n",
      "  Train Loss: 0.0117\n",
      "Epoch 288/1000\n",
      "  Train Loss: 0.0117\n",
      "Epoch 289/1000\n",
      "  Train Loss: 0.0116\n",
      "Epoch 290/1000\n",
      "  Train Loss: 0.0116\n",
      "Epoch 291/1000\n",
      "  Train Loss: 0.0116\n",
      "Epoch 292/1000\n",
      "  Train Loss: 0.0116\n",
      "Epoch 293/1000\n",
      "  Train Loss: 0.0116\n",
      "Epoch 294/1000\n",
      "  Train Loss: 0.0116\n",
      "Epoch 295/1000\n",
      "  Train Loss: 0.0116\n",
      "Epoch 296/1000\n",
      "  Train Loss: 0.0116\n",
      "Epoch 297/1000\n",
      "  Train Loss: 0.0116\n",
      "Epoch 298/1000\n",
      "  Train Loss: 0.0116\n",
      "Epoch 299/1000\n",
      "  Train Loss: 0.0116\n",
      "Epoch 300/1000\n",
      "  Train Loss: 0.0116\n",
      "Epoch 301/1000\n",
      "  Train Loss: 0.0116\n",
      "Epoch 302/1000\n",
      "  Train Loss: 0.0116\n",
      "Epoch 303/1000\n",
      "  Train Loss: 0.0116\n",
      "Epoch 304/1000\n",
      "  Train Loss: 0.0115\n",
      "Epoch 305/1000\n",
      "  Train Loss: 0.0115\n",
      "Epoch 306/1000\n",
      "  Train Loss: 0.0115\n",
      "Epoch 307/1000\n",
      "  Train Loss: 0.0115\n",
      "Epoch 308/1000\n",
      "  Train Loss: 0.0115\n",
      "Epoch 309/1000\n",
      "  Train Loss: 0.0115\n",
      "Epoch 310/1000\n",
      "  Train Loss: 0.0115\n",
      "Epoch 311/1000\n",
      "  Train Loss: 0.0115\n",
      "Epoch 312/1000\n",
      "  Train Loss: 0.0115\n",
      "Epoch 313/1000\n",
      "  Train Loss: 0.0115\n",
      "Epoch 314/1000\n",
      "  Train Loss: 0.0115\n",
      "Epoch 315/1000\n",
      "  Train Loss: 0.0115\n",
      "Epoch 316/1000\n",
      "  Train Loss: 0.0115\n",
      "Epoch 317/1000\n",
      "  Train Loss: 0.0115\n",
      "Epoch 318/1000\n",
      "  Train Loss: 0.0115\n",
      "Epoch 319/1000\n",
      "  Train Loss: 0.0114\n",
      "Epoch 320/1000\n",
      "  Train Loss: 0.0114\n",
      "Epoch 321/1000\n",
      "  Train Loss: 0.0114\n",
      "Epoch 322/1000\n",
      "  Train Loss: 0.0114\n",
      "Epoch 323/1000\n",
      "  Train Loss: 0.0114\n",
      "Epoch 324/1000\n",
      "  Train Loss: 0.0114\n",
      "Epoch 325/1000\n",
      "  Train Loss: 0.0114\n",
      "Epoch 326/1000\n",
      "  Train Loss: 0.0114\n",
      "Epoch 327/1000\n",
      "  Train Loss: 0.0114\n",
      "Epoch 328/1000\n",
      "  Train Loss: 0.0114\n",
      "Epoch 329/1000\n",
      "  Train Loss: 0.0114\n",
      "Epoch 330/1000\n",
      "  Train Loss: 0.0114\n",
      "Epoch 331/1000\n",
      "  Train Loss: 0.0114\n",
      "Epoch 332/1000\n",
      "  Train Loss: 0.0114\n",
      "Epoch 333/1000\n",
      "  Train Loss: 0.0113\n",
      "Epoch 334/1000\n",
      "  Train Loss: 0.0113\n",
      "Epoch 335/1000\n",
      "  Train Loss: 0.0113\n",
      "Epoch 336/1000\n",
      "  Train Loss: 0.0113\n",
      "Epoch 337/1000\n",
      "  Train Loss: 0.0113\n",
      "Epoch 338/1000\n",
      "  Train Loss: 0.0113\n",
      "Epoch 339/1000\n",
      "  Train Loss: 0.0113\n",
      "Epoch 340/1000\n",
      "  Train Loss: 0.0113\n",
      "Epoch 341/1000\n",
      "  Train Loss: 0.0113\n",
      "Epoch 342/1000\n",
      "  Train Loss: 0.0113\n",
      "Epoch 343/1000\n",
      "  Train Loss: 0.0113\n",
      "Epoch 344/1000\n",
      "  Train Loss: 0.0113\n",
      "Epoch 345/1000\n",
      "  Train Loss: 0.0113\n",
      "Epoch 346/1000\n",
      "  Train Loss: 0.0113\n",
      "Epoch 347/1000\n",
      "  Train Loss: 0.0113\n",
      "Epoch 348/1000\n",
      "  Train Loss: 0.0112\n",
      "Epoch 349/1000\n",
      "  Train Loss: 0.0112\n",
      "Epoch 350/1000\n",
      "  Train Loss: 0.0112\n",
      "Epoch 351/1000\n",
      "  Train Loss: 0.0112\n",
      "Epoch 352/1000\n",
      "  Train Loss: 0.0112\n",
      "Epoch 353/1000\n",
      "  Train Loss: 0.0112\n",
      "Epoch 354/1000\n",
      "  Train Loss: 0.0112\n",
      "Epoch 355/1000\n",
      "  Train Loss: 0.0112\n",
      "Epoch 356/1000\n",
      "  Train Loss: 0.0112\n",
      "Epoch 357/1000\n",
      "  Train Loss: 0.0112\n",
      "Epoch 358/1000\n",
      "  Train Loss: 0.0112\n",
      "Epoch 359/1000\n",
      "  Train Loss: 0.0112\n",
      "Epoch 360/1000\n",
      "  Train Loss: 0.0112\n",
      "Epoch 361/1000\n",
      "  Train Loss: 0.0112\n",
      "Epoch 362/1000\n",
      "  Train Loss: 0.0111\n",
      "Epoch 363/1000\n",
      "  Train Loss: 0.0111\n",
      "Epoch 364/1000\n",
      "  Train Loss: 0.0111\n",
      "Epoch 365/1000\n",
      "  Train Loss: 0.0111\n",
      "Epoch 366/1000\n",
      "  Train Loss: 0.0111\n",
      "Epoch 367/1000\n",
      "  Train Loss: 0.0111\n",
      "Epoch 368/1000\n",
      "  Train Loss: 0.0111\n",
      "Epoch 369/1000\n",
      "  Train Loss: 0.0111\n",
      "Epoch 370/1000\n",
      "  Train Loss: 0.0111\n",
      "Epoch 371/1000\n",
      "  Train Loss: 0.0111\n",
      "Epoch 372/1000\n",
      "  Train Loss: 0.0111\n",
      "Epoch 373/1000\n",
      "  Train Loss: 0.0111\n",
      "Epoch 374/1000\n",
      "  Train Loss: 0.0111\n",
      "Epoch 375/1000\n",
      "  Train Loss: 0.0111\n",
      "Epoch 376/1000\n",
      "  Train Loss: 0.0110\n",
      "Epoch 377/1000\n",
      "  Train Loss: 0.0110\n",
      "Epoch 378/1000\n",
      "  Train Loss: 0.0110\n",
      "Epoch 379/1000\n",
      "  Train Loss: 0.0110\n",
      "Epoch 380/1000\n",
      "  Train Loss: 0.0110\n",
      "Epoch 381/1000\n",
      "  Train Loss: 0.0110\n",
      "Epoch 382/1000\n",
      "  Train Loss: 0.0110\n",
      "Epoch 383/1000\n",
      "  Train Loss: 0.0110\n",
      "Epoch 384/1000\n",
      "  Train Loss: 0.0110\n",
      "Epoch 385/1000\n",
      "  Train Loss: 0.0110\n",
      "Epoch 386/1000\n",
      "  Train Loss: 0.0110\n",
      "Epoch 387/1000\n",
      "  Train Loss: 0.0110\n",
      "Epoch 388/1000\n",
      "  Train Loss: 0.0110\n",
      "Epoch 389/1000\n",
      "  Train Loss: 0.0110\n",
      "Epoch 390/1000\n",
      "  Train Loss: 0.0109\n",
      "Epoch 391/1000\n",
      "  Train Loss: 0.0109\n",
      "Epoch 392/1000\n",
      "  Train Loss: 0.0109\n",
      "Epoch 393/1000\n",
      "  Train Loss: 0.0109\n",
      "Epoch 394/1000\n",
      "  Train Loss: 0.0109\n",
      "Epoch 395/1000\n",
      "  Train Loss: 0.0109\n",
      "Epoch 396/1000\n",
      "  Train Loss: 0.0109\n",
      "Epoch 397/1000\n",
      "  Train Loss: 0.0109\n",
      "Epoch 398/1000\n",
      "  Train Loss: 0.0109\n",
      "Epoch 399/1000\n",
      "  Train Loss: 0.0109\n",
      "Epoch 400/1000\n",
      "  Train Loss: 0.0109\n",
      "Epoch 401/1000\n",
      "  Train Loss: 0.0109\n",
      "Epoch 402/1000\n",
      "  Train Loss: 0.0109\n",
      "Epoch 403/1000\n",
      "  Train Loss: 0.0109\n",
      "Epoch 404/1000\n",
      "  Train Loss: 0.0108\n",
      "Epoch 405/1000\n",
      "  Train Loss: 0.0108\n",
      "Epoch 406/1000\n",
      "  Train Loss: 0.0108\n",
      "Epoch 407/1000\n",
      "  Train Loss: 0.0108\n",
      "Epoch 408/1000\n",
      "  Train Loss: 0.0108\n",
      "Epoch 409/1000\n",
      "  Train Loss: 0.0108\n",
      "Epoch 410/1000\n",
      "  Train Loss: 0.0108\n",
      "Epoch 411/1000\n",
      "  Train Loss: 0.0108\n",
      "Epoch 412/1000\n",
      "  Train Loss: 0.0108\n",
      "Epoch 413/1000\n",
      "  Train Loss: 0.0108\n",
      "Epoch 414/1000\n",
      "  Train Loss: 0.0108\n",
      "Epoch 415/1000\n",
      "  Train Loss: 0.0108\n",
      "Epoch 416/1000\n",
      "  Train Loss: 0.0108\n",
      "Epoch 417/1000\n",
      "  Train Loss: 0.0107\n",
      "Epoch 418/1000\n",
      "  Train Loss: 0.0107\n",
      "Epoch 419/1000\n",
      "  Train Loss: 0.0107\n",
      "Epoch 420/1000\n",
      "  Train Loss: 0.0107\n",
      "Epoch 421/1000\n",
      "  Train Loss: 0.0107\n",
      "Epoch 422/1000\n",
      "  Train Loss: 0.0107\n",
      "Epoch 423/1000\n",
      "  Train Loss: 0.0107\n",
      "Epoch 424/1000\n",
      "  Train Loss: 0.0107\n",
      "Epoch 425/1000\n",
      "  Train Loss: 0.0107\n",
      "Epoch 426/1000\n",
      "  Train Loss: 0.0107\n",
      "Epoch 427/1000\n",
      "  Train Loss: 0.0107\n",
      "Epoch 428/1000\n",
      "  Train Loss: 0.0107\n",
      "Epoch 429/1000\n",
      "  Train Loss: 0.0107\n",
      "Epoch 430/1000\n",
      "  Train Loss: 0.0107\n",
      "Epoch 431/1000\n",
      "  Train Loss: 0.0106\n",
      "Epoch 432/1000\n",
      "  Train Loss: 0.0106\n",
      "Epoch 433/1000\n",
      "  Train Loss: 0.0106\n",
      "Epoch 434/1000\n",
      "  Train Loss: 0.0106\n",
      "Epoch 435/1000\n",
      "  Train Loss: 0.0106\n",
      "Epoch 436/1000\n",
      "  Train Loss: 0.0106\n",
      "Epoch 437/1000\n",
      "  Train Loss: 0.0106\n",
      "Epoch 438/1000\n",
      "  Train Loss: 0.0106\n",
      "Epoch 439/1000\n",
      "  Train Loss: 0.0106\n",
      "Epoch 440/1000\n",
      "  Train Loss: 0.0106\n",
      "Epoch 441/1000\n",
      "  Train Loss: 0.0106\n",
      "Epoch 442/1000\n",
      "  Train Loss: 0.0106\n",
      "Epoch 443/1000\n",
      "  Train Loss: 0.0106\n",
      "Epoch 444/1000\n",
      "  Train Loss: 0.0105\n",
      "Epoch 445/1000\n",
      "  Train Loss: 0.0105\n",
      "Epoch 446/1000\n",
      "  Train Loss: 0.0105\n",
      "Epoch 447/1000\n",
      "  Train Loss: 0.0105\n",
      "Epoch 448/1000\n",
      "  Train Loss: 0.0105\n",
      "Epoch 449/1000\n",
      "  Train Loss: 0.0105\n",
      "Epoch 450/1000\n",
      "  Train Loss: 0.0105\n",
      "Epoch 451/1000\n",
      "  Train Loss: 0.0105\n",
      "Epoch 452/1000\n",
      "  Train Loss: 0.0105\n",
      "Epoch 453/1000\n",
      "  Train Loss: 0.0105\n",
      "Epoch 454/1000\n",
      "  Train Loss: 0.0105\n",
      "Epoch 455/1000\n",
      "  Train Loss: 0.0105\n",
      "Epoch 456/1000\n",
      "  Train Loss: 0.0105\n",
      "Epoch 457/1000\n",
      "  Train Loss: 0.0104\n",
      "Epoch 458/1000\n",
      "  Train Loss: 0.0104\n",
      "Epoch 459/1000\n",
      "  Train Loss: 0.0104\n",
      "Epoch 460/1000\n",
      "  Train Loss: 0.0104\n",
      "Epoch 461/1000\n",
      "  Train Loss: 0.0104\n",
      "Epoch 462/1000\n",
      "  Train Loss: 0.0104\n",
      "Epoch 463/1000\n",
      "  Train Loss: 0.0104\n",
      "Epoch 464/1000\n",
      "  Train Loss: 0.0104\n",
      "Epoch 465/1000\n",
      "  Train Loss: 0.0104\n",
      "Epoch 466/1000\n",
      "  Train Loss: 0.0104\n",
      "Epoch 467/1000\n",
      "  Train Loss: 0.0104\n",
      "Epoch 468/1000\n",
      "  Train Loss: 0.0104\n",
      "Epoch 469/1000\n",
      "  Train Loss: 0.0104\n",
      "Epoch 470/1000\n",
      "  Train Loss: 0.0104\n",
      "Epoch 471/1000\n",
      "  Train Loss: 0.0103\n",
      "Epoch 472/1000\n",
      "  Train Loss: 0.0103\n",
      "Epoch 473/1000\n",
      "  Train Loss: 0.0103\n",
      "Epoch 474/1000\n",
      "  Train Loss: 0.0103\n",
      "Epoch 475/1000\n",
      "  Train Loss: 0.0103\n",
      "Epoch 476/1000\n",
      "  Train Loss: 0.0103\n",
      "Epoch 477/1000\n",
      "  Train Loss: 0.0103\n",
      "Epoch 478/1000\n",
      "  Train Loss: 0.0103\n",
      "Epoch 479/1000\n",
      "  Train Loss: 0.0103\n",
      "Epoch 480/1000\n",
      "  Train Loss: 0.0103\n",
      "Epoch 481/1000\n",
      "  Train Loss: 0.0103\n",
      "Epoch 482/1000\n",
      "  Train Loss: 0.0103\n",
      "Epoch 483/1000\n",
      "  Train Loss: 0.0103\n",
      "Epoch 484/1000\n",
      "  Train Loss: 0.0102\n",
      "Epoch 485/1000\n",
      "  Train Loss: 0.0102\n",
      "Epoch 486/1000\n",
      "  Train Loss: 0.0102\n",
      "Epoch 487/1000\n",
      "  Train Loss: 0.0102\n",
      "Epoch 488/1000\n",
      "  Train Loss: 0.0102\n",
      "Epoch 489/1000\n",
      "  Train Loss: 0.0102\n",
      "Epoch 490/1000\n",
      "  Train Loss: 0.0102\n",
      "Epoch 491/1000\n",
      "  Train Loss: 0.0102\n",
      "Epoch 492/1000\n",
      "  Train Loss: 0.0102\n",
      "Epoch 493/1000\n",
      "  Train Loss: 0.0102\n",
      "Epoch 494/1000\n",
      "  Train Loss: 0.0102\n",
      "Epoch 495/1000\n",
      "  Train Loss: 0.0102\n",
      "Epoch 496/1000\n",
      "  Train Loss: 0.0102\n",
      "Epoch 497/1000\n",
      "  Train Loss: 0.0101\n",
      "Epoch 498/1000\n",
      "  Train Loss: 0.0101\n",
      "Epoch 499/1000\n",
      "  Train Loss: 0.0101\n",
      "Epoch 500/1000\n",
      "  Train Loss: 0.0101\n",
      "Epoch 501/1000\n",
      "  Train Loss: 0.0101\n",
      "Epoch 502/1000\n",
      "  Train Loss: 0.0101\n",
      "Epoch 503/1000\n",
      "  Train Loss: 0.0101\n",
      "Epoch 504/1000\n",
      "  Train Loss: 0.0101\n",
      "Epoch 505/1000\n",
      "  Train Loss: 0.0101\n",
      "Epoch 506/1000\n",
      "  Train Loss: 0.0101\n",
      "Epoch 507/1000\n",
      "  Train Loss: 0.0101\n",
      "Epoch 508/1000\n",
      "  Train Loss: 0.0101\n",
      "Epoch 509/1000\n",
      "  Train Loss: 0.0101\n",
      "Epoch 510/1000\n",
      "  Train Loss: 0.0100\n",
      "Epoch 511/1000\n",
      "  Train Loss: 0.0100\n",
      "Epoch 512/1000\n",
      "  Train Loss: 0.0100\n",
      "Epoch 513/1000\n",
      "  Train Loss: 0.0100\n",
      "Epoch 514/1000\n",
      "  Train Loss: 0.0100\n",
      "Epoch 515/1000\n",
      "  Train Loss: 0.0100\n",
      "Epoch 516/1000\n",
      "  Train Loss: 0.0100\n",
      "Epoch 517/1000\n",
      "  Train Loss: 0.0100\n",
      "Epoch 518/1000\n",
      "  Train Loss: 0.0100\n",
      "Epoch 519/1000\n",
      "  Train Loss: 0.0100\n",
      "Epoch 520/1000\n",
      "  Train Loss: 0.0100\n",
      "Epoch 521/1000\n",
      "  Train Loss: 0.0100\n",
      "Epoch 522/1000\n",
      "  Train Loss: 0.0100\n",
      "Epoch 523/1000\n",
      "  Train Loss: 0.0099\n",
      "Epoch 524/1000\n",
      "  Train Loss: 0.0099\n",
      "Epoch 525/1000\n",
      "  Train Loss: 0.0099\n",
      "Epoch 526/1000\n",
      "  Train Loss: 0.0099\n",
      "Epoch 527/1000\n",
      "  Train Loss: 0.0099\n",
      "Epoch 528/1000\n",
      "  Train Loss: 0.0099\n",
      "Epoch 529/1000\n",
      "  Train Loss: 0.0099\n",
      "Epoch 530/1000\n",
      "  Train Loss: 0.0099\n",
      "Epoch 531/1000\n",
      "  Train Loss: 0.0099\n",
      "Epoch 532/1000\n",
      "  Train Loss: 0.0099\n",
      "Epoch 533/1000\n",
      "  Train Loss: 0.0099\n",
      "Epoch 534/1000\n",
      "  Train Loss: 0.0099\n",
      "Epoch 535/1000\n",
      "  Train Loss: 0.0099\n",
      "Epoch 536/1000\n",
      "  Train Loss: 0.0098\n",
      "Epoch 537/1000\n",
      "  Train Loss: 0.0098\n",
      "Epoch 538/1000\n",
      "  Train Loss: 0.0098\n",
      "Epoch 539/1000\n",
      "  Train Loss: 0.0098\n",
      "Epoch 540/1000\n",
      "  Train Loss: 0.0098\n",
      "Epoch 541/1000\n",
      "  Train Loss: 0.0098\n",
      "Epoch 542/1000\n",
      "  Train Loss: 0.0098\n",
      "Epoch 543/1000\n",
      "  Train Loss: 0.0098\n",
      "Epoch 544/1000\n",
      "  Train Loss: 0.0098\n",
      "Epoch 545/1000\n",
      "  Train Loss: 0.0098\n",
      "Epoch 546/1000\n",
      "  Train Loss: 0.0098\n",
      "Epoch 547/1000\n",
      "  Train Loss: 0.0098\n",
      "Epoch 548/1000\n",
      "  Train Loss: 0.0098\n",
      "Epoch 549/1000\n",
      "  Train Loss: 0.0097\n",
      "Epoch 550/1000\n",
      "  Train Loss: 0.0097\n",
      "Epoch 551/1000\n",
      "  Train Loss: 0.0097\n",
      "Epoch 552/1000\n",
      "  Train Loss: 0.0097\n",
      "Epoch 553/1000\n",
      "  Train Loss: 0.0097\n",
      "Epoch 554/1000\n",
      "  Train Loss: 0.0097\n",
      "Epoch 555/1000\n",
      "  Train Loss: 0.0097\n",
      "Epoch 556/1000\n",
      "  Train Loss: 0.0097\n",
      "Epoch 557/1000\n",
      "  Train Loss: 0.0097\n",
      "Epoch 558/1000\n",
      "  Train Loss: 0.0097\n",
      "Epoch 559/1000\n",
      "  Train Loss: 0.0097\n",
      "Epoch 560/1000\n",
      "  Train Loss: 0.0097\n",
      "Epoch 561/1000\n",
      "  Train Loss: 0.0097\n",
      "Epoch 562/1000\n",
      "  Train Loss: 0.0096\n",
      "Epoch 563/1000\n",
      "  Train Loss: 0.0096\n",
      "Epoch 564/1000\n",
      "  Train Loss: 0.0096\n",
      "Epoch 565/1000\n",
      "  Train Loss: 0.0096\n",
      "Epoch 566/1000\n",
      "  Train Loss: 0.0096\n",
      "Epoch 567/1000\n",
      "  Train Loss: 0.0096\n",
      "Epoch 568/1000\n",
      "  Train Loss: 0.0096\n",
      "Epoch 569/1000\n",
      "  Train Loss: 0.0096\n",
      "Epoch 570/1000\n",
      "  Train Loss: 0.0096\n",
      "Epoch 571/1000\n",
      "  Train Loss: 0.0096\n",
      "Epoch 572/1000\n",
      "  Train Loss: 0.0096\n",
      "Epoch 573/1000\n",
      "  Train Loss: 0.0096\n",
      "Epoch 574/1000\n",
      "  Train Loss: 0.0096\n",
      "Epoch 575/1000\n",
      "  Train Loss: 0.0096\n",
      "Epoch 576/1000\n",
      "  Train Loss: 0.0095\n",
      "Epoch 577/1000\n",
      "  Train Loss: 0.0095\n",
      "Epoch 578/1000\n",
      "  Train Loss: 0.0095\n",
      "Epoch 579/1000\n",
      "  Train Loss: 0.0095\n",
      "Epoch 580/1000\n",
      "  Train Loss: 0.0095\n",
      "Epoch 581/1000\n",
      "  Train Loss: 0.0095\n",
      "Epoch 582/1000\n",
      "  Train Loss: 0.0095\n",
      "Epoch 583/1000\n",
      "  Train Loss: 0.0095\n",
      "Epoch 584/1000\n",
      "  Train Loss: 0.0095\n",
      "Epoch 585/1000\n",
      "  Train Loss: 0.0095\n",
      "Epoch 586/1000\n",
      "  Train Loss: 0.0095\n",
      "Epoch 587/1000\n",
      "  Train Loss: 0.0095\n",
      "Epoch 588/1000\n",
      "  Train Loss: 0.0095\n",
      "Epoch 589/1000\n",
      "  Train Loss: 0.0094\n",
      "Epoch 590/1000\n",
      "  Train Loss: 0.0094\n",
      "Epoch 591/1000\n",
      "  Train Loss: 0.0094\n",
      "Epoch 592/1000\n",
      "  Train Loss: 0.0094\n",
      "Epoch 593/1000\n",
      "  Train Loss: 0.0094\n",
      "Epoch 594/1000\n",
      "  Train Loss: 0.0094\n",
      "Epoch 595/1000\n",
      "  Train Loss: 0.0094\n",
      "Epoch 596/1000\n",
      "  Train Loss: 0.0094\n",
      "Epoch 597/1000\n",
      "  Train Loss: 0.0094\n",
      "Epoch 598/1000\n",
      "  Train Loss: 0.0094\n",
      "Epoch 599/1000\n",
      "  Train Loss: 0.0094\n",
      "Epoch 600/1000\n",
      "  Train Loss: 0.0094\n",
      "Epoch 601/1000\n",
      "  Train Loss: 0.0094\n",
      "Epoch 602/1000\n",
      "  Train Loss: 0.0093\n",
      "Epoch 603/1000\n",
      "  Train Loss: 0.0093\n",
      "Epoch 604/1000\n",
      "  Train Loss: 0.0093\n",
      "Epoch 605/1000\n",
      "  Train Loss: 0.0093\n",
      "Epoch 606/1000\n",
      "  Train Loss: 0.0093\n",
      "Epoch 607/1000\n",
      "  Train Loss: 0.0093\n",
      "Epoch 608/1000\n",
      "  Train Loss: 0.0093\n",
      "Epoch 609/1000\n",
      "  Train Loss: 0.0093\n",
      "Epoch 610/1000\n",
      "  Train Loss: 0.0093\n",
      "Epoch 611/1000\n",
      "  Train Loss: 0.0093\n",
      "Epoch 612/1000\n",
      "  Train Loss: 0.0093\n",
      "Epoch 613/1000\n",
      "  Train Loss: 0.0093\n",
      "Epoch 614/1000\n",
      "  Train Loss: 0.0093\n",
      "Epoch 615/1000\n",
      "  Train Loss: 0.0092\n",
      "Epoch 616/1000\n",
      "  Train Loss: 0.0092\n",
      "Epoch 617/1000\n",
      "  Train Loss: 0.0092\n",
      "Epoch 618/1000\n",
      "  Train Loss: 0.0092\n",
      "Epoch 619/1000\n",
      "  Train Loss: 0.0092\n",
      "Epoch 620/1000\n",
      "  Train Loss: 0.0092\n",
      "Epoch 621/1000\n",
      "  Train Loss: 0.0092\n",
      "Epoch 622/1000\n",
      "  Train Loss: 0.0092\n",
      "Epoch 623/1000\n",
      "  Train Loss: 0.0092\n",
      "Epoch 624/1000\n",
      "  Train Loss: 0.0092\n",
      "Epoch 625/1000\n",
      "  Train Loss: 0.0092\n",
      "Epoch 626/1000\n",
      "  Train Loss: 0.0092\n",
      "Epoch 627/1000\n",
      "  Train Loss: 0.0092\n",
      "Epoch 628/1000\n",
      "  Train Loss: 0.0091\n",
      "Epoch 629/1000\n",
      "  Train Loss: 0.0091\n",
      "Epoch 630/1000\n",
      "  Train Loss: 0.0091\n",
      "Epoch 631/1000\n",
      "  Train Loss: 0.0091\n",
      "Epoch 632/1000\n",
      "  Train Loss: 0.0091\n",
      "Epoch 633/1000\n",
      "  Train Loss: 0.0091\n",
      "Epoch 634/1000\n",
      "  Train Loss: 0.0091\n",
      "Epoch 635/1000\n",
      "  Train Loss: 0.0091\n",
      "Epoch 636/1000\n",
      "  Train Loss: 0.0091\n",
      "Epoch 637/1000\n",
      "  Train Loss: 0.0091\n",
      "Epoch 638/1000\n",
      "  Train Loss: 0.0091\n",
      "Epoch 639/1000\n",
      "  Train Loss: 0.0091\n",
      "Epoch 640/1000\n",
      "  Train Loss: 0.0091\n",
      "Epoch 641/1000\n",
      "  Train Loss: 0.0091\n",
      "Epoch 642/1000\n",
      "  Train Loss: 0.0090\n",
      "Epoch 643/1000\n",
      "  Train Loss: 0.0090\n",
      "Epoch 644/1000\n",
      "  Train Loss: 0.0090\n",
      "Epoch 645/1000\n",
      "  Train Loss: 0.0090\n",
      "Epoch 646/1000\n",
      "  Train Loss: 0.0090\n",
      "Epoch 647/1000\n",
      "  Train Loss: 0.0090\n",
      "Epoch 648/1000\n",
      "  Train Loss: 0.0090\n",
      "Epoch 649/1000\n",
      "  Train Loss: 0.0090\n",
      "Epoch 650/1000\n",
      "  Train Loss: 0.0090\n",
      "Epoch 651/1000\n",
      "  Train Loss: 0.0090\n",
      "Epoch 652/1000\n",
      "  Train Loss: 0.0090\n",
      "Epoch 653/1000\n",
      "  Train Loss: 0.0090\n",
      "Epoch 654/1000\n",
      "  Train Loss: 0.0090\n",
      "Epoch 655/1000\n",
      "  Train Loss: 0.0089\n",
      "Epoch 656/1000\n",
      "  Train Loss: 0.0089\n",
      "Epoch 657/1000\n",
      "  Train Loss: 0.0089\n",
      "Epoch 658/1000\n",
      "  Train Loss: 0.0089\n",
      "Epoch 659/1000\n",
      "  Train Loss: 0.0089\n",
      "Epoch 660/1000\n",
      "  Train Loss: 0.0089\n",
      "Epoch 661/1000\n",
      "  Train Loss: 0.0089\n",
      "Epoch 662/1000\n",
      "  Train Loss: 0.0089\n",
      "Epoch 663/1000\n",
      "  Train Loss: 0.0089\n",
      "Epoch 664/1000\n",
      "  Train Loss: 0.0089\n",
      "Epoch 665/1000\n",
      "  Train Loss: 0.0089\n",
      "Epoch 666/1000\n",
      "  Train Loss: 0.0089\n",
      "Epoch 667/1000\n",
      "  Train Loss: 0.0089\n",
      "Epoch 668/1000\n",
      "  Train Loss: 0.0088\n",
      "Epoch 669/1000\n",
      "  Train Loss: 0.0088\n",
      "Epoch 670/1000\n",
      "  Train Loss: 0.0088\n",
      "Epoch 671/1000\n",
      "  Train Loss: 0.0088\n",
      "Epoch 672/1000\n",
      "  Train Loss: 0.0088\n",
      "Epoch 673/1000\n",
      "  Train Loss: 0.0088\n",
      "Epoch 674/1000\n",
      "  Train Loss: 0.0088\n",
      "Epoch 675/1000\n",
      "  Train Loss: 0.0088\n",
      "Epoch 676/1000\n",
      "  Train Loss: 0.0088\n",
      "Epoch 677/1000\n",
      "  Train Loss: 0.0088\n",
      "Epoch 678/1000\n",
      "  Train Loss: 0.0088\n",
      "Epoch 679/1000\n",
      "  Train Loss: 0.0088\n",
      "Epoch 680/1000\n",
      "  Train Loss: 0.0088\n",
      "Epoch 681/1000\n",
      "  Train Loss: 0.0088\n",
      "Epoch 682/1000\n",
      "  Train Loss: 0.0087\n",
      "Epoch 683/1000\n",
      "  Train Loss: 0.0087\n",
      "Epoch 684/1000\n",
      "  Train Loss: 0.0087\n",
      "Epoch 685/1000\n",
      "  Train Loss: 0.0087\n",
      "Epoch 686/1000\n",
      "  Train Loss: 0.0087\n",
      "Epoch 687/1000\n",
      "  Train Loss: 0.0087\n",
      "Epoch 688/1000\n",
      "  Train Loss: 0.0087\n",
      "Epoch 689/1000\n",
      "  Train Loss: 0.0087\n",
      "Epoch 690/1000\n",
      "  Train Loss: 0.0087\n",
      "Epoch 691/1000\n",
      "  Train Loss: 0.0087\n",
      "Epoch 692/1000\n",
      "  Train Loss: 0.0087\n",
      "Epoch 693/1000\n",
      "  Train Loss: 0.0087\n",
      "Epoch 694/1000\n",
      "  Train Loss: 0.0087\n",
      "Epoch 695/1000\n",
      "  Train Loss: 0.0086\n",
      "Epoch 696/1000\n",
      "  Train Loss: 0.0086\n",
      "Epoch 697/1000\n",
      "  Train Loss: 0.0086\n",
      "Epoch 698/1000\n",
      "  Train Loss: 0.0086\n",
      "Epoch 699/1000\n",
      "  Train Loss: 0.0086\n",
      "Epoch 700/1000\n",
      "  Train Loss: 0.0086\n",
      "Epoch 701/1000\n",
      "  Train Loss: 0.0086\n",
      "Epoch 702/1000\n",
      "  Train Loss: 0.0086\n",
      "Epoch 703/1000\n",
      "  Train Loss: 0.0086\n",
      "Epoch 704/1000\n",
      "  Train Loss: 0.0086\n",
      "Epoch 705/1000\n",
      "  Train Loss: 0.0086\n",
      "Epoch 706/1000\n",
      "  Train Loss: 0.0086\n",
      "Epoch 707/1000\n",
      "  Train Loss: 0.0086\n",
      "Epoch 708/1000\n",
      "  Train Loss: 0.0086\n",
      "Epoch 709/1000\n",
      "  Train Loss: 0.0085\n",
      "Epoch 710/1000\n",
      "  Train Loss: 0.0085\n",
      "Epoch 711/1000\n",
      "  Train Loss: 0.0085\n",
      "Epoch 712/1000\n",
      "  Train Loss: 0.0085\n",
      "Epoch 713/1000\n",
      "  Train Loss: 0.0085\n",
      "Epoch 714/1000\n",
      "  Train Loss: 0.0085\n",
      "Epoch 715/1000\n",
      "  Train Loss: 0.0085\n",
      "Epoch 716/1000\n",
      "  Train Loss: 0.0085\n",
      "Epoch 717/1000\n",
      "  Train Loss: 0.0085\n",
      "Epoch 718/1000\n",
      "  Train Loss: 0.0085\n",
      "Epoch 719/1000\n",
      "  Train Loss: 0.0085\n",
      "Epoch 720/1000\n",
      "  Train Loss: 0.0085\n",
      "Epoch 721/1000\n",
      "  Train Loss: 0.0085\n",
      "Epoch 722/1000\n",
      "  Train Loss: 0.0085\n",
      "Epoch 723/1000\n",
      "  Train Loss: 0.0084\n",
      "Epoch 724/1000\n",
      "  Train Loss: 0.0084\n",
      "Epoch 725/1000\n",
      "  Train Loss: 0.0084\n",
      "Epoch 726/1000\n",
      "  Train Loss: 0.0084\n",
      "Epoch 727/1000\n",
      "  Train Loss: 0.0084\n",
      "Epoch 728/1000\n",
      "  Train Loss: 0.0084\n",
      "Epoch 729/1000\n",
      "  Train Loss: 0.0084\n",
      "Epoch 730/1000\n",
      "  Train Loss: 0.0084\n",
      "Epoch 731/1000\n",
      "  Train Loss: 0.0084\n",
      "Epoch 732/1000\n",
      "  Train Loss: 0.0084\n",
      "Epoch 733/1000\n",
      "  Train Loss: 0.0084\n",
      "Epoch 734/1000\n",
      "  Train Loss: 0.0084\n",
      "Epoch 735/1000\n",
      "  Train Loss: 0.0084\n",
      "Epoch 736/1000\n",
      "  Train Loss: 0.0083\n",
      "Epoch 737/1000\n",
      "  Train Loss: 0.0083\n",
      "Epoch 738/1000\n",
      "  Train Loss: 0.0083\n",
      "Epoch 739/1000\n",
      "  Train Loss: 0.0083\n",
      "Epoch 740/1000\n",
      "  Train Loss: 0.0083\n",
      "Epoch 741/1000\n",
      "  Train Loss: 0.0083\n",
      "Epoch 742/1000\n",
      "  Train Loss: 0.0083\n",
      "Epoch 743/1000\n",
      "  Train Loss: 0.0083\n",
      "Epoch 744/1000\n",
      "  Train Loss: 0.0083\n",
      "Epoch 745/1000\n",
      "  Train Loss: 0.0083\n",
      "Epoch 746/1000\n",
      "  Train Loss: 0.0083\n",
      "Epoch 747/1000\n",
      "  Train Loss: 0.0083\n",
      "Epoch 748/1000\n",
      "  Train Loss: 0.0083\n",
      "Epoch 749/1000\n",
      "  Train Loss: 0.0083\n",
      "Epoch 750/1000\n",
      "  Train Loss: 0.0082\n",
      "Epoch 751/1000\n",
      "  Train Loss: 0.0082\n",
      "Epoch 752/1000\n",
      "  Train Loss: 0.0082\n",
      "Epoch 753/1000\n",
      "  Train Loss: 0.0082\n",
      "Epoch 754/1000\n",
      "  Train Loss: 0.0082\n",
      "Epoch 755/1000\n",
      "  Train Loss: 0.0082\n",
      "Epoch 756/1000\n",
      "  Train Loss: 0.0082\n",
      "Epoch 757/1000\n",
      "  Train Loss: 0.0082\n",
      "Epoch 758/1000\n",
      "  Train Loss: 0.0082\n",
      "Epoch 759/1000\n",
      "  Train Loss: 0.0082\n",
      "Epoch 760/1000\n",
      "  Train Loss: 0.0082\n",
      "Epoch 761/1000\n",
      "  Train Loss: 0.0082\n",
      "Epoch 762/1000\n",
      "  Train Loss: 0.0082\n",
      "Epoch 763/1000\n",
      "  Train Loss: 0.0082\n",
      "Epoch 764/1000\n",
      "  Train Loss: 0.0081\n",
      "Epoch 765/1000\n",
      "  Train Loss: 0.0081\n",
      "Epoch 766/1000\n",
      "  Train Loss: 0.0081\n",
      "Epoch 767/1000\n",
      "  Train Loss: 0.0081\n",
      "Epoch 768/1000\n",
      "  Train Loss: 0.0081\n",
      "Epoch 769/1000\n",
      "  Train Loss: 0.0081\n",
      "Epoch 770/1000\n",
      "  Train Loss: 0.0081\n",
      "Epoch 771/1000\n",
      "  Train Loss: 0.0081\n",
      "Epoch 772/1000\n",
      "  Train Loss: 0.0081\n",
      "Epoch 773/1000\n",
      "  Train Loss: 0.0081\n",
      "Epoch 774/1000\n",
      "  Train Loss: 0.0081\n",
      "Epoch 775/1000\n",
      "  Train Loss: 0.0081\n",
      "Epoch 776/1000\n",
      "  Train Loss: 0.0081\n",
      "Epoch 777/1000\n",
      "  Train Loss: 0.0081\n",
      "Epoch 778/1000\n",
      "  Train Loss: 0.0080\n",
      "Epoch 779/1000\n",
      "  Train Loss: 0.0080\n",
      "Epoch 780/1000\n",
      "  Train Loss: 0.0080\n",
      "Epoch 781/1000\n",
      "  Train Loss: 0.0080\n",
      "Epoch 782/1000\n",
      "  Train Loss: 0.0080\n",
      "Epoch 783/1000\n",
      "  Train Loss: 0.0080\n",
      "Epoch 784/1000\n",
      "  Train Loss: 0.0080\n",
      "Epoch 785/1000\n",
      "  Train Loss: 0.0080\n",
      "Epoch 786/1000\n",
      "  Train Loss: 0.0080\n",
      "Epoch 787/1000\n",
      "  Train Loss: 0.0080\n",
      "Epoch 788/1000\n",
      "  Train Loss: 0.0080\n",
      "Epoch 789/1000\n",
      "  Train Loss: 0.0080\n",
      "Epoch 790/1000\n",
      "  Train Loss: 0.0080\n",
      "Epoch 791/1000\n",
      "  Train Loss: 0.0080\n",
      "Epoch 792/1000\n",
      "  Train Loss: 0.0079\n",
      "Epoch 793/1000\n",
      "  Train Loss: 0.0079\n",
      "Epoch 794/1000\n",
      "  Train Loss: 0.0079\n",
      "Epoch 795/1000\n",
      "  Train Loss: 0.0079\n",
      "Epoch 796/1000\n",
      "  Train Loss: 0.0079\n",
      "Epoch 797/1000\n",
      "  Train Loss: 0.0079\n",
      "Epoch 798/1000\n",
      "  Train Loss: 0.0079\n",
      "Epoch 799/1000\n",
      "  Train Loss: 0.0079\n",
      "Epoch 800/1000\n",
      "  Train Loss: 0.0079\n",
      "Epoch 801/1000\n",
      "  Train Loss: 0.0079\n",
      "Epoch 802/1000\n",
      "  Train Loss: 0.0079\n",
      "Epoch 803/1000\n",
      "  Train Loss: 0.0079\n",
      "Epoch 804/1000\n",
      "  Train Loss: 0.0079\n",
      "Epoch 805/1000\n",
      "  Train Loss: 0.0079\n",
      "Epoch 806/1000\n",
      "  Train Loss: 0.0078\n",
      "Epoch 807/1000\n",
      "  Train Loss: 0.0078\n",
      "Epoch 808/1000\n",
      "  Train Loss: 0.0078\n",
      "Epoch 809/1000\n",
      "  Train Loss: 0.0078\n",
      "Epoch 810/1000\n",
      "  Train Loss: 0.0078\n",
      "Epoch 811/1000\n",
      "  Train Loss: 0.0078\n",
      "Epoch 812/1000\n",
      "  Train Loss: 0.0078\n",
      "Epoch 813/1000\n",
      "  Train Loss: 0.0078\n",
      "Epoch 814/1000\n",
      "  Train Loss: 0.0078\n",
      "Epoch 815/1000\n",
      "  Train Loss: 0.0078\n",
      "Epoch 816/1000\n",
      "  Train Loss: 0.0078\n",
      "Epoch 817/1000\n",
      "  Train Loss: 0.0078\n",
      "Epoch 818/1000\n",
      "  Train Loss: 0.0078\n",
      "Epoch 819/1000\n",
      "  Train Loss: 0.0078\n",
      "Epoch 820/1000\n",
      "  Train Loss: 0.0078\n",
      "Epoch 821/1000\n",
      "  Train Loss: 0.0077\n",
      "Epoch 822/1000\n",
      "  Train Loss: 0.0077\n",
      "Epoch 823/1000\n",
      "  Train Loss: 0.0077\n",
      "Epoch 824/1000\n",
      "  Train Loss: 0.0077\n",
      "Epoch 825/1000\n",
      "  Train Loss: 0.0077\n",
      "Epoch 826/1000\n",
      "  Train Loss: 0.0077\n",
      "Epoch 827/1000\n",
      "  Train Loss: 0.0077\n",
      "Epoch 828/1000\n",
      "  Train Loss: 0.0077\n",
      "Epoch 829/1000\n",
      "  Train Loss: 0.0077\n",
      "Epoch 830/1000\n",
      "  Train Loss: 0.0077\n",
      "Epoch 831/1000\n",
      "  Train Loss: 0.0077\n",
      "Epoch 832/1000\n",
      "  Train Loss: 0.0077\n",
      "Epoch 833/1000\n",
      "  Train Loss: 0.0077\n",
      "Epoch 834/1000\n",
      "  Train Loss: 0.0077\n",
      "Epoch 835/1000\n",
      "  Train Loss: 0.0076\n",
      "Epoch 836/1000\n",
      "  Train Loss: 0.0076\n",
      "Epoch 837/1000\n",
      "  Train Loss: 0.0076\n",
      "Epoch 838/1000\n",
      "  Train Loss: 0.0076\n",
      "Epoch 839/1000\n",
      "  Train Loss: 0.0076\n",
      "Epoch 840/1000\n",
      "  Train Loss: 0.0076\n",
      "Epoch 841/1000\n",
      "  Train Loss: 0.0076\n",
      "Epoch 842/1000\n",
      "  Train Loss: 0.0076\n",
      "Epoch 843/1000\n",
      "  Train Loss: 0.0076\n",
      "Epoch 844/1000\n",
      "  Train Loss: 0.0076\n",
      "Epoch 845/1000\n",
      "  Train Loss: 0.0076\n",
      "Epoch 846/1000\n",
      "  Train Loss: 0.0076\n",
      "Epoch 847/1000\n",
      "  Train Loss: 0.0076\n",
      "Epoch 848/1000\n",
      "  Train Loss: 0.0076\n",
      "Epoch 849/1000\n",
      "  Train Loss: 0.0075\n",
      "Epoch 850/1000\n",
      "  Train Loss: 0.0075\n",
      "Epoch 851/1000\n",
      "  Train Loss: 0.0075\n",
      "Epoch 852/1000\n",
      "  Train Loss: 0.0075\n",
      "Epoch 853/1000\n",
      "  Train Loss: 0.0075\n",
      "Epoch 854/1000\n",
      "  Train Loss: 0.0075\n",
      "Epoch 855/1000\n",
      "  Train Loss: 0.0075\n",
      "Epoch 856/1000\n",
      "  Train Loss: 0.0075\n",
      "Epoch 857/1000\n",
      "  Train Loss: 0.0075\n",
      "Epoch 858/1000\n",
      "  Train Loss: 0.0075\n",
      "Epoch 859/1000\n",
      "  Train Loss: 0.0075\n",
      "Epoch 860/1000\n",
      "  Train Loss: 0.0075\n",
      "Epoch 861/1000\n",
      "  Train Loss: 0.0075\n",
      "Epoch 862/1000\n",
      "  Train Loss: 0.0075\n",
      "Epoch 863/1000\n",
      "  Train Loss: 0.0075\n",
      "Epoch 864/1000\n",
      "  Train Loss: 0.0074\n",
      "Epoch 865/1000\n",
      "  Train Loss: 0.0074\n",
      "Epoch 866/1000\n",
      "  Train Loss: 0.0074\n",
      "Epoch 867/1000\n",
      "  Train Loss: 0.0074\n",
      "Epoch 868/1000\n",
      "  Train Loss: 0.0074\n",
      "Epoch 869/1000\n",
      "  Train Loss: 0.0074\n",
      "Epoch 870/1000\n",
      "  Train Loss: 0.0074\n",
      "Epoch 871/1000\n",
      "  Train Loss: 0.0074\n",
      "Epoch 872/1000\n",
      "  Train Loss: 0.0074\n",
      "Epoch 873/1000\n",
      "  Train Loss: 0.0074\n",
      "Epoch 874/1000\n",
      "  Train Loss: 0.0074\n",
      "Epoch 875/1000\n",
      "  Train Loss: 0.0074\n",
      "Epoch 876/1000\n",
      "  Train Loss: 0.0074\n",
      "Epoch 877/1000\n",
      "  Train Loss: 0.0074\n",
      "Epoch 878/1000\n",
      "  Train Loss: 0.0074\n",
      "Epoch 879/1000\n",
      "  Train Loss: 0.0073\n",
      "Epoch 880/1000\n",
      "  Train Loss: 0.0073\n",
      "Epoch 881/1000\n",
      "  Train Loss: 0.0073\n",
      "Epoch 882/1000\n",
      "  Train Loss: 0.0073\n",
      "Epoch 883/1000\n",
      "  Train Loss: 0.0073\n",
      "Epoch 884/1000\n",
      "  Train Loss: 0.0073\n",
      "Epoch 885/1000\n",
      "  Train Loss: 0.0073\n",
      "Epoch 886/1000\n",
      "  Train Loss: 0.0073\n",
      "Epoch 887/1000\n",
      "  Train Loss: 0.0073\n",
      "Epoch 888/1000\n",
      "  Train Loss: 0.0073\n",
      "Epoch 889/1000\n",
      "  Train Loss: 0.0073\n",
      "Epoch 890/1000\n",
      "  Train Loss: 0.0073\n",
      "Epoch 891/1000\n",
      "  Train Loss: 0.0073\n",
      "Epoch 892/1000\n",
      "  Train Loss: 0.0073\n",
      "Epoch 893/1000\n",
      "  Train Loss: 0.0072\n",
      "Epoch 894/1000\n",
      "  Train Loss: 0.0072\n",
      "Epoch 895/1000\n",
      "  Train Loss: 0.0072\n",
      "Epoch 896/1000\n",
      "  Train Loss: 0.0072\n",
      "Epoch 897/1000\n",
      "  Train Loss: 0.0072\n",
      "Epoch 898/1000\n",
      "  Train Loss: 0.0072\n",
      "Epoch 899/1000\n",
      "  Train Loss: 0.0072\n",
      "Epoch 900/1000\n",
      "  Train Loss: 0.0072\n",
      "Epoch 901/1000\n",
      "  Train Loss: 0.0072\n",
      "Epoch 902/1000\n",
      "  Train Loss: 0.0072\n",
      "Epoch 903/1000\n",
      "  Train Loss: 0.0072\n",
      "Epoch 904/1000\n",
      "  Train Loss: 0.0072\n",
      "Epoch 905/1000\n",
      "  Train Loss: 0.0072\n",
      "Epoch 906/1000\n",
      "  Train Loss: 0.0072\n",
      "Epoch 907/1000\n",
      "  Train Loss: 0.0072\n",
      "Epoch 908/1000\n",
      "  Train Loss: 0.0071\n",
      "Epoch 909/1000\n",
      "  Train Loss: 0.0071\n",
      "Epoch 910/1000\n",
      "  Train Loss: 0.0071\n",
      "Epoch 911/1000\n",
      "  Train Loss: 0.0071\n",
      "Epoch 912/1000\n",
      "  Train Loss: 0.0071\n",
      "Epoch 913/1000\n",
      "  Train Loss: 0.0071\n",
      "Epoch 914/1000\n",
      "  Train Loss: 0.0071\n",
      "Epoch 915/1000\n",
      "  Train Loss: 0.0071\n",
      "Epoch 916/1000\n",
      "  Train Loss: 0.0071\n",
      "Epoch 917/1000\n",
      "  Train Loss: 0.0071\n",
      "Epoch 918/1000\n",
      "  Train Loss: 0.0071\n",
      "Epoch 919/1000\n",
      "  Train Loss: 0.0071\n",
      "Epoch 920/1000\n",
      "  Train Loss: 0.0071\n",
      "Epoch 921/1000\n",
      "  Train Loss: 0.0071\n",
      "Epoch 922/1000\n",
      "  Train Loss: 0.0071\n",
      "Epoch 923/1000\n",
      "  Train Loss: 0.0070\n",
      "Epoch 924/1000\n",
      "  Train Loss: 0.0070\n",
      "Epoch 925/1000\n",
      "  Train Loss: 0.0070\n",
      "Epoch 926/1000\n",
      "  Train Loss: 0.0070\n",
      "Epoch 927/1000\n",
      "  Train Loss: 0.0070\n",
      "Epoch 928/1000\n",
      "  Train Loss: 0.0070\n",
      "Epoch 929/1000\n",
      "  Train Loss: 0.0070\n",
      "Epoch 930/1000\n",
      "  Train Loss: 0.0070\n",
      "Epoch 931/1000\n",
      "  Train Loss: 0.0070\n",
      "Epoch 932/1000\n",
      "  Train Loss: 0.0070\n",
      "Epoch 933/1000\n",
      "  Train Loss: 0.0070\n",
      "Epoch 934/1000\n",
      "  Train Loss: 0.0070\n",
      "Epoch 935/1000\n",
      "  Train Loss: 0.0070\n",
      "Epoch 936/1000\n",
      "  Train Loss: 0.0070\n",
      "Epoch 937/1000\n",
      "  Train Loss: 0.0070\n",
      "Epoch 938/1000\n",
      "  Train Loss: 0.0069\n",
      "Epoch 939/1000\n",
      "  Train Loss: 0.0069\n",
      "Epoch 940/1000\n",
      "  Train Loss: 0.0069\n",
      "Epoch 941/1000\n",
      "  Train Loss: 0.0069\n",
      "Epoch 942/1000\n",
      "  Train Loss: 0.0069\n",
      "Epoch 943/1000\n",
      "  Train Loss: 0.0069\n",
      "Epoch 944/1000\n",
      "  Train Loss: 0.0069\n",
      "Epoch 945/1000\n",
      "  Train Loss: 0.0069\n",
      "Epoch 946/1000\n",
      "  Train Loss: 0.0069\n",
      "Epoch 947/1000\n",
      "  Train Loss: 0.0069\n",
      "Epoch 948/1000\n",
      "  Train Loss: 0.0069\n",
      "Epoch 949/1000\n",
      "  Train Loss: 0.0069\n",
      "Epoch 950/1000\n",
      "  Train Loss: 0.0069\n",
      "Epoch 951/1000\n",
      "  Train Loss: 0.0069\n",
      "Epoch 952/1000\n",
      "  Train Loss: 0.0069\n",
      "Epoch 953/1000\n",
      "  Train Loss: 0.0068\n",
      "Epoch 954/1000\n",
      "  Train Loss: 0.0068\n",
      "Epoch 955/1000\n",
      "  Train Loss: 0.0068\n",
      "Epoch 956/1000\n",
      "  Train Loss: 0.0068\n",
      "Epoch 957/1000\n",
      "  Train Loss: 0.0068\n",
      "Epoch 958/1000\n",
      "  Train Loss: 0.0068\n",
      "Epoch 959/1000\n",
      "  Train Loss: 0.0068\n",
      "Epoch 960/1000\n",
      "  Train Loss: 0.0068\n",
      "Epoch 961/1000\n",
      "  Train Loss: 0.0068\n",
      "Epoch 962/1000\n",
      "  Train Loss: 0.0068\n",
      "Epoch 963/1000\n",
      "  Train Loss: 0.0068\n",
      "Epoch 964/1000\n",
      "  Train Loss: 0.0068\n",
      "Epoch 965/1000\n",
      "  Train Loss: 0.0068\n",
      "Epoch 966/1000\n",
      "  Train Loss: 0.0068\n",
      "Epoch 967/1000\n",
      "  Train Loss: 0.0068\n",
      "Epoch 968/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 969/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 970/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 971/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 972/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 973/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 974/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 975/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 976/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 977/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 978/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 979/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 980/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 981/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 982/1000\n",
      "  Train Loss: 0.0067\n",
      "Epoch 983/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 984/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 985/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 986/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 987/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 988/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 989/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 990/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 991/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 992/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 993/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 994/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 995/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 996/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 997/1000\n",
      "  Train Loss: 0.0066\n",
      "Epoch 998/1000\n",
      "  Train Loss: 0.0065\n",
      "Epoch 999/1000\n",
      "  Train Loss: 0.0065\n",
      "Epoch 1000/1000\n",
      "  Train Loss: 0.0065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ra43rid/torch_plnet/models/ranking_models.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  skills = self.forward(torch.tensor(X, dtype=torch.float32))\n",
      "/tmp/ipykernel_3952059/3994309802.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  conformities = oracle.get_conformity(torch.tensor(X_test,device=\"cuda\"), torch.tensor(y_test,device=\"cuda\")).detach().cpu().numpy()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m num_instances_to_check \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m150\u001b[39m,\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# num_instances_to_check = [40]\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m taus, gammas \u001b[38;5;241m=\u001b[39m \u001b[43mconduct_oracle_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_instances_to_check\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_instances_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 110\u001b[0m, in \u001b[0;36mconduct_oracle_experiment\u001b[0;34m(num_instances_to_check, generator, learning_rate, num_epochs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    106\u001b[0m         executor\u001b[38;5;241m.\u001b[39msubmit(evaluate_model, model, oracle, name, X_test, y_test, taus, gammas)\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m model, oracle, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(models, oracles, names)\n\u001b[1;32m    108\u001b[0m     ]\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[0;32m--> 110\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure completion\u001b[39;00m\n\u001b[1;32m    112\u001b[0m skills_from_model \u001b[38;5;241m=\u001b[39m model_lac(X_test)\n\u001b[1;32m    113\u001b[0m own_lac \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtake_along_dim(skills_from_model, y_test\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[0;32mIn[3], line 68\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, oracle, name, X_test, y_test, taus, gammas)\u001b[0m\n\u001b[1;32m     61\u001b[0m skills \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtake_along_axis(\n\u001b[1;32m     62\u001b[0m     model\u001b[38;5;241m.\u001b[39mpredict_class_skills(X_test),\n\u001b[1;32m     63\u001b[0m     y_test[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     64\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     65\u001b[0m )\n\u001b[1;32m     66\u001b[0m conformities \u001b[38;5;241m=\u001b[39m oracle\u001b[38;5;241m.\u001b[39mget_conformity(torch\u001b[38;5;241m.\u001b[39mtensor(X_test,device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m), torch\u001b[38;5;241m.\u001b[39mtensor(y_test,device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 68\u001b[0m tau_corr, _ \u001b[38;5;241m=\u001b[39m \u001b[43mkendalltau\u001b[49m\u001b[43m(\u001b[49m\u001b[43mskills\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconformities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m gamma_corr \u001b[38;5;241m=\u001b[39m goodman_kruskal_gamma(skills, conformities)\n\u001b[1;32m     71\u001b[0m taus[name]\u001b[38;5;241m.\u001b[39mappend(tau_corr)\n",
      "File \u001b[0;32m~/torch_plnet/venv/lib/python3.10/site-packages/scipy/stats/_stats_py.py:5894\u001b[0m, in \u001b[0;36mkendalltau\u001b[0;34m(x, y, nan_policy, method, variant, alternative)\u001b[0m\n\u001b[1;32m   5662\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkendalltau\u001b[39m(x, y, \u001b[38;5;241m*\u001b[39m, nan_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpropagate\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   5663\u001b[0m                method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, variant\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo-sided\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   5664\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Calculate Kendall's tau, a correlation measure for ordinal data.\u001b[39;00m\n\u001b[1;32m   5665\u001b[0m \n\u001b[1;32m   5666\u001b[0m \u001b[38;5;124;03m    Kendall's tau is a measure of the correspondence between two rankings.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5892\u001b[0m \n\u001b[1;32m   5893\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5894\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m   5895\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(y)\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m   5897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39msize:\n",
      "File \u001b[0;32m~/torch_plnet/venv/lib/python3.10/site-packages/torch/_tensor.py:1087\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1087\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# X_train, y_train = make_classification(\n",
    "#     n_samples=100, n_features=3, n_classes=3, n_informative=3, n_redundant=0, n_repeated=0, n_clusters_per_class=1, random_state=42\n",
    "# )\n",
    "\n",
    "clf = clf_3d_3c\n",
    "\n",
    "clf.fit(None, None)\n",
    "num_instances_to_check = np.linspace(10,150,5).astype(int)\n",
    "# num_instances_to_check = [40]\n",
    "\n",
    "taus, gammas = conduct_oracle_experiment(num_instances_to_check=num_instances_to_check,generator=clf,learning_rate=0.01, num_epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5,rc={'text.usetex' : True})\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rc('font', **{'family': 'serif'})\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 3)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title(\"\")\n",
    "ax.set_ylabel(r\"Kendall's $\\tau$-b\")\n",
    "ax.set_xlabel(r\"No. Instances\")\n",
    "# ax.set_ylim([0.0,1])\n",
    "sns.lineplot(x=num_instances_to_check, y=taus[\"lac\"], ax = ax, marker=\"o\",label=\"LAC\", legend=False)\n",
    "sns.lineplot(x=num_instances_to_check, y=taus[\"aps\"], ax = ax, marker=\"o\",label=\"APS\", legend=False)\n",
    "sns.lineplot(x=num_instances_to_check, y=taus[\"own_aps\"], ax = ax, marker=\"o\",label=\"Reconstructed APS\", legend=False, linestyle=\"--\")\n",
    "# sns.lineplot(x=num_instances_to_check, y=taus[\"rand_aps\"], ax = ax, marker=\"o\",label=\"APS\", legend=False)\n",
    "# sns.lineplot(x=num_instances_to_check, y=taus[\"own_rand_aps\"], ax = ax, marker=\"o\",label=\"APS\", legend=False)\n",
    "# sns.lineplot(x=num_instances_to_check, y=tau_corrs_own_aps, ax = ax, marker=\"o\",label=\"Reconstructed APS\", linestyle=\"--\", legend=False)\n",
    "# sns.lineplot(x=num_instances_to_check, y=tau_corrs_SAPS, ax = ax, marker=\"o\", label=\"SAPS\", legend=False)\n",
    "lgd = fig.legend(loc='upper center', ncol=3, bbox_to_anchor=(0.5, 0.08), frameon=False)\n",
    "\n",
    "fig.tight_layout() \n",
    "plt.savefig(\"replicating.pdf\",bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "# axes[1].set_title(\"APS\")\n",
    "# axes[1].set_ylabel(r\"Kendalls $\\tau$\")\n",
    "# axes[1].set_xlabel(r\"No. Pairs\")\n",
    "# sns.lineplot(x=num_pairs_to_check, y=tau_corrs_APS, ax = axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_lac' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m skills_from_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_lac\u001b[49m(X_test)\n\u001b[0;32m      2\u001b[0m y_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_test, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m skills_from_model \u001b[38;5;241m=\u001b[39m skills_from_model \u001b[38;5;241m-\u001b[39m skills_from_model\u001b[38;5;241m.\u001b[39mmin()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_lac' is not defined"
     ]
    }
   ],
   "source": [
    "skills_from_model = model_lac(X_test)\n",
    "y_test = torch.tensor(y_test, device=\"cuda\")\n",
    "skills_from_model = skills_from_model - skills_from_model.min()\n",
    "skills_from_model = skills_from_model / (skills_from_model.max() - skills_from_model.min())\n",
    "own_aps = aps._calculate_single_label(-torch.tensor(skills_from_model), y_test).detach().cpu().numpy()\n",
    "y_test = torch.tensor(y_test, device=\"cuda\")\n",
    "own_aps = aps._calculate_single_label(-torch.tensor(skills_from_model), y_test).detach().cpu().numpy()\n",
    "# own_aps = np.take_along_axis(own_aps, y_test.detach().numpy()[:,np.newaxis], axis=1)\n",
    "aps_scores = oracle_annotator_aps.get_conformity(X_test, y_test).detach().cpu().numpy()\n",
    "tau_corr, p_value = kendalltau(own_aps, aps_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
